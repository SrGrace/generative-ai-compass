# ü§ù Community-contributed resources and case studies

## RAG (Retrieval-Augmented Generation)
- [RAG From Scratch](https://github.com/langchain-ai/rag-from-scratch) by LangChain
- [LLM Applications for production](https://github.com/ray-project/llm-applications/tree/main) by ray-project
- [LLM tutorials](https://github.com/ollama/ollama/tree/main/examples) by Ollama
- [All RAG Variants](https://github.com/SrGrace/RAG) by SrGrace
- [AutoRAG](https://github.com/Marker-Inc-Korea/AutoRAG)
- 

## Agentic AI
- [Agentic AI](https://github.com/SrGrace/AgenticAI) by SrGrace
- [Agents](https://www.kaggle.com/whitepaper-agents) by Google
- 

## Fine-Tuning
- [PEFT example notebooks](https://github.com/huggingface/peft/tree/main/examples) by Huggingface
- [LLM Fine-tuning tutorials](https://github.com/ashishpatel26/LLM-Finetuning) by ashishpatel26
- [Fine-tune Llama 3.1 Ultra-Efficiently with Unsloth](https://huggingface.co/blog/mlabonne/sft-llama3) by Maxime Labonne on ü§ó
- 

## Comprehensive LLM Code Repositories
- [LLM PlayLab](https://github.com/Sakil786/LLM-PlayLab) by Sakil786
  - This playlab encompasses a multitude of projects crafted through the utilization of Large Language Models.
- [Parlance Labs - Educational Resources](https://parlance-labs.com/education/) by parlance labs
  - Hear from practitioners on a wide range of topics on LLMs, including RAG, evaluation, applications, fine-tuning and prompt engineering.
-  

## LLMs
- [A Visual Guide to Mixture of Experts (MoE)](https://newsletter.maartengrootendorst.com/p/a-visual-guide-to-mixture-of-experts) by Maarten Grootendorst
- [üó£Ô∏è Large Language Model Course](https://github.com/mlabonne/llm-course) by Maxime Labonne & Pietro Monticone
- 
