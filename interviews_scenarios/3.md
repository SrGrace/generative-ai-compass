### 3. We have a 200MB vision model: 5 FPS on our device, high power draw. How do you get it into production without killing accuracy?
---

Here's how you answer:

Don't immediately jump to architecture changes.

You: "My primary strategy would be Quantization."

Interviewer: "Define it."

You: "Reducing the numerical precision of weights and activations - moving from 32-bit floats (FP32) to 16-bit (FP16), 8-bit (INT8), or even binary."

Interviewer: "And the benefits for our problem?"

You: "Immediate and significant:"
 - Memory: Halves (FP16), quarters (INT8) the model size. Critical for constrained devices.
 - Speed: Lower-precision operations are much faster on dedicated hardware. 2-4x for INT8.
 - Power: Less data movement, simpler arithmetic = less power.

Interviewer: "What about the accuracy hit?"

You: "That's the core challenge. The art is minimizing it. We have two main approaches:"
1. Post-Training Quantization (PTQ): 
 - Fastest to implement, no retraining. 
 - We calibrate the model with a small dataset to determine min/max ranges for activations and weights. 
 - Good for quick wins, but accuracy can suffer.

2. Quantization-Aware Training (QAT)
 - Typically achieves much higher accuracy, often matching FP32. 
 - We simulate quantization during training itself, so the model 'learns' to be robust to the precision reduction. 
 - It requires retraining, but the results are usually superior.

Interviewer: "Any other considerations for robust quantization?"

You: "Absolutely. Critical factors include:"
 - Calibration: Using a truly representative dataset for range determination.
 - Per-tensor vs. Per-channel: Deciding how granular the scaling factors are.
 - Hardware Alignment: Ensuring the quantization scheme aligns with the target device's accelerator capabilities (e.g., NVIDIA's TensorRT vs. Edge TPUs).
 - Mixed Precision: Keeping sensitive layers (like the final classifier) in higher precision if needed.

Interviewer: "Why is this important for an edge AI role?"

You: "It's the most effective way to bridge the gap between large, accurate cloud models and the tight constraints of embedded devices. It's crucial for real-world deployment."
