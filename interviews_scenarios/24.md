### We want to fine-tune an LLM sequentially on multiple domains without losing prior knowledge. How do you prevent catastrophic forgetting?
---

You: "Catastrophic forgetting happens when fine-tuning on new tasks overwrites critical weights learned from previous tasks. One effective solution is Elastic Weight Consolidation (EWC)."

Interviewer: "Explain EWC."

You: " EWC adds a regularization term to the loss that penalizes changes to parameters deemed important for previous tasks.
Mathematically, we compute the Fisher Information Matrix after training on task A to estimate parameter importance. Then, when training on task B, the loss becomes:

 EWC Total Loss = Loss_new_task + (λ / 2) * Σ_i F_i * (θ_i - θ_i*)^2
 
 where:
 - Loss_new_task -> standard loss for the current task
 - θ_i* -> parameter value after the previous task
 - F_i -> Fisher information for parameter i (importance measure)
 - λ -> hyperparameter balancing stability vs plasticity
 - Σ_i -> sum over all parameters

Interviewer: "How would you implement this at scale for a transformer?"

You: "Key steps:
1. Compute Fisher diagonals efficiently: Instead of full matrices, use diagonal approximation to scale to billions of parameters.
2. Checkpoint previous task weights: Store θ* after each task for the regularization term.
3. Dynamic λ scheduling: Adjust importance based on task similarity - higher λ for similar tasks to prevent interference, lower for unrelated tasks to allow plasticity.
4. Combine with adapters or LoRA: Fine-tuning only a subset of parameters reduces risk of forgetting and improves memory efficiency."

Interviewer: "What are potential limitations?"

You: "EWC assumes tasks are somewhat independent; if tasks are highly correlated, Fisher estimates can be noisy. It also adds compute overhead for storing and applying the regularization, especially across many sequential tasks."

Interviewer: "What about deployment?"

You: "Incremental task training with monitoring on previous-task validation metrics. If forgetting occurs, you can selectively freeze important layers or reweight the Fisher terms. Observability is key - track performance drift across all prior domains.”
