### We want an LLM to learn sequential tasks without forgetting previous ones. EWC (Elastic Weight Consolidation) works, but what other strategies would you consider?
---

You: "One of the most effective approaches is Replay-based Continual Learning. Instead of only relying on regularization, the model rehearses previous tasks using stored or generated examples during new-task training."

Interviewer: "Explain in more detail."

You: "Stepwise:
1. Memory buffer: Store a small, representative subset of previous task data. For LLMs, this could be a set of input-output pairs capturing key patterns.
2. Interleaved training: During training on a new task, mix batches from the current task with samples from the memory buffer. This maintains gradients aligned with prior knowledge.
3. Generative replay (optional): If storing raw data is infeasible, a smaller generative model can recreate pseudo-examples from past tasks. The main model trains on these along with new data.

This ensures the model retains prior knowledge while adapting to new tasks."

Interviewer: "How do you handle memory constraints?"

You: "Use selective sampling - prioritize examples that are most informative, such as edge cases or high-loss examples.
Combine with parameter-efficient fine-tuning like LoRA or adapters so only a subset of weights are updated. This reduces interference and memory overhead."

Interviewer: "What about task drift or distribution shift?"

You: "Track performance on previous-task validation sets. If drift occurs, increase rehearsal frequency or resample memory buffer.
Also, you can assign task-specific weights to balance influence between past and new tasks."

Interviewer: "And deployment at scale?"

You: "Replay can be incorporated into an incremental fine-tuning pipeline. Monitor cross-task metrics continuously.
For LLMs deployed in production, a lightweight memory buffer plus selective replay ensures stable outputs without retraining from scratch.‚Äù
