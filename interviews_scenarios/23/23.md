### We want to fine-tune a large language model on domain-specific documents. How would you approach it while avoiding catastrophic forgetting?
---

You: "Fine-tuning requires a balance between learning the new domain and retaining general capabilities. I’d start with data preparation and sampling strategies."

Interviewer: "Go on. What steps would you follow?"

You: "Stepwise:
1. Dataset curation: Ensure the domain-specific corpus is clean, representative, and diverse. Include counterexamples to avoid overfitting.
2. Adaptive fine-tuning: Use techniques like LoRA or adapters rather than full-parameter tuning. Keeps base capabilities intact while learning new patterns efficiently.
3. Regularization strategies: Implement dropout, weight decay, and embedding regularization to prevent catastrophic forgetting.
4. Validation and evaluation: Track domain-specific metrics (accuracy, F1, ROUGE) alongside general LLM capabilities to ensure balance.
5. Iterative checkpoints: Save intermediate models and evaluate incremental improvements to detect drift or forgetting early.

Interviewer: "How do you handle tokenization issues in a new domain?"

You: "I’d analyze the token distribution. If domain-specific terms are frequently split, consider adding them to the tokenizer vocabulary. This reduces out-of-vocabulary embeddings and improves model performance.

Interviewer: "Deployment concerns?"

You: "I'd monitor latency and memory footprint. Use model distillation or quantization if needed. Maintain fallback to base model in production for queries outside the domain. Observability and versioning are critical for safe rollout."
