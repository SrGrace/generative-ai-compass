### We need to predict product recommendations across billions of nodes. How do you scale a GNN?
---

You: "Naive GNNs struggle at scale because neighborhood aggregation grows with O(E). At this scale, we need sampling and partitioning strategies."

Interviewer: "Go on."

You: "Key strategies:
1. Neighbor sampling: Fix the number of neighbors per node to prevent memory explosion.
2. Mini-batch training: Process subgraphs instead of the full graph to avoid propagating across billions of nodes.
3. Graph partitioning: Shard the graph with minimal cross-partition edges to reduce communication overhead.

Interviewer: "What about inference latency?"

You: "Precompute embeddings for high-degree nodes to avoid repeated multi-hop computation. Use approximate nearest neighbor (ANN) search in the embedding space. Cache subgraph computations for queries that repeat frequently."

Interviewer: "Explain inductive capability in GNNs."

You: "Classic GNNs are transductive - they only work on nodes seen during training. Models like GraphSAGE or attention-based GNNs can embed unseen nodes on-the-fly, enabling real-time recommendations for new products or users."

Interviewer: "Nods. What are bottlenecks at 10B nodes?"

You: "
 - Memory: storing adjacency lists.
 - Communication: cross-partition messaging.
 - Compute: multi-hop aggregation.

To mitigate these: hybrid CPU-GPU pipelines, sampling-aware batching, and pre-fusion of embeddings can scale training and inference efficiently across Amazon-scale graphs."
