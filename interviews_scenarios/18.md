### Everyone says tokenization just splits text. What’s really happening?
---

You: "Tokenization converts text into discrete units the model can process: words, subwords, or bytes. For instance, 'transformers' might become ['trans', 'form', 'ers']. Each token maps to a vector embedding."

Interviewer: "Why not just split on spaces?"

You: "Two reasons: efficiency and generalization - 
 - Space-splitting creates massive vocabularies. 
 - Subword tokenization reduces vocabulary size, handles rare words gracefully, and preserves meaning patterns. 
 - If the model never saw 'transformable', it can still interpret it via 'trans' + 'able'."

Interviewer: "How does this affect inference?"

You:
 - Sequence length: Token count affects memory and context.
 - Embedding lookup: Smaller vocab = faster computation.
 - Output quality: Bad tokenization can break generation or create hallucinations.

Interviewer: "So it’s more than just splitting words?"

You: "Exactly. Tokenization is the interface between natural language and math. Poor tokenization = poor embeddings = poor output. Choosing the right tokenizer is a foundational design decision for enterprise LLMs."
