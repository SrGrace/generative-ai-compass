### A client has a RAG-based system that isn't giving accurate results. After investigation, you find the retrieval system is failing. How would you improve it?
---

This is how you answer...

You: "At Meta scale, retrieval quality is everything. A powerful LLM cannot compensate for irrelevant or incomplete context. The first step is understanding why the retrieval is failing."

Interviewer: "Go on. What would you do concretely?"

You: "I'd approach it methodically:
1. Audit the retrieval pipeline:
   - Examine embeddings, similarity metrics, and indexes.
   - Run benchmark queries with expected relevant documents.
   - Purpose: Identify whether poor recall, weak embeddings, or mismatched similarity measures are the bottleneck.
2. Upgrade or fine-tune embeddings:
   - Generic embeddings miss domain nuances.
   - Fine-tune embeddings on domain-specific data or leverage embeddings optimized for semantic search.
   - Benefit: Ensures top-K retrieved documents are truly relevant.
3. Optimize indexing and search:
   - ANN indexes improve speed but can hurt recall.
   - Tune index parameters, consider hybrid search (dense + sparse), or use brute-force search for critical queries.
   - Outcome: Improves the likelihood of retrieving correct documents.
4. Feedback-driven improvement:
   - Collect relevance feedback from users or automated evaluation.
   - Track metrics like recall@k, MRR, and nDCG.
   - Use feedback to refine embeddings and query strategies iteratively.
5. Refine query representations:
   - Sometimes the query is misaligned with embeddings.
   - Use prompt engineering, rephrasing, or query expansion.
   - Result: Better alignment between queries and documents, boosting retrieval accuracy.

Interviewer: "Sounds thorough. How do you deploy this safely without breaking the system?"

You: "Incremental rollout with benchmarked evaluation, logging top-K results, and monitoring similarity scores ensures improvements donâ€™t regress. Observability is key - if retrieval gets better, the LLM output improves almost automatically."
