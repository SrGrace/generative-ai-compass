1. [What’s the real bottleneck in LLM serving throughput - and how does PagedAttention fix it?](https://github.com/SrGrace/generative-ai-compass/blob/main/interviews_scenarios/1.md) 
2. [Your embedding lookups are painfully slow. What’s going on, and how do you fix it?](https://github.com/SrGrace/generative-ai-compass/blob/main/interviews_scenarios/2.md)
3. [We have a 200MB vision model: 5 FPS on our device, high power draw. How do you get it into production without killing accuracy?](https://github.com/SrGrace/generative-ai-compass/blob/main/interviews_scenarios/3.md)
4. [Vanilla Gradient Descent is quaint for huge models. What's your go-to optimizer when you're optimizing for scale, and why?](https://github.com/SrGrace/generative-ai-compass/blob/main/interviews_scenarios/4.md)
5. [Our team needs to build RAG over 10 million documents. Which vector database would you recommend and, crucially, why?](https://github.com/SrGrace/generative-ai-compass/blob/main/interviews_scenarios/5.md)
6. [We're building autonomous agents for complex, multi-step reasoning over extended periods. How do you tackle the long-term memory problem beyond just increasing context window size?](https://github.com/SrGrace/generative-ai-compass/blob/main/interviews_scenarios/6.md)
7. [We're serving billions of LLM inference requests daily across various applications. What caching strategies are you thinking about to maximize throughput and minimize latency?](https://github.com/SrGrace/generative-ai-compass/blob/main/interviews_scenarios/7.md)
8. [We're exploring next-generation AI agents for highly complex, open-ended tasks - think scientific discovery or strategic planning. What architectural paradigms go beyond simple 'plan-and-execute' to enable deep, adaptive reasoning?](https://github.com/SrGrace/generative-ai-compass/blob/main/interviews_scenarios/8.md)
9. [Why does chain-of-thought (CoT) reasoning improve LLM performance on complex tasks - but sometimes fails catastrophically?](https://github.com/SrGrace/generative-ai-compass/blob/main/interviews_scenarios/9.md)
10. [How does speculative decoding speed up LLM inference?](https://github.com/SrGrace/generative-ai-compass/blob/main/interviews_scenarios/10.md)
11. [Why do tool-using agents often hallucinate function calls even when APIs are available?](https://github.com/SrGrace/generative-ai-compass/blob/main/interviews_scenarios/11.md)
12. [Why do reflection-based agents degrade over long horizons?](https://github.com/SrGrace/generative-ai-compass/blob/main/interviews_scenarios/12.md)
13. [We're seeing incredible adoption of our new internal LLM-powered assistant, but inference costs are spiraling. How would you approach optimizing the inference pipeline for a model like Llama 3 8B, handling thousands of requests per second?](https://github.com/SrGrace/generative-ai-compass/blob/main/interviews_scenarios/13.md)
14. [](https://github.com/SrGrace/generative-ai-compass/blob/main/interviews_scenarios/14.md)
15. [](https://github.com/SrGrace/generative-ai-compass/blob/main/interviews_scenarios/15.md)
16. [](https://github.com/SrGrace/generative-ai-compass/blob/main/interviews_scenarios/16.md)
17. [](https://github.com/SrGrace/generative-ai-compass/blob/main/interviews_scenarios/17.md)
18. [](https://github.com/SrGrace/generative-ai-compass/blob/main/interviews_scenarios/18.md)
19. [](https://github.com/SrGrace/generative-ai-compass/blob/main/interviews_scenarios/19.md)
20. [](https://github.com/SrGrace/generative-ai-compass/blob/main/interviews_scenarios/20.md)

