1. [What’s the real bottleneck in LLM serving throughput - and how does PagedAttention fix it?](https://github.com/SrGrace/generative-ai-compass/blob/main/interviews_scenarios/1.md) 
2. [Your embedding lookups are painfully slow. What’s going on, and how do you fix it?](https://github.com/SrGrace/generative-ai-compass/blob/main/interviews_scenarios/2.md)
3. [We have a 200MB vision model: 5 FPS on our device, high power draw. How do you get it into production without killing accuracy?](https://github.com/SrGrace/generative-ai-compass/blob/main/interviews_scenarios/3.md)
4. [Vanilla Gradient Descent is quaint for huge models. What's your go-to optimizer when you're optimizing for scale, and why?](https://github.com/SrGrace/generative-ai-compass/blob/main/interviews_scenarios/4.md)
5. [Our team needs to build RAG over 10 million documents. Which vector database would you recommend and, crucially, why?](https://github.com/SrGrace/generative-ai-compass/blob/main/interviews_scenarios/5.md)
6. [We're building autonomous agents for complex, multi-step reasoning over extended periods. How do you tackle the long-term memory problem beyond just increasing context window size?](https://github.com/SrGrace/generative-ai-compass/blob/main/interviews_scenarios/6.md)
7. [We're serving billions of LLM inference requests daily across various applications. What caching strategies are you thinking about to maximize throughput and minimize latency?](https://github.com/SrGrace/generative-ai-compass/blob/main/interviews_scenarios/7.md)
8. [](https://github.com/SrGrace/generative-ai-compass/blob/main/interviews_scenarios/8.md)
9. [](https://github.com/SrGrace/generative-ai-compass/blob/main/interviews_scenarios/9.md)
10. [](https://github.com/SrGrace/generative-ai-compass/blob/main/interviews_scenarios/10.md)

