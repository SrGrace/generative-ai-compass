### Beyond increasing context window size, what are the most effective architectural and data-centric strategies you'd employ to enable an LLM to effectively reason over gigabytes of domain-specific, unstructured data?
---

You don't just mention RAG and summarization.

You reply:
Multi-Granular Hierarchical RAG: 
 - Instead of flat chunking, create a multi-layered index. 
 - First, coarse-grain chunks for high-level semantic retrieval, then fine-grain chunks within the relevant coarse sections for detailed extraction. 
 - This prevents semantic dilution and improves recall precision.

Recursive Self-Refinement with Scratchpad: 
 - The LLM internally generates a 'scratchpad' or 'working memory' as it processes. 
 - For extremely long documents, it recursively summarizes or extracts key facts into this scratchpad, 
 - then re-processes the original document against its refined internal state, progressively building a compact, high-fidelity understanding.

Graph-Augmented Reasoning: 
 - Extract entities and relationships from the unstructured data to build a knowledge graph. 
 - The LLM then reasons over this structured graph for factual consistency and inferential steps, rather than solely on raw text, preventing disconnected hallucination.

The interviewer probes:
"How do you handle the temporal consistency and versioning of this external knowledge when the underlying data sources are constantly updating?"

You explain:
Event-Driven Indexing with Delta Updates: 
 - Instead of full re-indexing, listen for change data capture (CDC) events from source systems. 
 - Apply incremental updates to the vector index for only the changed documents, propagating invalidated cached RAG responses.

Time-Travel RAG Querying: 
 - Implement an index that can snapshot knowledge at specific timestamps.
 - This allows the LLM to answer questions "as of" a certain date, critical for auditing, legal, or financial applications where historical accuracy matters.

Probabilistic Cache Invalidation: 
 - For less critical applications, use a probabilistic approach. 
 - When a source document is updated, invalidate a portion of related RAG responses in cache based on estimated staleness, balancing freshness with computational cost.
