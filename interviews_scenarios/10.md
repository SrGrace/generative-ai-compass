### How does speculative decoding speed up LLM inference?
---

You respond: "It uses a smaller draft model to predict multiple tokens ahead. The main model verifies predictions in parallel. Correct predictions allow skipping full forward passes for several tokens."

Interviewer: "What if the draft predictions fail?"

You:
 - Rollback triggers recomputation for mismatched tokens
 - KV cache partial updates must be recomputed -> memory and compute spike
 - Kernel-level synchronization adds latency

Interviewer: "Can you quantify the benefits?"

You: Sure - 
 - For a 30B parameter model, context 4,096 tokens, batch 64
 - Draft predicts 4 tokens ahead, 80% match -> 3,200 tokens skipped per batch
 - Latency drops ~400 ms -> ~180 ms (~2.2x speedup)
 - 20% mismatch adds ~50 ms -> net gain remains significant

Interviewer: "Memory implications?"

You:
 - Main KV cache ~28 GB FP16
 - Draft model cache adds ~2â€“3 GB
 - Total ~30 GB -> fits comfortably on 80GB GPU
 - Freed memory allows larger batch size or longer context

Interviewer: "How do you optimize speculative decoding?"

You:
 1. Asynchronous verification kernels -> reduce idle GPU cycles
 2. Dynamic speculation window -> adjust ahead-predict size based on draft entropy
 3. Chunked KV updates -> minimize recomputation memory overhead
 4. Mixed precision draft -> FP16 or INT8 to reduce memory footprint

Interviewer: "Any trade-offs to watch?"

You:
 - High mismatch rates can negate gains
 - Extra kernels may increase code complexity
 - Need careful memory orchestration to avoid exceeding GPU capacity

Speculative decoding is risk-managed acceleration. Proper KV cache handling, prediction verification, and kernel orchestration are key to real-world throughput gains while maintaining memory efficiency.
