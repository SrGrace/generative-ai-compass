### Why do reflection-based agents degrade over long horizons?
---

You start:
 - Reflection loops let LLMs critique their own reasoning, but every reflection adds meta-context - more tokens, more uncertainty propagation.
 - Over many iterations, the agent starts compounding its own summaries, introducing drift - the "self-delusion" effect.

You explain:
 - Each reflection compresses prior reasoning.
 - Errors get amplified, not corrected.
 - The context window becomes polluted with meta-data instead of signal.

Then you suggest the fix:
 - Use episodic memory for reflection checkpoints
 - Keep structured reasoning traces separate from summaries
 - Apply consistency scoring between reflection and original output

You close:
"Reflection improves reasoning but only if you constrain memory growth.
 Otherwise, the agent forgets whatâ€™s real."
