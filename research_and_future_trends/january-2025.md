## Week 1/4
| Title | Summary | Topics |
| --- | --- | --- |
| [CODEELO: Benchmarking Competition-level Code Generation of LLMs with Human-comparable Elo Ratings](https://arxiv.org/pdf/2501.01257) | For years, we've wondered, how do coding LLMs really stack up against human developers? <br><br> Most benchmarks have fallen short, offering unrealistic test cases or missing the nuances of real-world programming challenges. <br><br> CODEELO: a new benchmark - addresses this by integrating directly with CodeForces, the global hub for competitive programming. 🌐 <br><br> 🔑 Key Insights: <br> &nbsp; 🔹 Real Problems, Real Stakes: It uses CodeForces problems, test cases, and conditions - just like human participants face. <br> &nbsp; 🔹 Elo Ratings for LLMs: CODEELO assigns ratings comparable to human players, offering unprecedented insight into LLM capabilities. <br> &nbsp; 🥇 Only two models, OpenAI’s o1-mini (1578) and QwQ-32B-Preview (1261), scored above 1200 Elo. Most others? Stuck in the bottom 20% of human participants. <br> &nbsp; 🧠 Strengths and Weaknesses: Top models excel in math but falter with dynamic programming and complex data structures. | Coding Benchmark |
| [Metadata Conditioning Accelerates Language Model Pre-training](https://arxiv.org/pdf/2501.01956) | This new research introduces MeCo (Metadata Conditioning then Cooldown), a method that brings simplicity, efficiency, and control to language model pre-training.  <br><br> 🔑 Here’s why this approach is so compelling: <br> &nbsp; ✅ Efficient Pre-training: MeCo uses metadata like URLs (e.g., en.wikipedia.org) during training to provide contextual cues, enabling the model to group and learn from documents more effectively. This reduces training data requirements by 33% - a 1.6B model achieves standard pre-training performance with fewer resources. <br> &nbsp; ✅ Steerable AI: By conditioning inference on metadata, even fabricated ones (e.g., factquizmaster.com), we can influence model behaviour. This reduces toxic outputs and boosts task-specific performance. <br> &nbsp; ✅ Universal Applicability: Compatible across model scales (600M to 8B parameters) and training datase | LLM Pre-Training |
| [AdaSkip: Adaptive Sublayer Skipping for Accelerating Long-Context LLM Inference](https://arxiv.org/pdf/2501.02336) | As LLMs tackle longer and longer texts (some now handling 1M+ tokens!), making them run efficiently becomes crucial. <br><br> This recent research proposes AdaSkip - it offers a practical solution without the complex training or fine-tuning needed by other methods - a smart way to accelerate LLMs when dealing with long text! <br><br> 🔑 Key highlights: <br> &nbsp; 🔹 Unlike previous approaches that skip entire layers, AdaSkip intelligently identifies and skips less important sub-components, maintaining model quality while boosting speed <br> &nbsp; 🔹 Works during both "prefilling" (initial processing) and "decoding" (generation) phases <br> &nbsp; 🔹 Adapts automatically to different models and contexts - no one-size-fits-all approach!  <br> &nbsp; 🔹 Achieved impressive results across multiple models (LLaMA, InternLM, Vicuna) and tasks | LLM Inference |
| [RAG-Check: Evaluating Multimodal Retrieval Augmented Generation Performance](https://arxiv.org/pdf/2501.03995) | Combining text, images, and more, multimodal RAG unlocks incredible potential - but it also opens the door to new challenges. Chief among them? Hallucination. <br><br> 🚨 Hallucinations in multimodal RAG systems stem from three layers:  <br> &nbsp;  1️⃣ Selection-hallucination: When irrelevant data (e.g., images, documents) are retrieved. <br> &nbsp;  2️⃣ Context-generation-hallucination: When vision-LMs generate inaccurate textual descriptions of visual data. <br> &nbsp;  3️⃣ Response-generation-hallucination: When LLMs produce irrelevant or incorrect responses despite good context. <br><br> 🧪 RAG-Check: This new Framework tackles these issues head-on by introducing two novel metrics to evaluate reliability: <br> &nbsp;  🔍 Relevance Score (RS): How relevant is the retrieved data? <br> &nbsp;  ✔️ Correctness Score (CS): How accurate are the generated responses? <br><br> 🎯 Key results? <br> &nbsp;  🔹 Both models achieve 88% accuracy on test data. <br> &nbsp;  🔹 The CS metric aligns with human evaluations 91% of the time, and RS outperforms traditional measures by over 20%! <br><br> 🔖 As multimodal RAG becomes mainstream, the conversation around hallucination detection and reliability metrics will only grow louder. Papers like this could define the standards for what comes next. | Multimodal RAG Evaluation |
| [Understanding Before Reasoning: Enhancing Chain-of-Thought with Iterative Summarization Pre-Prompting](https://arxiv.org/pdf/2501.04341) | This recent paper introduces Iterative Summarization Pre-Prompting (ISP2), a novel pre-prompting technique designed to enhance the reasoning capabilities of LLMs by refining the input context before reasoning starts. <br><br> Traditional Chain-of-Thought (CoT) prompting focuses on breaking down reasoning into steps but often struggles when essential information is missing or implicit.  <br><br> 💡 ISP2 tackles this by:  <br> &nbsp; 1️⃣ Extracting entities and key descriptions to form "information pairs." <br> &nbsp;  2️⃣ Iteratively merging and refining these pairs using a reliability rating system. <br> &nbsp;  3️⃣ Feeding the refined context back to the LLM for better reasoning. <br><br> 📈 The Results: <br> &nbsp;  🔹 A 7.1% improvement in reasoning tasks when combined with CoT. <br> &nbsp;  🔹 Exceptional plug-and-play compatibility across models  <br> &nbsp;  🔹 Significant advancements in commonsense and reasoning benchmarks. <br><br> 🤖 By focusing on understanding the problem space through iterative summarization, ISP2 enables LLMs to generate more accurate, contextually rich answers.  | Pre-Prompting |
| [Agent Laboratory: Using LLM Agents as Research Assistants](https://arxiv.org/pdf/2501.04227) | This research proposes a framework called Agent Laboratory. It uses Agents to guide research through three key phases:  <br> &nbsp; 🔹 literature review,  <br> &nbsp; 🔹 experimentation, and  <br> &nbsp; 🔹 report writing.  <br><br> 🔑 Key Insights: <br> &nbsp; 1️⃣ High-Quality Outputs: The framework's best-performing model, o1-preview, not only delivered exceptional research reports but also generated machine learning code that achieved state-of-the-art performance. 🎯 <br> &nbsp;  2️⃣ Cost Efficiency: Research expenses dropped dramatically - achieving an 84% cost reduction compared to traditional methods. <br> &nbsp;  3️⃣ Human-AI Collaboration: Human feedback at every stage significantly elevated the quality of outputs, showcasing the importance of collaboration over complete automation. <br><br> 🧩 While the results are promising, challenges like hallucinated results and technical limitations highlight the need for further refinement. But with frameworks like this, the future of AI-assisted scientific discovery is undeniably bright. | Research Agents |
| []() |  |  |


## Week 2/4
| Title | Summary | Topics |
| --- | --- | --- |
| [Imagine while Reasoning in Space: Multimodal Visualization-of-Thought](https://arxiv.org/pdf/2501.07542) | This recent paper introduces a novel approach to reasoning that bridges text and visuals seamlessly! <br><br> Understanding complex problems often requires more than just words - it demands visualization.  <br><br> 🌟 Inspired by how humans process information, this Multimodal Visualization-of-Thought (MVoT) paradigm takes AI reasoning to the next level by combining verbal and visual thinking. <br><br> Instead of relying solely on traditional text-based reasoning methods like Chain-of-Thought (CoT), MVoT allows AI to generate image visualizations of reasoning processes. This approach not only enhances accuracy but also provides clearer, more interpretable insights - especially in tasks like spatial navigation and dynamic problem-solving. <br><br> 📊 Key Highlights: <br> &nbsp;  🔹 20% performance boost in challenging spatial reasoning scenarios compared to CoT. <br> &nbsp;  🔹 Introduction of a token discrepancy loss, improving visual coherence and fidelity. <br> &nbsp;  🔹 MVoT excels in interpreting and solving problems where CoT struggles, like navigating intricate environments or predicting dynamic outcomes. <br><br> The possibilities this opens for AI applications in fields like robotics, education, and healthcare are immense!  <br><br> Imagine AI assisting with clear, visual reasoning steps for tasks like urban planning or disaster management. | Multimodal Prompting |
| [Lifelong Learning of Large Language Model based Agents: A Roadmap](https://arxiv.org/pdf/2501.07278) | This recent paper lays out a compelling roadmap for embedding lifelong learning into LLM-based agents. Here’s what stands out: <br><br> ♎ Core Pillars for Lifelong LLM Agents: <br> &nbsp; 1️⃣ Perception Module: Integrates multimodal inputs (text, images, etc.) to understand the environment. <br> &nbsp; 2️⃣ Memory Module: Stores evolving knowledge while avoiding catastrophic forgetting. <br> &nbsp; 3️⃣ Action Module: Facilitates interactions and decision-making to adapt in real time. <br><br> 💡 Key Challenges Addressed: <br> &nbsp; 🔹 Overcoming catastrophic forgetting 🧠 <br> &nbsp; 🔹 Balancing adaptability and knowledge retention <br> &nbsp; 🔹 Managing multimodal information effectively <br><br> 🌍 It has real world potential - From household assistants to complex decision-support systems, lifelong learning LLM agents are poised to excel in dynamic scenarios, enabling applications like gaming, autonomous systems, and interactive tools. | Agents Roadmap |
| [Governing AI Agents](https://arxiv.org/pdf/2501.07913) | As AI evolves from static tools to autonomous agents capable of independent decision-making, we face a growing challenge - how do we ensure these agents act in alignment with human values? <br><br> Unlike language models, AI agents act autonomously, taking complex actions, such as booking trips, managing finances, or even negotiating contracts. This autonomy raises critical questions about accountability, ethics, and liability. <br><br> This recent paper explores this shift and presents a roadmap for governing AI agents.  <br><br> ⚖️ Three Governance Challenges <br> &nbsp;  1️⃣ Information Asymmetry: Agents often operate with knowledge inaccessible to their human principals, creating trust issues. <br> &nbsp;  2️⃣ Authority and Loyalty: How much discretion should agents have? How do we ensure they prioritize user interests? <br> &nbsp;  3️⃣ Liability: Who is responsible when an agent makes an error or causes harm? <br><br> 🌍 Proposed Solutions - the paper suggests a three-pronged strategy: <br> &nbsp;  🔹 Inclusivity: Ensure agents align not just with user goals but also with broader societal values. <br> &nbsp;  🔹 Visibility: Enhance transparency in agent operations to predict and mitigate risks. <br> &nbsp;  🔹 Liability: Develop new legal frameworks to assign responsibility fairly among developers, operators, and users. <br><br> As AI agents become integral to our daily lives and businesses, these governance principles will shape their impact. The risks are high - autonomous hacking, algorithmic bias, and even ethical breaches but the potential for innovation and efficiency is immense. | Agents' Governance |
| [Towards Large Reasoning Models: A Survey on Scaling LLM Reasoning Capabilities](https://arxiv.org/pdf/2501.09686) | 🧠 As AI evolves, so does its capacity for reasoning but how far can we push the boundaries of logic and learning in LLMs? <br><br> This new survey provides a roadmap toward Large Reasoning Models (LRMs), designed to expand the reasoning capabilities of AI. <br><br> Here's a glimpse into the key findings: <br><br> 💡 Reasoning through Reinforcement <br> &nbsp;  🔹 By applying reinforcement learning techniques, LLMs are mastering step-by-step reasoning processes, reducing reliance on expensive human annotations. <br> &nbsp;  🔹 Innovative Process Reward Models (PRMs) assign feedback at each reasoning step, improving accuracy in tasks like mathematical problem-solving and logical deduction. <br><br> 🚀 Scaling for Smarter AI <br> &nbsp;  🔹 Combining train-time scaling (more reasoning data during training) and test-time scaling (deliberate reasoning during inference) has led to unprecedented gains in reasoning accuracy. <br> &nbsp;  🔹 The introduction of advanced prompting techniques, like Tree-of-Thought (ToT) and Chain-of-Thought (CoT), empowers LLMs to tackle multi-step, abstract tasks effectively. <br><br> With LRMs, the vision is to transition AI from conversational agents to reasoning powerhouses capable of contributing to domains like scientific discovery, programming, and decision-making. <br><br> The survey also highlights cutting-edge projects like OpenAI’s O1 series, which achieved doctorate-level reasoning abilities, and promising open-source initiatives paving the way for accessible large reasoning models. | Large Reasoning Models |
| [TRANSFORMER^2 : SELF-ADAPTIVE LLMS](https://arxiv.org/pdf/2501.06252) | Introduces Transformer^2, a novel self-adaptation framework that adapts LLMs for unseen tasks in real-time by selectively adjusting singular components of their weight matrices;  <br><br> it’s built with two key phases:  <br> &nbsp; 1) a dispatch system that analyzes and identifies the properties of the incoming task, and  <br> &nbsp; 2) a step that combines "expert" vectors (trained via reinforcement learning) to create task-specific behaviors;  <br><br> claims to be more efficient than LoRA with fewer parameters and can works across different LLM architectures. | LLMs |
| [MiniMax-01: Scaling Foundation Models with Lightning Attention](https://arxiv.org/pdf/2501.08313) | Introduces a new series of models that integrate Mixture-of-Experts; introduces a model with 32 experts and 456B parameters, and 45.9B are activated for each token;  <br><br> claims match the performance of state-of-the-art models like GPT-4o and Claude-3.5-Sonnet while offering a 20-32x longer context window;  <br><br> it can handle context windows of up to 4 million tokens;  <br><br> it integrates linear attention with optimized hardware utilization which enhances the efficiency and scalability of the LLM;  <br><br> there is also a vision model called MiniMax-VL-01 built through continued training with 512 billion vision-language tokens. | Foundation Models |
| []() |  |  |


## Week 3/4
| Title | Summary | Topics |
| --- | --- | --- |
| [Evolving Deeper LLM Thinking](https://arxiv.org/pdf/2501.09891) | This recent paper by researchers at Google DeepMind proposes a new approach - "Mind Evolution". It takes LLMs beyond static reasoning to achieve dynamic, iterative problem-solving. <br><br> Here’s a quick dive into what makes it worth a while: <br><br> 🌟 Inspired by Nature: <br>The method applies genetic algorithms - concepts of evolution like selection, crossover, and mutation to refine solutions iteratively. Think of it as AI brainstorming, improving its responses generation by generation. 🤔  <br><br> ⚙️ How It Works: <br> &nbsp;  🔹 Starts with diverse solution candidates for a problem. <br> &nbsp;  🔹 Uses an LLM to recombine and refine them, guided by feedback from an evaluator. <br> &nbsp;  🔹 Continues refining until it reaches the optimal answer or hits compute limits. <br><br> 📈 Performance Gains: <br> &nbsp;  🔹 Achieved 95.6% success in the TravelPlanner benchmark, compared to 55.6% using traditional Best-of-N strategies. <br> &nbsp;  🔹 Tackled natural language planning challenges like trip planning, meeting scheduling, and even creative tasks like embedding hidden messages in poetry! <br><br> 💡 Unlike conventional methods, Mind Evolution thrives in unstructured, natural language spaces, making it ingenious for tasks where formal problem-solving frameworks don’t exist - like planning complex travel itineraries, crafting poetic steganography etc. | LLM Reasoning |
| [DeepSeek-R1](https://github.com/deepseek-ai/DeepSeek-R1/blob/main/DeepSeek_R1.pdf) | 1️⃣ DeepSeek-R1-Zero: <br> &nbsp;  🔹 Trained purely with RL, this model developed advanced reasoning behaviours like self-verification, reflection, and generating long reasoning chains - all without supervised guidance. <br> &nbsp;  🔹 This could be the first open and transparent demonstration that RL alone can unlock reasoning capabilities in LLMs. <br><br> 2️⃣ DeepSeek-R1: <br> &nbsp;  🔹 By incorporating a small amount of cold-start SFT data, DeepSeek-R1 achieved performance comparable to OpenAI’s o1-1217 model across a wide range of reasoning tasks. <br> &nbsp;  🔹 Lesson learned? While SFT isn’t essential, it can be a valuable accelerator. <br><br> 3️⃣ Distilling Reasoning to Smaller Models: <br> &nbsp;  🔹 The team distilled the capabilities of DeepSeek-R1 into smaller, efficient models, like DeepSeek-R1-Distill-Qwen-14B, which significantly outperformed the open-source QwQ-32B-Preview. <br> &nbsp;  🔹 This shows how much of a large model’s reasoning power can be preserved in smaller versions - a big win for accessibility and efficiency. <br><br> 📊 Impressive Performance Highlights: <br> &nbsp;  🔹 71% accuracy on the AIME benchmark (DeepSeek-R1-Zero), starting from just 15.6%! <br> &nbsp;  🔹 Achieved expert-level performance in reasoning-intensive tasks like math, coding, and logic. <br> &nbsp;  🔹 Outperformed benchmarks like OpenAI-o1-mini on tasks such as MATH-500 and GPQA Diamond. <br><br> ☘️ What Sets DeepSeek Apart? <br> &nbsp;  🔹 Transparency: The paper openly details not just the successes but also the failures - something rare but incredibly valuable in AI research. <br> &nbsp;  🔹 Community-first Approach: They released their models under an MIT license, making this work freely accessible to researchers and developers worldwide - a big gift to the community. | LLM Reasoning |
| [Adaptive Retrieval without Self-Knowledge? Bringing Uncertainty Back Home](https://arxiv.org/pdf/2501.12835v1) | RAG has become a go-to method for improving accuracy and reducing hallucinations in LLMs. But it’s not without its challenges - high computational costs and the risk of introducing irrelevant information. <br><br> This recent study analysed 35 adaptive retrieval methods, including 8 new approaches and 27 uncertainty estimation techniques, across 6 datasets using 10 metrics. The findings shed light on a promising direction: <br><br> 🧠 Key Insights: <br><br> 1️⃣ Efficiency Without Sacrificing Performance: <br> &nbsp; 🔹 Uncertainty estimation techniques often matched the QA performance of complex retrieval pipelines while being significantly more efficient. <br> &nbsp; 🔹 These methods leverage the LLM’s self-knowledge to determine when retrieval is truly necessary, minimizing unnecessary calls. <br><br> 2️⃣ Balancing Trade-Offs: <br> &nbsp; 🔹 Techniques like Mean Entropy and EigValLaplacian excelled in balancing performance, efficiency, and self-knowledge, showing that simpler approaches can often deliver optimal results. <br><br> 3️⃣ RAG-Level Outcomes, Lower Costs: <br> &nbsp; 🔹 By combining LLM self-awareness with uncertainty estimation, models can achieve results comparable to RAG without the added computational burden. <br><br> The Bottom Line - Sometimes, the best solutions aren’t about adding more complexity, they’re about making better decisions. Uncertainty estimation shows how simple adjustments can lead to meaningful improvements in both performance and cost-efficiency. | Retrieval |
| [Hallucinations Can Improve Large Language Models in Drug Discovery](https://arxiv.org/pdf/2501.13824v1) | Hallucinations are usually seen as a downside for LLMs. Yet, as Andrej Karpathy once suggested, hallucination could be an LLM’s greatest feature. But is there evidence to back that up? <br><br> This recent study says YES.  <br><br> It demonstrates that hallucinated text can actually improve LLM performance in drug discovery tasks, offering a unique twist to how we think about AI creativity. <br><br> 🔬 Key Findings: <br> &nbsp;  🔹 Better Accuracy: Llama-3.1-8B achieved an 18.35% gain in ROC-AUC compared to prompts without hallucination, highlighting significant improvements in molecule property prediction. <br> &nbsp;  🔹 GPT-4o Leads the Pack: Hallucinations generated by GPT-4o consistently provided the greatest performance boost across all tested models. <br> &nbsp;  🔹 Creativity Meets Utility: Hallucinated text often introduces unrelated but meaningful context, helping LLMs generate more insightful predictions. <br><br> 🧪 How It Works: <br> &nbsp;  🔹 The study used LLMs to describe molecules based on SMILES (🤔 a compact way to represent a chemical's structure using a line of symbols) strings. These hallucinated descriptions - creative, sometimes incorrect insights were added to input prompts. <br> &nbsp;  🔹 Evaluated on five datasets, the hallucinated prompts led to better predictions in tasks like identifying toxicity, blood-brain barrier permeability, and antiviral properties. <br><br> 💡This study challenges the notion that hallucinations are always bad. In fact, it opens the door to strategic hallucination, leveraging AI’s creative outputs to accelerate innovation in fields like pharmaceuticals. | LLMs Hallucinations |
| []() |  |  |
| []() |  |  |


## Week 4/4
| Title | Summary | Topics |
| --- | --- | --- |
| [Chain of Agents: Large Language Models Collaborating on Long-Context Tasks](https://openreview.net/pdf?id=LuCLf4BJsr) | 🔑 Key Insights from the paper: <br><br> 🧠 How it works: <br> &nbsp;  🔹 Text Splitting: CoA divides long inputs into smaller chunks, assigning each to a worker agent. <br> &nbsp;  🔹 Collaborative Communication: Worker agents process their chunks sequentially, passing key information to the next.<br> &nbsp; 🔹 Manager Agent: Finally, a manager agent synthesizes all contributions into a coherent final response. <br><br> 💡 Why it’s better: <br> &nbsp;  🔹 Unlike input reduction (which risks missing critical context) or window extension (which struggles with focus), CoA keeps agents focused and collaborative. <br> &nbsp;  🔹 It avoids the “lost-in-the-middle” issue while maintaining efficiency, with time complexity reduced to O(nk). <br><br> 📊 Results: <br> &nbsp;  🔹 Outperforms Traditional Methods: CoA beat existing approaches like RAG and Full-Context models by up to 10% on tasks like question answering and summarization. <br> &nbsp;  🔹 Excels with Longer Inputs: For texts over 400k tokens, CoA showed up to 100% improvement over baseline methods. <br> &nbsp;  🔹 Tested across 9 datasets, CoA proved robust on tasks like QA, summarization, and even code completion. <br><br> CoA works seamlessly across various LLMs, including PaLM 2, Gemini, and Claude 3. It’s training-free and adaptable, making it a cost-effective option for processing long and complex contexts. | Chain of Agents |
| [MASTER: A Multi-Agent System with LLM Specialized MCTS](https://arxiv.org/pdf/2501.14304) | Huawei’s MASTER (Multi-Agent System with Tactical Execution and Reasoning) framework introduces a novel way of combining multi-agent collaboration with an adaptation of Monte Carlo Tree Search (MCTS) for strategic planning and reasoning with LLMs.<br><br> 🔑 Key Insights from the paper:<br><br> Traditional MCTS methods in AI face two major hurdles: <br> &nbsp;  1️⃣ Dependence on Objective Rewards: Tasks like question answering often lack a clear ground truth, making evaluation difficult. <br> &nbsp;  2️⃣ High Computational Costs: MCTS requires numerous simulations for every decision, leading to inefficiencies.<br><br> ☘️ How MASTER Works to solve these challenges - It takes a unique approach by optimizing multi-agent collaboration through: <br> &nbsp;  🔹 Simulation-Free Decision-Making: Instead of traditional simulations, it uses LLMs for self-evaluation, significantly reducing computational demands. <br> &nbsp;  🔹 Dynamic Agent Recruitment: The number of agents adjusts based on task complexity, ensuring resource efficiency. <br> &nbsp;  🔹 Confidence-Weighted Rewards: Incorporating LLM confidence levels into the reward system balances exploration and exploitation for better outcomes - Interesting!<br><br> 📊 Results: <br> &nbsp;  🔹 HotpotQA (Multi-hop QA): Achieved 76% accuracy, improving on prior methods. <br> &nbsp;  🔹 WebShop (E-commerce Decision-Making): Reached 80% accuracy, a significant improvement in task performance. <br> &nbsp;  🔹 MBPP (Programming Tasks): Delivered 91% accuracy, demonstrating strong reasoning in coding challenges.<br><br> MASTER demonstrates how multi-agent systems can collaborate efficiently to address complex tasks across diverse domains, including decision-making, programming, and reasoning. Its training-free design and adaptability make it compatible with various LLMs, providing a flexible solution for real-world applications. | Agentic AI |
| [Chain-of-Retrieval Augmented Generation](https://arxiv.org/pdf/2501.14342) | This recent paper by Microsoft and Renmin University has introduced CoRAG (Chain-of-Retrieval Augmented Generation), a novel framework that addresses the limitations of traditional RAG models by adding a step-by-step reasoning and retrieval process.<br><br> This approach is designed to handle complex queries where a single retrieval step isn’t enough, making it especially useful for multi-hop question answering and knowledge-intensive tasks. <br><br> 🔍 Why CoRAG?<br><br> Conventional RAG models struggle with: <br> &nbsp;  1️⃣ Limited Retrieval Scope: A single retrieval step often fails to capture all relevant context for complex queries. <br> &nbsp;  2️⃣ Static Querying: No mechanism to refine or reformulate queries dynamically based on intermediate results. <br><br> CoRAG changes the scene by enabling iterative retrieval and reasoning, dynamically reformulating queries and improving the quality of retrieved information over multiple steps.<br><br> 💡 How CoRAG Works? <br> &nbsp;  1️⃣ Dynamic Query Reformulation: <br> &nbsp; &nbsp; 🔹 The model starts with an initial query and retrieves relevant documents. <br> &nbsp; &nbsp; 🔹 If retrieval fails or results are incomplete, CoRAG reformulates the query based on the current state, mimicking a human-like approach to problem-solving. <br> &nbsp;  2️⃣ Training with Retrieval Chains: <br> &nbsp; &nbsp; 🔹 By using rejection sampling, CoRAG augments datasets with intermediate retrieval chains that guide the model on how to break down complex queries into manageable steps. <br> &nbsp;  3️⃣ Flexible Decoding Strategies: <br> &nbsp; &nbsp; 🔹 CoRAG supports greedy decoding, best-of-N sampling, and tree search to balance performance and token efficiency at test time.<br><br> 📊 Results: <br> &nbsp;  🔹 Multi-hop QA (e.g., HotpotQA): More than 10-point improvement in exact match (EM) score compared to strong baselines. <br> &nbsp;  🔹 KILT Benchmark: Achieved new state-of-the-art performance across diverse knowledge-intensive tasks. <br> &nbsp;  🔹 Scalability: Handles long retrieval chains effectively, offering a log-linear relationship between token usage and model performance. | RAG |
| [SFT Memorizes, RL Generalizes: A Comparative Study of Foundation Model Post-training](https://arxiv.org/pdf/2501.17161) | This recent study, "SFT Memorizes, RL Generalizes" delves into the comparative effects of Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) in foundation model post-training.  <br><br> ☘️ Here are the key takeaways: <br><br> 🔹 SFT vs RL: While SFT excels at adapting models to follow instructions, it heavily relies on memorization. RL, on the other hand, demonstrates an impressive ability to generalize, especially in unseen scenarios across both textual and visual domains. <br><br> 🔹 Tasks Studied: <br> &nbsp;  🔸 GeneralPoints: A card-based arithmetic reasoning game. <br> &nbsp;  🔸 V-IRL: A real-world navigation environment focusing on spatial reasoning. <br><br> RL outperformed SFT in adapting to rule variations (e.g., changing card values) and visual changes (e.g., shifting card colours or navigating different cities). <br><br> 🔹 Generalization Beyond Rules: RL's ability to leverage outcome-based rewards enhances both reasoning and visual recognition capabilities, enabling state-of-the-art performance in challenging tasks. 🧠 <br><br> 🔹 The Role of SFT: Despite its limitations in generalization, SFT plays a crucial role in stabilizing model outputs, making it a valuable precursor to RL. 🔄  <br><br> This research reinforces the importance of RL in creating robust, adaptable AI systems that can tackle real-world variability. However, it also highlights the synergy between SFT and RL for optimal performance. | Foundation Models |
| [Janus-Pro: Unified Multimodal Understanding and Generation with Data and Model Scaling](https://arxiv.org/pdf/2501.17811) | 🎨 DeepSeek AI recently introduced Janus-Pro, a unified multimodal model designed to bridge the gap between multimodal understanding and text-to-image generation.  <br><br> 🔑 Key Highlights:  <br><br> 1️⃣ Optimized Training Strategy: <br> &nbsp;  🔸 Redesigned three-stage process enhancing efficiency and stability in text-to-image generation. <br> &nbsp;  🔸 Focused training on high-quality data improving outputs while minimizing computational costs <br><br> 2️⃣ Data Scaling: <br> &nbsp;  🔸 Incorporation of 90M new samples for multimodal understanding (e.g., YFCC, Docmatix datasets). <br> &nbsp;  🔸 Improved text-to-image outputs with a 1:1 real-to-synthetic data ratio, ensuring aesthetic quality and consistency <br><br> 3️⃣ Model Scaling: <br> &nbsp;  🔸 Scaling up from 1.5B to 7B parameters accelerates convergence and boosts performance on complex benchmarks. <br><br> 📊 Results: <br> &nbsp;  🔸 Achieved a top score of 79.2 on multimodal benchmarks, outperforming models like MetaMorph. <br> &nbsp;  🔸 Set new standards in text-to-image tasks with 80% accuracy on GenEval, exceeding even DALL-E 3 and Stable Diffusion 3 Medium. <br><br> 💡 By decoupling visual encoding for understanding and generation, Janus-Pro uniquely balances logic and creativity, paving the way for AI systems that truly adapt to diverse tasks - definitely an interesting read! | MultiModal |
| []() |  |  |
