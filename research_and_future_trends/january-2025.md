## Week 1/4
| Title | Summary | Topics |
| --- | --- | --- |
| [CODEELO: Benchmarking Competition-level Code Generation of LLMs with Human-comparable Elo Ratings](https://arxiv.org/pdf/2501.01257) | For years, we've wondered, how do coding LLMs really stack up against human developers? <br><br> Most benchmarks have fallen short, offering unrealistic test cases or missing the nuances of real-world programming challenges. <br><br> CODEELO: a new benchmark - addresses this by integrating directly with CodeForces, the global hub for competitive programming. 🌐 <br><br> 🔑 Key Insights: <br> &nbsp; 🔹 Real Problems, Real Stakes: It uses CodeForces problems, test cases, and conditions - just like human participants face. <br> &nbsp; 🔹 Elo Ratings for LLMs: CODEELO assigns ratings comparable to human players, offering unprecedented insight into LLM capabilities. <br> &nbsp; 🥇 Only two models, OpenAI’s o1-mini (1578) and QwQ-32B-Preview (1261), scored above 1200 Elo. Most others? Stuck in the bottom 20% of human participants. <br> &nbsp; 🧠 Strengths and Weaknesses: Top models excel in math but falter with dynamic programming and complex data structures. | Coding Benchmark |
| [Metadata Conditioning Accelerates Language Model Pre-training](https://arxiv.org/pdf/2501.01956) | This new research introduces MeCo (Metadata Conditioning then Cooldown), a method that brings simplicity, efficiency, and control to language model pre-training.  <br><br> 🔑 Here’s why this approach is so compelling: <br> &nbsp; ✅ Efficient Pre-training: MeCo uses metadata like URLs (e.g., en.wikipedia.org) during training to provide contextual cues, enabling the model to group and learn from documents more effectively. This reduces training data requirements by 33% - a 1.6B model achieves standard pre-training performance with fewer resources. <br> &nbsp; ✅ Steerable AI: By conditioning inference on metadata, even fabricated ones (e.g., factquizmaster.com), we can influence model behaviour. This reduces toxic outputs and boosts task-specific performance. <br> &nbsp; ✅ Universal Applicability: Compatible across model scales (600M to 8B parameters) and training datase | LLM Pre-Training |
| [AdaSkip: Adaptive Sublayer Skipping for Accelerating Long-Context LLM Inference](https://arxiv.org/pdf/2501.02336) | As LLMs tackle longer and longer texts (some now handling 1M+ tokens!), making them run efficiently becomes crucial. <br><br> This recent research proposes AdaSkip - it offers a practical solution without the complex training or fine-tuning needed by other methods - a smart way to accelerate LLMs when dealing with long text! <br><br> 🔑 Key highlights: <br> &nbsp; 🔹 Unlike previous approaches that skip entire layers, AdaSkip intelligently identifies and skips less important sub-components, maintaining model quality while boosting speed <br> &nbsp; 🔹 Works during both "prefilling" (initial processing) and "decoding" (generation) phases <br> &nbsp; 🔹 Adapts automatically to different models and contexts - no one-size-fits-all approach!  <br> &nbsp; 🔹 Achieved impressive results across multiple models (LLaMA, InternLM, Vicuna) and tasks | LLM Inference |
| [RAG-Check: Evaluating Multimodal Retrieval Augmented Generation Performance](https://arxiv.org/pdf/2501.03995) | Combining text, images, and more, multimodal RAG unlocks incredible potential - but it also opens the door to new challenges. Chief among them? Hallucination. <br><br> 🚨 Hallucinations in multimodal RAG systems stem from three layers:  <br> &nbsp;  1️⃣ Selection-hallucination: When irrelevant data (e.g., images, documents) are retrieved. <br> &nbsp;  2️⃣ Context-generation-hallucination: When vision-LMs generate inaccurate textual descriptions of visual data. <br> &nbsp;  3️⃣ Response-generation-hallucination: When LLMs produce irrelevant or incorrect responses despite good context. <br><br> 🧪 RAG-Check: This new Framework tackles these issues head-on by introducing two novel metrics to evaluate reliability: <br> &nbsp;  🔍 Relevance Score (RS): How relevant is the retrieved data? <br> &nbsp;  ✔️ Correctness Score (CS): How accurate are the generated responses? <br><br> 🎯 Key results? <br> &nbsp;  🔹 Both models achieve 88% accuracy on test data. <br> &nbsp;  🔹 The CS metric aligns with human evaluations 91% of the time, and RS outperforms traditional measures by over 20%! <br><br> 🔖 As multimodal RAG becomes mainstream, the conversation around hallucination detection and reliability metrics will only grow louder. Papers like this could define the standards for what comes next. | Multimodal RAG Evaluation |
| [Understanding Before Reasoning: Enhancing Chain-of-Thought with Iterative Summarization Pre-Prompting](https://arxiv.org/pdf/2501.04341) | This recent paper introduces Iterative Summarization Pre-Prompting (ISP2), a novel pre-prompting technique designed to enhance the reasoning capabilities of LLMs by refining the input context before reasoning starts. <br><br> Traditional Chain-of-Thought (CoT) prompting focuses on breaking down reasoning into steps but often struggles when essential information is missing or implicit.  <br><br> 💡 ISP2 tackles this by:  <br> &nbsp; 1️⃣ Extracting entities and key descriptions to form "information pairs." <br> &nbsp;  2️⃣ Iteratively merging and refining these pairs using a reliability rating system. <br> &nbsp;  3️⃣ Feeding the refined context back to the LLM for better reasoning. <br><br> 📈 The Results: <br> &nbsp;  🔹 A 7.1% improvement in reasoning tasks when combined with CoT. <br> &nbsp;  🔹 Exceptional plug-and-play compatibility across models  <br> &nbsp;  🔹 Significant advancements in commonsense and reasoning benchmarks. <br><br> 🤖 By focusing on understanding the problem space through iterative summarization, ISP2 enables LLMs to generate more accurate, contextually rich answers.  | Pre-Prompting |
| [Agent Laboratory: Using LLM Agents as Research Assistants](https://arxiv.org/pdf/2501.04227) | This research proposes a framework called Agent Laboratory. It uses Agents to guide research through three key phases:  <br> &nbsp; 🔹 literature review,  <br> &nbsp; 🔹 experimentation, and  <br> &nbsp; 🔹 report writing.  <br><br> 🔑 Key Insights: <br> &nbsp; 1️⃣ High-Quality Outputs: The framework's best-performing model, o1-preview, not only delivered exceptional research reports but also generated machine learning code that achieved state-of-the-art performance. 🎯 <br> &nbsp;  2️⃣ Cost Efficiency: Research expenses dropped dramatically - achieving an 84% cost reduction compared to traditional methods. <br> &nbsp;  3️⃣ Human-AI Collaboration: Human feedback at every stage significantly elevated the quality of outputs, showcasing the importance of collaboration over complete automation. <br><br> 🧩 While the results are promising, challenges like hallucinated results and technical limitations highlight the need for further refinement. But with frameworks like this, the future of AI-assisted scientific discovery is undeniably bright. | Research Agents |
| []() |  |  |


## Week 2/4
| Title | Summary | Topics |
| --- | --- | --- |
| [Imagine while Reasoning in Space: Multimodal Visualization-of-Thought](https://arxiv.org/pdf/2501.07542) | This recent paper introduces a novel approach to reasoning that bridges text and visuals seamlessly! <br><br> Understanding complex problems often requires more than just words - it demands visualization.  <br><br> 🌟 Inspired by how humans process information, this Multimodal Visualization-of-Thought (MVoT) paradigm takes AI reasoning to the next level by combining verbal and visual thinking. <br><br> Instead of relying solely on traditional text-based reasoning methods like Chain-of-Thought (CoT), MVoT allows AI to generate image visualizations of reasoning processes. This approach not only enhances accuracy but also provides clearer, more interpretable insights - especially in tasks like spatial navigation and dynamic problem-solving. <br><br> 📊 Key Highlights: <br> &nbsp;  🔹 20% performance boost in challenging spatial reasoning scenarios compared to CoT. <br> &nbsp;  🔹 Introduction of a token discrepancy loss, improving visual coherence and fidelity. <br> &nbsp;  🔹 MVoT excels in interpreting and solving problems where CoT struggles, like navigating intricate environments or predicting dynamic outcomes. <br><br> The possibilities this opens for AI applications in fields like robotics, education, and healthcare are immense!  <br><br> Imagine AI assisting with clear, visual reasoning steps for tasks like urban planning or disaster management. | Multimodal Prompting |
| [Lifelong Learning of Large Language Model based Agents: A Roadmap](https://arxiv.org/pdf/2501.07278) | This recent paper lays out a compelling roadmap for embedding lifelong learning into LLM-based agents. Here’s what stands out: <br><br> ♎ Core Pillars for Lifelong LLM Agents: <br> &nbsp; 1️⃣ Perception Module: Integrates multimodal inputs (text, images, etc.) to understand the environment. <br> &nbsp; 2️⃣ Memory Module: Stores evolving knowledge while avoiding catastrophic forgetting. <br> &nbsp; 3️⃣ Action Module: Facilitates interactions and decision-making to adapt in real time. <br><br> 💡 Key Challenges Addressed: <br> &nbsp; 🔹 Overcoming catastrophic forgetting 🧠 <br> &nbsp; 🔹 Balancing adaptability and knowledge retention <br> &nbsp; 🔹 Managing multimodal information effectively <br><br> 🌍 It has real world potential - From household assistants to complex decision-support systems, lifelong learning LLM agents are poised to excel in dynamic scenarios, enabling applications like gaming, autonomous systems, and interactive tools. | Agents Roadmap |
| [Governing AI Agents](https://arxiv.org/pdf/2501.07913) | As AI evolves from static tools to autonomous agents capable of independent decision-making, we face a growing challenge - how do we ensure these agents act in alignment with human values? <br><br> Unlike language models, AI agents act autonomously, taking complex actions, such as booking trips, managing finances, or even negotiating contracts. This autonomy raises critical questions about accountability, ethics, and liability. <br><br> This recent paper explores this shift and presents a roadmap for governing AI agents.  <br><br> ⚖️ Three Governance Challenges <br> &nbsp;  1️⃣ Information Asymmetry: Agents often operate with knowledge inaccessible to their human principals, creating trust issues. <br> &nbsp;  2️⃣ Authority and Loyalty: How much discretion should agents have? How do we ensure they prioritize user interests? <br> &nbsp;  3️⃣ Liability: Who is responsible when an agent makes an error or causes harm? <br><br> 🌍 Proposed Solutions - the paper suggests a three-pronged strategy: <br> &nbsp;  🔹 Inclusivity: Ensure agents align not just with user goals but also with broader societal values. <br> &nbsp;  🔹 Visibility: Enhance transparency in agent operations to predict and mitigate risks. <br> &nbsp;  🔹 Liability: Develop new legal frameworks to assign responsibility fairly among developers, operators, and users. <br><br> As AI agents become integral to our daily lives and businesses, these governance principles will shape their impact. The risks are high - autonomous hacking, algorithmic bias, and even ethical breaches but the potential for innovation and efficiency is immense. | Agents' Governance |
| [Towards Large Reasoning Models: A Survey on Scaling LLM Reasoning Capabilities](https://arxiv.org/pdf/2501.09686) | 🧠 As AI evolves, so does its capacity for reasoning but how far can we push the boundaries of logic and learning in LLMs? <br><br> This new survey provides a roadmap toward Large Reasoning Models (LRMs), designed to expand the reasoning capabilities of AI. <br><br> Here's a glimpse into the key findings: <br><br> 💡 Reasoning through Reinforcement <br> &nbsp;  🔹 By applying reinforcement learning techniques, LLMs are mastering step-by-step reasoning processes, reducing reliance on expensive human annotations. <br> &nbsp;  🔹 Innovative Process Reward Models (PRMs) assign feedback at each reasoning step, improving accuracy in tasks like mathematical problem-solving and logical deduction. <br><br> 🚀 Scaling for Smarter AI <br> &nbsp;  🔹 Combining train-time scaling (more reasoning data during training) and test-time scaling (deliberate reasoning during inference) has led to unprecedented gains in reasoning accuracy. <br> &nbsp;  🔹 The introduction of advanced prompting techniques, like Tree-of-Thought (ToT) and Chain-of-Thought (CoT), empowers LLMs to tackle multi-step, abstract tasks effectively. <br><br> With LRMs, the vision is to transition AI from conversational agents to reasoning powerhouses capable of contributing to domains like scientific discovery, programming, and decision-making. <br><br> The survey also highlights cutting-edge projects like OpenAI’s O1 series, which achieved doctorate-level reasoning abilities, and promising open-source initiatives paving the way for accessible large reasoning models. | Large Reasoning Models |
| [TRANSFORMER^2 : SELF-ADAPTIVE LLMS](https://arxiv.org/pdf/2501.06252) | Introduces Transformer^2, a novel self-adaptation framework that adapts LLMs for unseen tasks in real-time by selectively adjusting singular components of their weight matrices;  <br><br> it’s built with two key phases:  <br> &nbsp; 1) a dispatch system that analyzes and identifies the properties of the incoming task, and  <br> &nbsp; 2) a step that combines "expert" vectors (trained via reinforcement learning) to create task-specific behaviors;  <br><br> claims to be more efficient than LoRA with fewer parameters and can works across different LLM architectures. | LLMs |
| [MiniMax-01: Scaling Foundation Models with Lightning Attention](https://arxiv.org/pdf/2501.08313) | Introduces a new series of models that integrate Mixture-of-Experts; introduces a model with 32 experts and 456B parameters, and 45.9B are activated for each token;  <br><br> claims match the performance of state-of-the-art models like GPT-4o and Claude-3.5-Sonnet while offering a 20-32x longer context window;  <br><br> it can handle context windows of up to 4 million tokens;  <br><br> it integrates linear attention with optimized hardware utilization which enhances the efficiency and scalability of the LLM;  <br><br> there is also a vision model called MiniMax-VL-01 built through continued training with 512 billion vision-language tokens. | Foundation Models |
| []() |  |  |


## Week 3/4
| Title | Summary | Topics |
| --- | --- | --- |
| [Evolving Deeper LLM Thinking](https://arxiv.org/pdf/2501.09891) | This recent paper by researchers at Google DeepMind proposes a new approach - "Mind Evolution". It takes LLMs beyond static reasoning to achieve dynamic, iterative problem-solving. <br><br> Here’s a quick dive into what makes it worth a while: <br><br> 🌟 Inspired by Nature: <br>The method applies genetic algorithms - concepts of evolution like selection, crossover, and mutation to refine solutions iteratively. Think of it as AI brainstorming, improving its responses generation by generation. 🤔  <br><br> ⚙️ How It Works: <br> &nbsp;  🔹 Starts with diverse solution candidates for a problem. <br> &nbsp;  🔹 Uses an LLM to recombine and refine them, guided by feedback from an evaluator. <br> &nbsp;  🔹 Continues refining until it reaches the optimal answer or hits compute limits. <br><br> 📈 Performance Gains: <br> &nbsp;  🔹 Achieved 95.6% success in the TravelPlanner benchmark, compared to 55.6% using traditional Best-of-N strategies. <br> &nbsp;  🔹 Tackled natural language planning challenges like trip planning, meeting scheduling, and even creative tasks like embedding hidden messages in poetry! <br><br> 💡 Unlike conventional methods, Mind Evolution thrives in unstructured, natural language spaces, making it ingenious for tasks where formal problem-solving frameworks don’t exist - like planning complex travel itineraries, crafting poetic steganography etc. | LLM Reasoning |
| []() |  |  |
| []() |  |  |


## Week 4/4
| Title | Summary | Topics |
| --- | --- | --- |
| []() |  |  |
| []() |  |  |
| []() |  |  |
