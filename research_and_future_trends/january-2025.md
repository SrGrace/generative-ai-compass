## Week 1/4
| Title | Summary | Topics |
| --- | --- | --- |
| [CODEELO: Benchmarking Competition-level Code Generation of LLMs with Human-comparable Elo Ratings](https://arxiv.org/pdf/2501.01257) | For years, we've wondered, how do coding LLMs really stack up against human developers? <br><br> Most benchmarks have fallen short, offering unrealistic test cases or missing the nuances of real-world programming challenges. <br><br> CODEELO: a new benchmark - addresses this by integrating directly with CodeForces, the global hub for competitive programming. 🌐 <br><br> 🔑 Key Insights: <br> &nbsp; 🔹 Real Problems, Real Stakes: It uses CodeForces problems, test cases, and conditions - just like human participants face. <br> &nbsp; 🔹 Elo Ratings for LLMs: CODEELO assigns ratings comparable to human players, offering unprecedented insight into LLM capabilities. <br> &nbsp; 🥇 Only two models, OpenAI’s o1-mini (1578) and QwQ-32B-Preview (1261), scored above 1200 Elo. Most others? Stuck in the bottom 20% of human participants. <br> &nbsp; 🧠 Strengths and Weaknesses: Top models excel in math but falter with dynamic programming and complex data structures. | Coding Benchmark |
| [Metadata Conditioning Accelerates Language Model Pre-training](https://arxiv.org/pdf/2501.01956) | This new research introduces MeCo (Metadata Conditioning then Cooldown), a method that brings simplicity, efficiency, and control to language model pre-training.  <br><br> 🔑 Here’s why this approach is so compelling: <br> &nbsp; ✅ Efficient Pre-training: MeCo uses metadata like URLs (e.g., en.wikipedia.org) during training to provide contextual cues, enabling the model to group and learn from documents more effectively. This reduces training data requirements by 33% - a 1.6B model achieves standard pre-training performance with fewer resources. <br> &nbsp; ✅ Steerable AI: By conditioning inference on metadata, even fabricated ones (e.g., factquizmaster.com), we can influence model behaviour. This reduces toxic outputs and boosts task-specific performance. <br> &nbsp; ✅ Universal Applicability: Compatible across model scales (600M to 8B parameters) and training datase | LLM Pre-Training |
| []() |  |  |
| []() |  |  |


## Week 2/4
| Title | Summary | Topics |
| --- | --- | --- |
| []() |  |  |
| []() |  |  |


## Week 3/4
| Title | Summary | Topics |
| --- | --- | --- |
| []() |  |  |
| []() |  |  |


## Week 4/4
| Title | Summary | Topics |
| --- | --- | --- |
| []() |  |  |
| []() |  |  |
