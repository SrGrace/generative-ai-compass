## Week 1/4
| Title | Summary | Topics |
| --- | --- | --- |
| [CODEELO: Benchmarking Competition-level Code Generation of LLMs with Human-comparable Elo Ratings](https://arxiv.org/pdf/2501.01257) | For years, we've wondered, how do coding LLMs really stack up against human developers? <br><br> Most benchmarks have fallen short, offering unrealistic test cases or missing the nuances of real-world programming challenges. <br><br> CODEELO: a new benchmark - addresses this by integrating directly with CodeForces, the global hub for competitive programming. 🌐 <br><br> 🔑 Key Insights: <br> &nbsp; 🔹 Real Problems, Real Stakes: It uses CodeForces problems, test cases, and conditions - just like human participants face. <br> &nbsp; 🔹 Elo Ratings for LLMs: CODEELO assigns ratings comparable to human players, offering unprecedented insight into LLM capabilities. <br> &nbsp; 🥇 Only two models, OpenAI’s o1-mini (1578) and QwQ-32B-Preview (1261), scored above 1200 Elo. Most others? Stuck in the bottom 20% of human participants. <br> &nbsp; 🧠 Strengths and Weaknesses: Top models excel in math but falter with dynamic programming and complex data structures. | Coding Benchmark |
| [Metadata Conditioning Accelerates Language Model Pre-training](https://arxiv.org/pdf/2501.01956) | This new research introduces MeCo (Metadata Conditioning then Cooldown), a method that brings simplicity, efficiency, and control to language model pre-training.  <br><br> 🔑 Here’s why this approach is so compelling: <br> &nbsp; ✅ Efficient Pre-training: MeCo uses metadata like URLs (e.g., en.wikipedia.org) during training to provide contextual cues, enabling the model to group and learn from documents more effectively. This reduces training data requirements by 33% - a 1.6B model achieves standard pre-training performance with fewer resources. <br> &nbsp; ✅ Steerable AI: By conditioning inference on metadata, even fabricated ones (e.g., factquizmaster.com), we can influence model behaviour. This reduces toxic outputs and boosts task-specific performance. <br> &nbsp; ✅ Universal Applicability: Compatible across model scales (600M to 8B parameters) and training datase | LLM Pre-Training |
| [AdaSkip: Adaptive Sublayer Skipping for Accelerating Long-Context LLM Inference](https://arxiv.org/pdf/2501.02336) | As LLMs tackle longer and longer texts (some now handling 1M+ tokens!), making them run efficiently becomes crucial. <br><br> This recent research proposes AdaSkip - it offers a practical solution without the complex training or fine-tuning needed by other methods - a smart way to accelerate LLMs when dealing with long text! <br><br> 🔑 Key highlights: <br> &nbsp; 🔹 Unlike previous approaches that skip entire layers, AdaSkip intelligently identifies and skips less important sub-components, maintaining model quality while boosting speed <br> &nbsp; 🔹 Works during both "prefilling" (initial processing) and "decoding" (generation) phases <br> &nbsp; 🔹 Adapts automatically to different models and contexts - no one-size-fits-all approach!  <br> &nbsp; 🔹 Achieved impressive results across multiple models (LLaMA, InternLM, Vicuna) and tasks | LLM Inference |
| []() |  |  |
| []() |  |  |


## Week 2/4
| Title | Summary | Topics |
| --- | --- | --- |
| []() |  |  |
| []() |  |  |
| []() |  |  |


## Week 3/4
| Title | Summary | Topics |
| --- | --- | --- |
| []() |  |  |
| []() |  |  |
| []() |  |  |


## Week 4/4
| Title | Summary | Topics |
| --- | --- | --- |
| []() |  |  |
| []() |  |  |
| []() |  |  |
