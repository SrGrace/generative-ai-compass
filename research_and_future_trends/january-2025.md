## Week 1/4
| Title | Summary | Topics |
| --- | --- | --- |
| [CODEELO: Benchmarking Competition-level Code Generation of LLMs with Human-comparable Elo Ratings](https://arxiv.org/pdf/2501.01257) | For years, we've wondered, how do coding LLMs really stack up against human developers? <br><br> Most benchmarks have fallen short, offering unrealistic test cases or missing the nuances of real-world programming challenges. <br><br> CODEELO: a new benchmark - addresses this by integrating directly with CodeForces, the global hub for competitive programming. 🌐 <br><br> 🔑 Key Insights: <br> &nbsp; 🔹 Real Problems, Real Stakes: It uses CodeForces problems, test cases, and conditions - just like human participants face. <br> &nbsp; 🔹 Elo Ratings for LLMs: CODEELO assigns ratings comparable to human players, offering unprecedented insight into LLM capabilities. <br> &nbsp; 🥇 Only two models, OpenAI’s o1-mini (1578) and QwQ-32B-Preview (1261), scored above 1200 Elo. Most others? Stuck in the bottom 20% of human participants. <br> &nbsp; 🧠 Strengths and Weaknesses: Top models excel in math but falter with dynamic programming and complex data structures. | Coding Benchmark |
| []() |  |  |
| []() |  |  |


## Week 2/4
| Title | Summary | Topics |
| --- | --- | --- |
| []() |  |  |
| []() |  |  |


## Week 3/4
| Title | Summary | Topics |
| --- | --- | --- |
| []() |  |  |
| []() |  |  |


## Week 4/4
| Title | Summary | Topics |
| --- | --- | --- |
| []() |  |  |
| []() |  |  |
