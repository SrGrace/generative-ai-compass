## Week 1/4
| Title | Summary | Topics |
| --- | --- | --- |
| [Chain-of-Thought Matters: Improving Long-Context Language Models with Reasoning Path Supervision](https://arxiv.org/pdf/2502.20790) | LLMs struggle with long-context reasoning. ü´† <br><br> They can read thousands of tokens, but pulling the right information and reasoning through it? That‚Äôs where things fall apart. <br><br> So, I looked at why this happens. <br> &nbsp;  -> LLMs often retrieve the wrong information from long documents. <br> &nbsp;  -> Chain-of-Thought (CoT) helps, but not always - sometimes it adds unnecessary steps. <br> &nbsp;  -> Even when CoT works, models don‚Äôt learn how to reason better over time. <br><br> What‚Äôs the fix?  <br><br> This recent paper proposes LONGREPS - a process-supervised approach that teaches LLMs how to reason over long contexts. It -  <br> &nbsp;  -> Bootstraps reasoning paths by letting the model generate multiple solutions. <br> &nbsp;  -> Filters out low-quality reasoning using a new quality assessment method. <br> &nbsp;  -> Fine-tunes models on only high-quality reasoning paths to improve long-context understanding. <br><br> And the results? <br> &nbsp;  ‚úÖ +13.6 F1 gain on MuSiQue, a multi-hop QA benchmark. <br> &nbsp;  ‚úÖ +9.3 average improvement across multiple long-context reasoning tasks. <br> &nbsp;  ‚úÖ Better generalization across domains like legal, finance, and academic research. | LLM Reasoning |
| [LLM Post-Training: A Deep Dive into Reasoning Large Language Models](https://arxiv.org/pdf/2502.21321) | LLMs are trained on massive datasets. But raw pretraining isn‚Äôt enough. <br><br> Post-training is where they actually learn to reason, align with human intent, and improve factual accuracy. <br><br> This recent survey provides a systematic exploration of post-training methodologies, analyzing their role in refining LLMs beyond pretraining, addressing key challenges such as catastrophic forgetting, reward hacking, and inference-time trade-offs. <br><br> üîë Key Insights:  <br> &nbsp;  -> Fine-tuning helps adapt models to specific tasks but can be expensive and risks overfitting. <br> &nbsp;  -> Reinforcement learning (RL) improves adaptability but requires careful reward modeling to avoid unintended behaviors. <br> &nbsp;  -> Test-time scaling dynamically adjusts model behavior during inference, improving efficiency. <br><br> The key challenge?  <br> &nbsp;  -> Balancing accuracy, efficiency, and alignment while avoiding issues like catastrophic forgetting, reward hacking, and inference trade-offs. <br><br> And the results? <br> &nbsp;  ‚úÖ Post-training significantly improves reasoning and factual consistency. <br> &nbsp;  ‚úÖ RL methods like RLHF and DPO help align LLMs with user preferences. <br> &nbsp;  ‚úÖ Fine-tuning combined with retrieval-based approaches enhances factual accuracy. | LLM Post-training |
| [How Well do LLMs Compress Their Own Chain-of-Thought? A Token Complexity Approach](https://arxiv.org/pdf/2503.01141) | LLMs can reason step by step. But are they doing it efficiently? üßê  <br><br> Chain-of-Thought (CoT) prompting works, but it increases token usage and raises inference costs. Simply telling the model to "be concise" helps, but how much compression is too much before accuracy drops? <br><br> This is how the numbers look: <br> &nbsp;  -> Cutting response length too aggressively leads to a sharp drop in accuracy. -> Most tasks have an intrinsic token complexity - a minimum number of tokens required for correct reasoning. <br> &nbsp;  -> All compression strategies - word limits, bullet points, abbreviations - fall on a universal tradeoff curve between response length and accuracy. <br><br> This paper suggests that the LLMs need adaptive compression: <br> &nbsp;  -> Shorter reasoning for easy questions. <br> &nbsp;  -> More detailed reasoning for harder ones. <br> &nbsp;  -> Smarter compression that optimizes response length without sacrificing accuracy. <br><br> And the results? <br> &nbsp;  ‚úÖ Up to 60% reduction in tokens while keeping accuracy intact. <br> &nbsp;  ‚úÖ A new benchmark to measure reasoning efficiency. ‚úÖ Insights into how LLMs can be trained to think more like humans. | Prompt compression |
| [The First Few Tokens Are All You Need: An Efficient and Effective Unsupervised Prefix Fine-Tuning Method for Reasoning Models](https://arxiv.org/pdf/2503.02875) | Fine-tuning LLMs is expensive. It requires large datasets, long training times, and massive compute. But what if the first few tokens were all you needed? <br><br> For background - this is how much fine-tuning actually improves reasoning: <br> &nbsp;  -> Full-sequence fine-tuning is wasteful - most errors happen later in the reasoning process. <br> &nbsp;  -> Models tend to start their reasoning correctly before making mistakes. <br> &nbsp;  -> If early reasoning is consistent, maybe we only need to fine-tune the first few tokens. <br><br> So, what's the alternative? <br><br> This recent paper from Tencent proposes Unsupervised Prefix Fine-Tuning (UPFT). <br> &nbsp;  -> It trains on just the first 8-32 tokens of reasoning steps, skipping full-sequence tuning. <br> &nbsp;  -> It achieves the same accuracy as full fine-tuning while cutting training cost by 75%. <br> &nbsp;  -> It reduces sampling overhead by 99%, making training dramatically more efficient. <br> &nbsp;  -> UPFT matches supervised fine-tuning, without using labeled data. <br> &nbsp;  -> Works across multiple LLM architectures, improving reasoning consistency. <br> &nbsp;  -> Proves that early reasoning steps are key to better model performance. <br><br> This is really cool. Smarter fine-tuning is the way to go! üòé  | LLM fine-tuning |
| []() |  |  |


## Week 2/4
| Title | Summary | Topics |
| --- | --- | --- |
| [PromptPex: Automatic Test Generation for Language Model Prompts](https://arxiv.org/pdf/2503.05070) | Prompts are code-like artifacts, but we don‚Äôt test them like we do traditional software. <br><br> A single-word tweak can break a prompt‚Äôs behavior. Switching AI models can change outputs unpredictably. And yet, most teams still rely on trial and error to debug prompts. <br><br> So, I looked at how we can test prompts systematically. <br> &nbsp;  -> Prompts behave like software but lack unit tests to catch regressions. <br> &nbsp;  -> Output changes depend on both prompt wording and the AI model used. <br> &nbsp;  -> Manual testing is slow, inconsistent, and doesn‚Äôt scale. <br><br> So, what‚Äôs the fix? <br><br> This recent paper from Microsoft Research proposes PromptPex - an automated tool that generates unit tests for prompts. <br> &nbsp;  -> Extracts input/output specifications to clarify what a prompt is supposed to do. <br> &nbsp;  -> Generates diverse test cases that push models to follow or break prompt constraints. <br> &nbsp;  -> Evaluates outputs across multiple models to flag inconsistencies. <br><br> And the results? <br> &nbsp;  ‚úÖ More non-compliant outputs found than baseline test methods. <br> &nbsp;  ‚úÖ Works across multiple AI models, helping teams migrate and debug prompts. <br> &nbsp;  ‚úÖ Automates prompt validation, reducing the risk of silent failures. | Test Generation for Prompts |
| [A Graph-based Verification Framework for Fact-Checking](https://arxiv.org/pdf/2503.07282) | Fact-checking with LLMs is improving. ü´°  <br><br> But most models still struggle with verifying complex claims.  <br><br> And this is a big problem because misinformation spreads fast and detecting falsehoods requires more than just retrieval. <br><br> First and foremost - Why LLM-based fact-checking often fails? <br> &nbsp;  -> Models break down claims into text-based sub-claims, but this often introduces ambiguity. <br> &nbsp;  -> Many sub-claims mix multiple entities or relationships, making verification harder. <br> &nbsp;  -> Fact-checking models struggle with referential ambiguities, misattributing facts to the wrong sources. <br><br> What‚Äôs the fix then? <br><br> This recent paper proposes GraphFC - a graph-based verification framework. <br> &nbsp;  -> It builds a structured claim graph, breaking claims into <subject, relation, object> triplets. <br> &nbsp;  -> It creates an evidence graph, ensuring claims are verified against structured facts. <br> &nbsp;  -> It guides reasoning with graph-based planning, prioritizing the most informative verifications first. <br><br> And the results? <br> &nbsp;  ‚úÖ State-of-the-art accuracy on three major fact-checking datasets. <br> &nbsp;  ‚úÖ Handles multi-hop reasoning better, making it more reliable for complex claims. <br> &nbsp;  ‚úÖ Reduces ambiguity, improving claim decomposition and verification consistency. | Fact Checking Framework |
| [Gemma 3 Technical Report](https://storage.googleapis.com/deepmind-media/gemma/Gemma3Report.pdf) | üîë Key highlights from the technical paper: <br> &nbsp; üîπ Multimodal capabilities ‚Äì Integrates a frozen SigLIP vision encoder, converting images into 256 soft tokens for seamless visual understanding. The Pan & Scan (P&S) method enhances image analysis, making it better suited for document QA and text recognition. <br> &nbsp; üîπ 128K context length ‚Äì Tackles long-context limitations with a 5:1 ratio of local (sliding-window) and global attention layers, significantly reducing memory overhead while maintaining strong performance. <br> &nbsp; üîπ Optimized for efficiency ‚Äì Uses knowledge distillation and quantization-aware training (QAT) to deliver smaller model footprints (int4, switched-fp8), making it deployable on a single GPU or TPU host. <br> &nbsp; üîπ Instruction-tuned for top-tier performance ‚Äì Post-training enhancements with specialized reward signals improve math, coding, multilingual chat, and more. Early rankings in LMSYS Chatbot Arena place Gemma-3-27B-IT among the top 10 models, outperforming DeepSeek-V3, LLaMA 3 405B, and Qwen2.5-70B. <br> &nbsp; üîπ Expansive language support & structured workflows ‚Äì Supports 35 languages out of the box and is pretrained on 140+ languages. It also enables function calling and structured outputs, making it great for agentic workflows. <br> &nbsp; üîπ Safety & privacy focus ‚Äì Advanced data filtering and decontamination techniques significantly reduce memorization rates and ensure low risks of personal data regurgitation. | LLM |
| [Towards Reasoning Era: A Survey of Long Chain-of-Thought for Reasoning Large Language Models](https://arxiv.org/pdf/2503.09567) | Are Large Language Models Overthinking? ü§î  <br><br> As AI systems become more capable, their ability to reason over complex problems is under the spotlight. This recent survey on Long Chain-of-Thought (Long CoT) Reasoning sheds light on how models like OpenAI-O1 and DeepSeek-R1 are challenging the norms of structured reasoning.  <br><br> But is more always better? <br> &nbsp; üîπ Short vs. Long Chain-of-Thought: Traditional models rely on Short CoT, a straightforward reasoning approach with limited logical depth. Long CoT, on the other hand, enables deeper reasoning by incorporating extensive exploration, reflection, and multi-step validation - helping models tackle math, coding, and multi-disciplinary reasoning. <br> &nbsp; üîπ Does Longer Reasoning Mean Smarter AI?: While Long CoT improves problem-solving, there's a catch: overthinking. Researchers found that beyond a certain depth, errors accumulate instead of improving accuracy. This phenomenon, called the "Reasoning Boundary Effect," highlights that adding more steps isn‚Äôt always beneficial. <br> &nbsp; üîπ Key Takeaways from the Survey -  <br> &nbsp; &nbsp; ‚úÖ Deep Reasoning: Models need structured approaches to manage long logical chains effectively. <br> &nbsp; &nbsp;  ‚úÖ Exploration vs. Efficiency: Scaling reasoning requires balancing depth with computational cost. <br> &nbsp; &nbsp;  ‚úÖ Reflection & Self-Correction: Feedback loops help refine logic, but too much self-checking can introduce inefficiencies. <br> &nbsp; &nbsp;  ‚úÖ Future Directions: AI reasoning is evolving towards multimodal, multilingual, and agentic systems for more robust problem-solving. | Long CoT Survey |
| []() |  |  |


## Week 3/4
| Title | Summary | Topics |
| --- | --- | --- |
| [GNNs as Predictors of Agentic Workflow Performances](https://arxiv.org/pdf/2503.11301) | LLM-based agents are powerful. <br><br> But optimizing agentic workflows - where multiple AI agents collaborate on complex tasks - is slow and expensive. <br><br> And this is a big problem because every optimization run requires repeated LLM calls, making the process computationally and financially costly. <br><br> So, I looked at how we can make this more efficient. <br> &nbsp;  -> Agentic workflows can be represented as computational graphs, with nodes as agents and edges as dependencies. <br> &nbsp;  -> Current optimization methods rely on trial-and-error, requiring extensive LLM executions. <br> &nbsp;  -> What if we could predict workflow performance instead of running every iteration? <br><br> What‚Äôs the alternative? - This recent paper suggests using Graph Neural Networks (GNNs) to predict workflow performance. <br> &nbsp;  -> GNNs learn patterns from previous executions, reducing the need for repeated LLM calls. <br> &nbsp;  -> FLORA-Bench, a new benchmarking platform, evaluates how well GNNs can predict workflow success. <br> &nbsp;  -> Initial results show GNNs can accelerate optimization cycles by 125x while maintaining accuracy. <br><br> And the results? <br> &nbsp;  ‚úÖ GNNs predict workflow performance without running costly LLM inference. <br> &nbsp;  ‚úÖ Accuracy remains strong, even across different LLM-driven agentic systems. <br> &nbsp;  ‚úÖ Shifting from LLM-intensive optimization to GNN-guided prediction makes workflows smarter and more scalable. | Agentic Workflow performace enhancement |
| [ThinkPatterns-21k: A Systematic Study on the Impact of Thinking Patterns in LLMs](https://arxiv.org/pdf/2503.12918) | LLMs can "think before responding" using different strategies. <br><br> But which thinking pattern works best? Is it better to self-ask, debate internally, or just monologue freely? <br><br> Here's what the research says. <br> &nbsp;  -> This recent paper proposes ThinkPatterns-21k analyzing 21,000 instruction-response pairs across models from 3B to 32B parameters. <br> &nbsp;  -> They tested five thinking patterns: <br> &nbsp;  üîπ Monologue: Free, unstructured thought <br> &nbsp;  üîπ Decomposition: Step-by-step breakdown <br> &nbsp;  üîπ Self-ask: Recursive questioning <br> &nbsp;  üîπ Self-debate: Weighing opposing views <br> &nbsp;  üîπ Self-critic: Draft-critique-refine cycle <br><br> And here‚Äôs what they found: <br> &nbsp;  ‚úÖ Smaller models (<30B) benefit from structured methods like decomposition and self-ask - up to +15% accuracy. <br> &nbsp;  ‚úÖ Larger models (32B) perform better with unstructured monologue - more flexible, less rigid. <br> &nbsp;  ‚úÖ Surprisingly, self-critic works well across all model sizes, balancing structure and flexibility. | LLMs Reasoning |
| [Stop Overthinking: A Survey on Efficient Reasoning for Large Language Models](https://arxiv.org/pdf/2503.16419) | LLMs are thinking too much - and it‚Äôs a problem. <br><br> When models like OpenAI's o1 or DeepSeek-R1 tackle complex reasoning tasks, they tend to overthink, generating excessively long reasoning chains that waste computation without improving accuracy. <br><br> This new survey, "Stop Overthinking: A Survey on Efficient Reasoning for Large Language Models," explores how to make AI reasoning more efficient while preserving accuracy. <br><br> üîç Key insights: <br> &nbsp; [1] LLMs waste computation by over-explaining:  <br> &nbsp; &nbsp;  -> Some models take hundreds of extra steps just to solve simple problems, making them slower and more expensive. <br> &nbsp; [2] Smaller models struggle more with efficient reasoning. <br> &nbsp; &nbsp;  -> While large models (32B+) can afford verbosity, smaller models (7B or less) need concise, optimized reasoning to perform well. <br> &nbsp; [3] Three core approaches to reducing overthinking: <br> &nbsp; &nbsp;  -> Model-based optimization: Training models to think smarter, not longer. <br> &nbsp; &nbsp;  -> Output-based efficiency: Dynamically shortening reasoning steps during inference. <br> &nbsp; &nbsp;  -> Prompt engineering: Designing inputs that guide the model to be concise. <br> &nbsp; [4] Reinforcement learning (RL) helps control reasoning length. <br> &nbsp; &nbsp;  -> LLMs can be trained to stop thinking sooner by rewarding correct but shorter responses. <br> &nbsp; [5] Prompting can make models more efficient instantly. <br> &nbsp; &nbsp;  -> Simply telling an LLM: "Solve this in less than 10 words" can dramatically cut reasoning steps while maintaining accuracy. | LLM Reasoning |
| []() |  |  |
| []() |  |  |


## Week 4/4
| Title | Summary | Topics |
| --- | --- | --- |
| [Survey on Evaluation of LLM-based Agents](https://arxiv.org/pdf/2503.16416) | LLM-based agents are no longer just chatbots. They plan, reason, use tools, and even self-reflect to adapt to dynamic environments. But how do we evaluate their real-world performance? üìä <br><br> This new survey breaks down four key dimensions of agent evaluation: <br> &nbsp; ‚úÖ Fundamental capabilities like planning, tool use, memory, and self-reflection <br> &nbsp; ‚úÖ Application-specific benchmarks for web, software, scientific, and conversational agents <br> &nbsp; ‚úÖ Generalist agent evaluation to assess adaptability across multiple domains <br> &nbsp; ‚úÖ Agent evaluation frameworks for ongoing monitoring and improvement <br><br> üìà The study also highlights emerging trends like: <br> &nbsp; üîπ More realistic, evolving benchmarks that move beyond static datasets <br> &nbsp; üîπ A shift toward granular, step-by-step evaluations instead of just pass/fail tests <br> &nbsp; üîπ The need for cost-efficiency metrics to balance performance with practical deployment <br><br> One key gap? Safety and compliance. While benchmarks focus on intelligence, trustworthiness and risk mitigation still need more attention. <br><br> As LLM agents continue to expand into enterprise and research, reliable evaluation will be just as critical as capability. | Agents Survey|
| [AgentRxiv: Towards Collaborative Autonomous Research](https://arxiv.org/pdf/2503.18102) | AI agents doing research autonomously isn‚Äôt new. But they‚Äôve mostly worked in silos, each agent generating its own research in isolation.<br><br> That‚Äôs what this new paper - AgentRxiv aims to change.<br><br> Instead of every agent starting from scratch, AgentRxiv lets AI research labs upload and retrieve each other‚Äôs findings - just like human researchers do on arXiv.<br><br> And this is a big shift because when agents build on prior work, they improve faster.<br><br> Here‚Äôs what happens when they collaborate: <br> &nbsp;  -> Agents using AgentRxiv improved accuracy by 13.7% on MATH-500 vs. agents working alone <br> &nbsp;  -> The best techniques they discovered generalized across multiple AI benchmarks, boosting performance by 3.3% on average <br> &nbsp;  -> Running multiple agent labs in parallel sped up research, achieving breakthroughs faster<br><br> AI agents are no longer just research executors. They‚Äôre research collaborators.<br><br> Now, imagine this at scale. AI-driven labs continuously iterating on drug discovery, materials science, or fundamental AI breakthroughs - without human bottlenecks.<br><br> But there‚Äôs a catch. <br> &nbsp;  1Ô∏è‚É£ These AI research agents still hallucinate results, sometimes making up findings. <br> &nbsp;  2Ô∏è‚É£ Plagiarism concerns remain - are these discoveries truly novel? <br> &nbsp;  3Ô∏è‚É£ Who takes responsibility when AI-generated research is flawed? | Research Agent |
| []() |  |  |
| []() |  |  |
