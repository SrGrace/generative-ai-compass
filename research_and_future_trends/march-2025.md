## Week 1/4
| Title | Summary | Topics |
| --- | --- | --- |
| [Chain-of-Thought Matters: Improving Long-Context Language Models with Reasoning Path Supervision](https://arxiv.org/pdf/2502.20790) | LLMs struggle with long-context reasoning. ü´† <br><br> They can read thousands of tokens, but pulling the right information and reasoning through it? That‚Äôs where things fall apart. <br><br> So, I looked at why this happens. <br> &nbsp;  -> LLMs often retrieve the wrong information from long documents. <br> &nbsp;  -> Chain-of-Thought (CoT) helps, but not always - sometimes it adds unnecessary steps. <br> &nbsp;  -> Even when CoT works, models don‚Äôt learn how to reason better over time. <br><br> What‚Äôs the fix?  <br><br> This recent paper proposes LONGREPS - a process-supervised approach that teaches LLMs how to reason over long contexts. It -  <br> &nbsp;  -> Bootstraps reasoning paths by letting the model generate multiple solutions. <br> &nbsp;  -> Filters out low-quality reasoning using a new quality assessment method. <br> &nbsp;  -> Fine-tunes models on only high-quality reasoning paths to improve long-context understanding. <br><br> And the results? <br> &nbsp;  ‚úÖ +13.6 F1 gain on MuSiQue, a multi-hop QA benchmark. <br> &nbsp;  ‚úÖ +9.3 average improvement across multiple long-context reasoning tasks. <br> &nbsp;  ‚úÖ Better generalization across domains like legal, finance, and academic research. | LLM Reasoning |
| [LLM Post-Training: A Deep Dive into Reasoning Large Language Models](https://arxiv.org/pdf/2502.21321) | LLMs are trained on massive datasets. But raw pretraining isn‚Äôt enough. <br><br> Post-training is where they actually learn to reason, align with human intent, and improve factual accuracy. <br><br> This recent survey provides a systematic exploration of post-training methodologies, analyzing their role in refining LLMs beyond pretraining, addressing key challenges such as catastrophic forgetting, reward hacking, and inference-time trade-offs. <br><br> üîë Key Insights:  <br> &nbsp;  -> Fine-tuning helps adapt models to specific tasks but can be expensive and risks overfitting. <br> &nbsp;  -> Reinforcement learning (RL) improves adaptability but requires careful reward modeling to avoid unintended behaviors. <br> &nbsp;  -> Test-time scaling dynamically adjusts model behavior during inference, improving efficiency. <br><br> The key challenge?  <br> &nbsp;  -> Balancing accuracy, efficiency, and alignment while avoiding issues like catastrophic forgetting, reward hacking, and inference trade-offs. <br><br> And the results? <br> &nbsp;  ‚úÖ Post-training significantly improves reasoning and factual consistency. <br> &nbsp;  ‚úÖ RL methods like RLHF and DPO help align LLMs with user preferences. <br> &nbsp;  ‚úÖ Fine-tuning combined with retrieval-based approaches enhances factual accuracy. | LLM Post-training |
| [How Well do LLMs Compress Their Own Chain-of-Thought? A Token Complexity Approach](https://arxiv.org/pdf/2503.01141) | LLMs can reason step by step. But are they doing it efficiently? üßê  <br><br> Chain-of-Thought (CoT) prompting works, but it increases token usage and raises inference costs. Simply telling the model to "be concise" helps, but how much compression is too much before accuracy drops? <br><br> This is how the numbers look: <br> &nbsp;  -> Cutting response length too aggressively leads to a sharp drop in accuracy. -> Most tasks have an intrinsic token complexity - a minimum number of tokens required for correct reasoning. <br> &nbsp;  -> All compression strategies - word limits, bullet points, abbreviations - fall on a universal tradeoff curve between response length and accuracy. <br><br> This paper suggests that the LLMs need adaptive compression: <br> &nbsp;  -> Shorter reasoning for easy questions. <br> &nbsp;  -> More detailed reasoning for harder ones. <br> &nbsp;  -> Smarter compression that optimizes response length without sacrificing accuracy. <br><br> And the results? <br> &nbsp;  ‚úÖ Up to 60% reduction in tokens while keeping accuracy intact. <br> &nbsp;  ‚úÖ A new benchmark to measure reasoning efficiency. ‚úÖ Insights into how LLMs can be trained to think more like humans. | Prompt compression |
| [The First Few Tokens Are All You Need: An Efficient and Effective Unsupervised Prefix Fine-Tuning Method for Reasoning Models](https://arxiv.org/pdf/2503.02875) | Fine-tuning LLMs is expensive. It requires large datasets, long training times, and massive compute. But what if the first few tokens were all you needed? <br><br> For background - this is how much fine-tuning actually improves reasoning: <br> &nbsp;  -> Full-sequence fine-tuning is wasteful - most errors happen later in the reasoning process. <br> &nbsp;  -> Models tend to start their reasoning correctly before making mistakes. <br> &nbsp;  -> If early reasoning is consistent, maybe we only need to fine-tune the first few tokens. <br><br> So, what's the alternative? <br><br> This recent paper from Tencent proposes Unsupervised Prefix Fine-Tuning (UPFT). <br> &nbsp;  -> It trains on just the first 8-32 tokens of reasoning steps, skipping full-sequence tuning. <br> &nbsp;  -> It achieves the same accuracy as full fine-tuning while cutting training cost by 75%. <br> &nbsp;  -> It reduces sampling overhead by 99%, making training dramatically more efficient. <br> &nbsp;  -> UPFT matches supervised fine-tuning, without using labeled data. <br> &nbsp;  -> Works across multiple LLM architectures, improving reasoning consistency. <br> &nbsp;  -> Proves that early reasoning steps are key to better model performance. <br><br> This is really cool. Smarter fine-tuning is the way to go! üòé  | LLM fine-tuning |
| []() |  |  |


## Week 2/4
| Title | Summary | Topics |
| --- | --- | --- |
| [PromptPex: Automatic Test Generation for Language Model Prompts](https://arxiv.org/pdf/2503.05070) | Prompts are code-like artifacts, but we don‚Äôt test them like we do traditional software. <br><br> A single-word tweak can break a prompt‚Äôs behavior. Switching AI models can change outputs unpredictably. And yet, most teams still rely on trial and error to debug prompts. <br><br> So, I looked at how we can test prompts systematically. <br> &nbsp;  -> Prompts behave like software but lack unit tests to catch regressions. <br> &nbsp;  -> Output changes depend on both prompt wording and the AI model used. <br> &nbsp;  -> Manual testing is slow, inconsistent, and doesn‚Äôt scale. <br><br> So, what‚Äôs the fix? <br><br> This recent paper from Microsoft Research proposes PromptPex - an automated tool that generates unit tests for prompts. <br> &nbsp;  -> Extracts input/output specifications to clarify what a prompt is supposed to do. <br> &nbsp;  -> Generates diverse test cases that push models to follow or break prompt constraints. <br> &nbsp;  -> Evaluates outputs across multiple models to flag inconsistencies. <br><br> And the results? <br> &nbsp;  ‚úÖ More non-compliant outputs found than baseline test methods. <br> &nbsp;  ‚úÖ Works across multiple AI models, helping teams migrate and debug prompts. <br> &nbsp;  ‚úÖ Automates prompt validation, reducing the risk of silent failures. | Test Generation for Prompts |
| [A Graph-based Verification Framework for Fact-Checking](https://arxiv.org/pdf/2503.07282) | Fact-checking with LLMs is improving. ü´°  <br><br> But most models still struggle with verifying complex claims.  <br><br> And this is a big problem because misinformation spreads fast and detecting falsehoods requires more than just retrieval. <br><br> First and foremost - Why LLM-based fact-checking often fails? <br> &nbsp;  -> Models break down claims into text-based sub-claims, but this often introduces ambiguity. <br> &nbsp;  -> Many sub-claims mix multiple entities or relationships, making verification harder. <br> &nbsp;  -> Fact-checking models struggle with referential ambiguities, misattributing facts to the wrong sources. <br><br> What‚Äôs the fix then? <br><br> This recent paper proposes GraphFC - a graph-based verification framework. <br> &nbsp;  -> It builds a structured claim graph, breaking claims into <subject, relation, object> triplets. <br> &nbsp;  -> It creates an evidence graph, ensuring claims are verified against structured facts. <br> &nbsp;  -> It guides reasoning with graph-based planning, prioritizing the most informative verifications first. <br><br> And the results? <br> &nbsp;  ‚úÖ State-of-the-art accuracy on three major fact-checking datasets. <br> &nbsp;  ‚úÖ Handles multi-hop reasoning better, making it more reliable for complex claims. <br> &nbsp;  ‚úÖ Reduces ambiguity, improving claim decomposition and verification consistency. | Fact Checking Framework |
| []() |  |  |
| []() |  |  |


## Week 3/4
| Title | Summary | Topics |
| --- | --- | --- |
| []() |  |  |
| []() |  |  |



## Week 4/4
| Title | Summary | Topics |
| --- | --- | --- |
| []() |  |  |
| []() |  |  |
