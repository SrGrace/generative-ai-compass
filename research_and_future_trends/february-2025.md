## Week 1/4
| Title | Summary | Topics |
| --- | --- | --- |
| [Memory-Efficient Fine-Tuning of Transformers via Token Selection](https://www.arxiv.org/pdf/2501.18824) | Fine-tuning LLMs is essential, but GPU memory is the bottleneck. <br><br> And this is a big deal because even the best memory-efficient methods like LoRA and QLoRA still need to cache all intermediate activations during the forward pass. <br><br> Here‚Äôs where TOKENTUNE comes in - a recent research from Meta AI <br><br> .. <br><br> To fine-tune a model like Llama2-7B: <br><br> &nbsp; -> QLoRA reduces memory usage but still needs 37% of full fine-tuning memory. <br><br> &nbsp; -> TOKENTUNE cuts that down further, needing just 21% of full fine-tuning memory when combined with QLoRA, saving ~79% memory overall. <br><br> How? <br><br> &nbsp; -> Instead of caching all tokens during backpropagation, it selects a subset of tokens for gradient computation while freezing the rest. <br><br> &nbsp; -> This dramatically reduces the memory footprint without hurting accuracy. <br><br> .. <br><br> So, what does this mean in practice? <br><br> &nbsp; 1> Fine-tuning large models becomes accessible even on smaller GPUs like the A100 or H100. <br><br> &nbsp; 2> Costs drop significantly for both research and deployment. <br><br> &nbsp; 3> It works on its own or with other methods, making it highly flexible. <br><br> But it also raises questions. <br><br> &nbsp; -> What happens when token selection isn‚Äôt optimal for specific tasks? <br><br> &nbsp;  -> Can this approach scale to models much larger than Llama2-7B? | fine-tuning |
| [SETS: Leveraging Self-Verification and Self-Correction for Improved Test-Time Scaling](https://arxiv.org/pdf/2501.19306) | Improving LLM performance during inference is always tricky. <br><br> And this is a big deal because traditional methods like repeated sampling or task-specific reward models quickly hit diminishing returns as compute scales. <br><br> .. <br><br> To improve outputs, most methods rely on sampling multiple responses and using majority votes. But here‚Äôs the catch: <br><br> &nbsp; -> As the number of samples increases, performance gains plateau. <br><br> &nbsp; -> Adding a task-specific reward model to verify solutions drives up costs and reduces scalability. <br><br> SETS takes a different approach. <br><br> .. <br><br> Instead of relying on just sampling: <br><br> &nbsp; -> It introduces Self-Verification to check if an answer meets constraints. <br><br> &nbsp; -> Then uses Self-Correction to refine incorrect responses. <br><br> &nbsp; -> This iterative process improves responses while keeping costs under control.<br><br> And the results? <br><br> &nbsp; -> On tasks like trip and meeting planning (NATURAL PLAN benchmark), SETS improved accuracy by up to 8.7% over repeated sampling. <br><br> &nbsp;  -> Confidence scores also got better, with up to 9% higher AUACC and reduced calibration errors, ensuring more reliable outputs. <br><br> .. <br><br> But the real kicker is efficiency: <br><br> SETS doesn‚Äôt need external reward models or extra fine-tuning. It works with existing LLM capabilities, making it scalable across tasks. <br><br> However, the effectiveness of SETS depends heavily on the underlying model's self-verification and correction abilities. Stronger these are, the better SETS performs. | LLM Reasoning |
| [Are Language Models Up to Sequential Optimization Problems? From Evaluation to a Hegelian-Inspired Enhancement](https://arxiv.org/pdf/2502.02573) | LLMs are being tested on everything, but what about solving Sequential Optimization Problems (SOPs)? <br><br> And this is a big question because SOPs - where decisions are made in sequence, each impacting the next - are critical in logistics, engineering, and more. <br><br> So, I dived into the paper to understand how well LLMs perform and how we can make them better. <br><br> .. <br><br> To evaluate LLMs, this recent paper introduces WorldGen, a framework to dynamically generate SOPs of varying complexity. <br><br> Here are the key findings: <br><br> &nbsp; -> For simple tasks (no local optima), models like GPT-4 performed well. <br><br> &nbsp; -> But as complexity increased (e.g., multiple local optima), success rates dropped drastically - down to 4% on medium-complexity tasks. <br><br> Clearly, LLMs struggle when the problems get real. <br><br> .. <br><br> To address this, they introduced a framework inspired by Hegelian Dialectics, called ACE (Actor, Critic, Synthesizer) <br><br> Here‚Äôs how it works: <br><br> &nbsp; -> The Actor generates a solution (thesis). <br><br> &nbsp; -> The Critic challenges it with flaws and alternatives (antithesis). <br><br> &nbsp; -> The Synthesizer combines both to refine the solution (synthesis). <br><br> This iterative cycle pushes the model to improve its reasoning dynamically without retraining or fine-tuning. <br><br> .. <br><br> And the results? <br><br> &nbsp; -> Using ACE, success rates jumped from 36% to 88% on simple tasks and improved for medium-complexity tasks too. <br><br> &nbsp; -> ACE outperformed popular techniques like Majority Vote and Debate while consuming fewer resources. <br><br> But ACE relies heavily on the LLM‚Äôs baseline capabilities. A weaker model = limited improvement. | Language Models |
| [CoAT: Chain-of-Associated-Thoughts Framework for Enhancing LLMs Reasoning ](https://arxiv.org/pdf/2502.02390v1) | Most LLMs rely on fast thinking - they generate answers based on a single query and predefined reasoning. <br><br> And this has worked well, but it struggles with problems requiring iterative reasoning or the ability to adapt to new information dynamically. <br><br> This recent research proposes a framework called CoAT (Chain-of-Associated-Thoughts) to enhance the reasoning process of LLMs. <br><br> .. <br><br> Here‚Äôs what CoAT does differently: <br><br> &nbsp; -> Associative Memory dynamically retrieves and integrates new knowledge during reasoning. Think of it as the model learning and updating its thoughts in real time. <br><br> &nbsp; -> It‚Äôs paired with Optimized Monte Carlo Tree Search (MCTS), which expands search spaces methodically, ensuring diverse reasoning paths while refining earlier answers. <br><br> .. <br><br> How does it perform? <br><br> &nbsp; -> On complex reasoning tasks, CoAT outperformed Tree-of-Thought (ToT) and RAG in accuracy, coherence, and diversity. <br><br> &nbsp; -> It generated richer, more relevant content by iteratively incorporating and validating new information. <br><br> And here‚Äôs the kicker:-> CoAT plugs into existing LLMs without retraining. It acts as a framework that enhances reasoning capabilities dynamically. üòé  | LLMs Reasoning |
| [SymAgent: A Neural-Symbolic Self-Learning Agent Framework for Complex Reasoning over Knowledge Graphs](https://arxiv.org/pdf/2502.03283v1) | Integrating LLMs with Knowledge Graphs (KGs) sounds great in theory. <br><br> But here‚Äôs the problem: <br> &nbsp; -> LLMs, while powerful, often hallucinate and struggle with complex reasoning tasks. <br> &nbsp; -> KGs, though reliable, are static and incomplete - they can‚Äôt handle dynamic, multi-step reasoning. <br><br> This recent framewok called SymAgent addresses these challenges. <br><br> .. <br><br> Here‚Äôs what it does differently: <br> &nbsp; -> SymAgent treats KGs as dynamic environments, transforming reasoning into an interactive, multi-step process. <br> &nbsp; -> It combines two key modules: <br> &nbsp; &nbsp; 1> Agent-Planner: Extracts symbolic rules from KGs to guide reasoning and break down complex questions. <br> &nbsp; &nbsp;  2> Agent-Executor: Dynamically retrieves information from KGs and external sources to address incompleteness. <br><br> And to top it off, it has a self-learning framework that enables the agent to improve over time without human supervision. <br><br> .. <br><br> The results? <br> &nbsp; -> On reasoning tasks, SymAgent outperformed other methods, even with weaker LLMs like Llama2-7B. <br> &nbsp; -> It improved KG quality by identifying missing triples and automatically updating them. <br> &nbsp; -> On average, it boosted reasoning accuracy by up to 37% across datasets. | Reasoning over KGs |
| []() |  |  |


## Week 2/4
| Title | Summary | Topics |
| --- | --- | --- |
| [LLMs can be easily Confused by Instructional Distractions](https://www.arxiv.org/pdf/2502.04362) | LLMs are great at following instructions, right? <br><br> But here‚Äôs the catch - what happens when the input itself looks like an instruction? ü§î  <br><br> This recent paper proposes DIM-Bench, a new benchmark designed to test this issue, called instructional distraction. <br><br> .. <br><br> üßê Here‚Äôs the problem: <br> &nbsp; üëâ LLMs are good at tasks like translation or rewriting, but when the input mimics an instruction (e.g., ‚Äúsolve this math problem‚Äù), they often get confused. <br> &nbsp; üëâ Instead of following the main instruction, they end up addressing the distracting input, even when both are clearly separated. <br><br> DIM-Bench evaluates how well LLMs handle this across 20 task combinations. <br><br> .. <br><br> And the results? <br> &nbsp; üëâ Models like GPT-4o and Llama-3.1-70B struggle. For example: <br> &nbsp; &nbsp;  -> In translation tasks, accuracy is 52.6%, but drops to 5.1% when paired with question-answering distractions. <br> &nbsp; &nbsp;  -> For rewriting + math problems, models often skip rewriting entirely and solve the math instead. <br> &nbsp; üëâ Prompting techniques like Chain-of-Thought reasoning help slightly, but none fully solve the issue. | LLMs Instruction following |
| [Can 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling](https://arxiv.org/pdf/2502.06703) | Scaling up LLMs has always been seen as the way forward. <br><br> But here‚Äôs the thing - Bigger models like GPT-4o deliver exceptional performance, but they are expensive and resource-heavy.  <br><br> Smaller models, on the other hand, are cheaper but often fail at handling complex reasoning tasks. <br><br> So, I dived into the idea of Test-Time Scaling (TTS) to see how it can help smaller models punch above their weight. <br><br> .. <br><br> ‚òòÔ∏è Here‚Äôs what TTS does: <br> &nbsp; üëâ Instead of making the model bigger, it optimally uses compute during inference. <br> &nbsp; üëâ Techniques like beam search, verifier trees, and reward models are used to refine reasoning without increasing model size. <br><br> The paper proposes compute-optimal TTS, which fine-tunes this approach for maximum efficiency. <br><br> .. <br><br> üìä And the results: <br> &nbsp; üëâ A 1B parameter model outperformed GPT-4o on mathematical reasoning tasks (MATH-500). <br> &nbsp; üëâ A 3B model beat Llama-3.1-405B, which is 135x larger. <br> &nbsp; üëâ A 7B model outperformed leading systems like o1 and DeepSeek-R1 on reasoning benchmarks. <br><br> This shows that reasoning efficiency can outmatch brute force scaling. | LLM Inference |
| []() |  |  |


## Week 3/4
| Title | Summary | Topics |
| --- | --- | --- |
| []() |  |  |
| []() |  |  |



## Week 4/4
| Title | Summary | Topics |
| --- | --- | --- |
| []() |  |  |
| []() |  |  |
