## Week 1/4
| Title | Summary | Topics |
| --- | --- | --- |
| [Memory-Efficient Fine-Tuning of Transformers via Token Selection](https://www.arxiv.org/pdf/2501.18824) | Fine-tuning LLMs is essential, but GPU memory is the bottleneck. <br><br> And this is a big deal because even the best memory-efficient methods like LoRA and QLoRA still need to cache all intermediate activations during the forward pass. <br><br> Here‚Äôs where TOKENTUNE comes in - a recent research from Meta AI <br><br> .. <br><br> To fine-tune a model like Llama2-7B: <br><br> &nbsp; -> QLoRA reduces memory usage but still needs 37% of full fine-tuning memory. <br><br> &nbsp; -> TOKENTUNE cuts that down further, needing just 21% of full fine-tuning memory when combined with QLoRA, saving ~79% memory overall. <br><br> How? <br><br> &nbsp; -> Instead of caching all tokens during backpropagation, it selects a subset of tokens for gradient computation while freezing the rest. <br><br> &nbsp; -> This dramatically reduces the memory footprint without hurting accuracy. <br><br> .. <br><br> So, what does this mean in practice? <br><br> &nbsp; 1> Fine-tuning large models becomes accessible even on smaller GPUs like the A100 or H100. <br><br> &nbsp; 2> Costs drop significantly for both research and deployment. <br><br> &nbsp; 3> It works on its own or with other methods, making it highly flexible. <br><br> But it also raises questions. <br><br> &nbsp; -> What happens when token selection isn‚Äôt optimal for specific tasks? <br><br> &nbsp;  -> Can this approach scale to models much larger than Llama2-7B? | fine-tuning |
| [SETS: Leveraging Self-Verification and Self-Correction for Improved Test-Time Scaling](https://arxiv.org/pdf/2501.19306) | Improving LLM performance during inference is always tricky. <br><br> And this is a big deal because traditional methods like repeated sampling or task-specific reward models quickly hit diminishing returns as compute scales. <br><br> .. <br><br> To improve outputs, most methods rely on sampling multiple responses and using majority votes. But here‚Äôs the catch: <br><br> &nbsp; -> As the number of samples increases, performance gains plateau. <br><br> &nbsp; -> Adding a task-specific reward model to verify solutions drives up costs and reduces scalability. <br><br> SETS takes a different approach. <br><br> .. <br><br> Instead of relying on just sampling: <br><br> &nbsp; -> It introduces Self-Verification to check if an answer meets constraints. <br><br> &nbsp; -> Then uses Self-Correction to refine incorrect responses. <br><br> &nbsp; -> This iterative process improves responses while keeping costs under control.<br><br> And the results? <br><br> &nbsp; -> On tasks like trip and meeting planning (NATURAL PLAN benchmark), SETS improved accuracy by up to 8.7% over repeated sampling. <br><br> &nbsp;  -> Confidence scores also got better, with up to 9% higher AUACC and reduced calibration errors, ensuring more reliable outputs. <br><br> .. <br><br> But the real kicker is efficiency: <br><br> SETS doesn‚Äôt need external reward models or extra fine-tuning. It works with existing LLM capabilities, making it scalable across tasks. <br><br> However, the effectiveness of SETS depends heavily on the underlying model's self-verification and correction abilities. Stronger these are, the better SETS performs. | LLM Reasoning |
| [Are Language Models Up to Sequential Optimization Problems? From Evaluation to a Hegelian-Inspired Enhancement](https://arxiv.org/pdf/2502.02573) | LLMs are being tested on everything, but what about solving Sequential Optimization Problems (SOPs)? <br><br> And this is a big question because SOPs - where decisions are made in sequence, each impacting the next - are critical in logistics, engineering, and more. <br><br> So, I dived into the paper to understand how well LLMs perform and how we can make them better. <br><br> .. <br><br> To evaluate LLMs, this recent paper introduces WorldGen, a framework to dynamically generate SOPs of varying complexity. <br><br> Here are the key findings: <br><br> &nbsp; -> For simple tasks (no local optima), models like GPT-4 performed well. <br><br> &nbsp; -> But as complexity increased (e.g., multiple local optima), success rates dropped drastically - down to 4% on medium-complexity tasks. <br><br> Clearly, LLMs struggle when the problems get real. <br><br> .. <br><br> To address this, they introduced a framework inspired by Hegelian Dialectics, called ACE (Actor, Critic, Synthesizer) <br><br> Here‚Äôs how it works: <br><br> &nbsp; -> The Actor generates a solution (thesis). <br><br> &nbsp; -> The Critic challenges it with flaws and alternatives (antithesis). <br><br> &nbsp; -> The Synthesizer combines both to refine the solution (synthesis). <br><br> This iterative cycle pushes the model to improve its reasoning dynamically without retraining or fine-tuning. <br><br> .. <br><br> And the results? <br><br> &nbsp; -> Using ACE, success rates jumped from 36% to 88% on simple tasks and improved for medium-complexity tasks too. <br><br> &nbsp; -> ACE outperformed popular techniques like Majority Vote and Debate while consuming fewer resources. <br><br> But ACE relies heavily on the LLM‚Äôs baseline capabilities. A weaker model = limited improvement. | Language Models |
| [CoAT: Chain-of-Associated-Thoughts Framework for Enhancing LLMs Reasoning ](https://arxiv.org/pdf/2502.02390v1) | Most LLMs rely on fast thinking - they generate answers based on a single query and predefined reasoning. <br><br> And this has worked well, but it struggles with problems requiring iterative reasoning or the ability to adapt to new information dynamically. <br><br> This recent research proposes a framework called CoAT (Chain-of-Associated-Thoughts) to enhance the reasoning process of LLMs. <br><br> .. <br><br> Here‚Äôs what CoAT does differently: <br><br> &nbsp; -> Associative Memory dynamically retrieves and integrates new knowledge during reasoning. Think of it as the model learning and updating its thoughts in real time. <br><br> &nbsp; -> It‚Äôs paired with Optimized Monte Carlo Tree Search (MCTS), which expands search spaces methodically, ensuring diverse reasoning paths while refining earlier answers. <br><br> .. <br><br> How does it perform? <br><br> &nbsp; -> On complex reasoning tasks, CoAT outperformed Tree-of-Thought (ToT) and RAG in accuracy, coherence, and diversity. <br><br> &nbsp; -> It generated richer, more relevant content by iteratively incorporating and validating new information. <br><br> And here‚Äôs the kicker:-> CoAT plugs into existing LLMs without retraining. It acts as a framework that enhances reasoning capabilities dynamically. üòé  | LLMs Reasoning |
| [SymAgent: A Neural-Symbolic Self-Learning Agent Framework for Complex Reasoning over Knowledge Graphs](https://arxiv.org/pdf/2502.03283v1) | Integrating LLMs with Knowledge Graphs (KGs) sounds great in theory. <br><br> But here‚Äôs the problem: <br> &nbsp; -> LLMs, while powerful, often hallucinate and struggle with complex reasoning tasks. <br> &nbsp; -> KGs, though reliable, are static and incomplete - they can‚Äôt handle dynamic, multi-step reasoning. <br><br> This recent framewok called SymAgent addresses these challenges. <br><br> .. <br><br> Here‚Äôs what it does differently: <br> &nbsp; -> SymAgent treats KGs as dynamic environments, transforming reasoning into an interactive, multi-step process. <br> &nbsp; -> It combines two key modules: <br> &nbsp; &nbsp; 1> Agent-Planner: Extracts symbolic rules from KGs to guide reasoning and break down complex questions. <br> &nbsp; &nbsp;  2> Agent-Executor: Dynamically retrieves information from KGs and external sources to address incompleteness. <br><br> And to top it off, it has a self-learning framework that enables the agent to improve over time without human supervision. <br><br> .. <br><br> The results? <br> &nbsp; -> On reasoning tasks, SymAgent outperformed other methods, even with weaker LLMs like Llama2-7B. <br> &nbsp; -> It improved KG quality by identifying missing triples and automatically updating them. <br> &nbsp; -> On average, it boosted reasoning accuracy by up to 37% across datasets. | Reasoning over KGs |
| []() |  |  |


## Week 2/4
| Title | Summary | Topics |
| --- | --- | --- |
| [LLMs can be easily Confused by Instructional Distractions](https://www.arxiv.org/pdf/2502.04362) | LLMs are great at following instructions, right? <br><br> But here‚Äôs the catch - what happens when the input itself looks like an instruction? ü§î  <br><br> This recent paper proposes DIM-Bench, a new benchmark designed to test this issue, called instructional distraction. <br><br> .. <br><br> üßê Here‚Äôs the problem: <br> &nbsp; üëâ LLMs are good at tasks like translation or rewriting, but when the input mimics an instruction (e.g., ‚Äúsolve this math problem‚Äù), they often get confused. <br> &nbsp; üëâ Instead of following the main instruction, they end up addressing the distracting input, even when both are clearly separated. <br><br> DIM-Bench evaluates how well LLMs handle this across 20 task combinations. <br><br> .. <br><br> And the results? <br> &nbsp; üëâ Models like GPT-4o and Llama-3.1-70B struggle. For example: <br> &nbsp; &nbsp;  -> In translation tasks, accuracy is 52.6%, but drops to 5.1% when paired with question-answering distractions. <br> &nbsp; &nbsp;  -> For rewriting + math problems, models often skip rewriting entirely and solve the math instead. <br> &nbsp; üëâ Prompting techniques like Chain-of-Thought reasoning help slightly, but none fully solve the issue. | LLMs Instruction following |
| [Can 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling](https://arxiv.org/pdf/2502.06703) | Scaling up LLMs has always been seen as the way forward. <br><br> But here‚Äôs the thing - Bigger models like GPT-4o deliver exceptional performance, but they are expensive and resource-heavy.  <br><br> Smaller models, on the other hand, are cheaper but often fail at handling complex reasoning tasks. <br><br> So, I dived into the idea of Test-Time Scaling (TTS) to see how it can help smaller models punch above their weight. <br><br> .. <br><br> ‚òòÔ∏è Here‚Äôs what TTS does: <br> &nbsp; üëâ Instead of making the model bigger, it optimally uses compute during inference. <br> &nbsp; üëâ Techniques like beam search, verifier trees, and reward models are used to refine reasoning without increasing model size. <br><br> The paper proposes compute-optimal TTS, which fine-tunes this approach for maximum efficiency. <br><br> .. <br><br> üìä And the results: <br> &nbsp; üëâ A 1B parameter model outperformed GPT-4o on mathematical reasoning tasks (MATH-500). <br> &nbsp; üëâ A 3B model beat Llama-3.1-405B, which is 135x larger. <br> &nbsp; üëâ A 7B model outperformed leading systems like o1 and DeepSeek-R1 on reasoning benchmarks. <br><br> This shows that reasoning efficiency can outmatch brute force scaling. | LLM Inference |
| [LLMs Can Easily Learn to Reason from Demonstrations Structure, not content, is what matters!](https://arxiv.org/pdf/2502.07374) | Most LLM improvements focus on more data or bigger models. <br><br> And this has worked, but what if structure matters more than content when it comes to reasoning? <br><br> So, I looked into Long Chain-of-Thought (Long CoT) reasoning and how models actually learn multi-step reasoning. <br><br> .. <br><br> ‚òòÔ∏è Here‚Äôs what the study found: <br> &nbsp; üëâ If a model is trained on incorrect answers but with proper reasoning steps, it still learns effectively. <br> &nbsp; üëâ But if the logical structure is disrupted by shuffling, deleting, or inserting reasoning steps, the accuracy drops significantly. <br><br> This means formatting, step-by-step breakdowns, and logical flow are more important than just having the right final answer. <br><br> .. <br><br> üìä To test this, the authors fine-tuned Qwen2.5-32B-Instruct with just 17k examples and got: <br> &nbsp; üëâ +40.0% on AIME 2024 (56.7% accuracy) <br> &nbsp; üëâ +8.1% on LiveCodeBench (57.0% accuracy) <br> &nbsp; üëâ Performance close to OpenAI o1-preview <br><br> Even more surprising? The model retained this reasoning ability with just 5% parameter updates using LoRA fine-tuning. üòé  | LLM Reasoning |
| [Auditing Prompt Caching in Language Model APIs](https://arxiv.org/pdf/2502.07776) | Most people do not think about prompt caching when using LLM APIs. <br><br> And this makes sense because caching improves response times and reduces compute costs. <br><br> But here‚Äôs the issue - prompt caching introduces side-channel timing attacks, where an attacker can infer what other users are prompting based on response time differences. <br><br> So, I looked into this recent study that audited real-world LLM APIs to check for prompt caching and its privacy risks. <br><br> .. <br><br> üßê Here‚Äôs what they found: <br> &nbsp;  üëâ 8 out of 17 API providers had prompt caching enabled. <br> &nbsp;  üëâ 7 of them shared cache globally, meaning attackers could potentially detect if another user sent a similar prompt. üëâ OpenAI‚Äôs text-embedding-3-small model was found to be decoder-only, something OpenAI had not publicly disclosed. <br><br> This means that even if API providers do not openly share caching policies, their implementations might still expose sensitive information. <br><br> .. <br><br> ü§î BTW, How does this attack work? <br> &nbsp;  üëâ If a prompt is cached, future calls with similar prefixes get processed faster. <br> &nbsp;  üëâ An attacker can send test prompts and measure response times to infer whether a specific phrase was recently used. <br> &nbsp;  üëâ This could expose user queries, company-sensitive prompts, or even leak details about proprietary model architectures. | Prompt Caching |
| [Logical Reasoning in Large Language Models: A Survey](https://arxiv.org/pdf/2502.09100) | LLMs are getting better at a lot of things. <br><br> But when it comes to logical reasoning, even with the emergence of advanced reasoning models like OpenAI o3 and DeepSeek-R1, they still struggle. <br><br> And this is a big problem because AI is being used in law, science, and decision-making, where logical consistency is not optional. <br><br> This recent survey breaks down how well LLMs reason, where they fail, and what researchers are doing about it. <br><br> .. <br><br> ‚òòÔ∏è Here‚Äôs where things stand: <br> &nbsp;  üëâ Models like GPT-4o and DeepSeek-R1 are good at heuristic reasoning but fail at formal logic and multi-step deduction. <br> &nbsp;  üëâ Many benchmarks mix pattern recognition with actual logic, so it is unclear if models are really reasoning or just memorizing. <br> &nbsp;  üëâ There are four key reasoning types - deductive, inductive, abductive, and analogical and each one exposes different weaknesses in LLMs. <br><br> Researchers are now trying different methods to fix these issues. <br><br> .. <br><br> üìä What is working? <br> &nbsp;  üëâ Better Training Data: Expert-curated datasets and synthetic logic-based examples help refine reasoning. <br> &nbsp;  üëâ Model Improvements: Fine-tuning, reinforcement learning, and inference-time strategies boost consistency. <br> &nbsp;  üëâ Neuro-Symbolic AI: Combining deep learning with structured logic makes models more rigorous in reasoning. <br><br> But even with these techniques, the fundamental gaps remain. | LLM Reasoning |
| []() |  |  |


## Week 3/4
| Title | Summary | Topics |
| --- | --- | --- |
| [Do Large Language Models Reason Causally Like Us? Even Better?](https://arxiv.org/pdf/2502.10215) | LLMs can generate human-like responses. <br><br> But when it comes to causal reasoning, even the most advanced models like GPT-4o and Claude-3 are far from perfect. <br><br> And this is a big problem because causal inference is crucial in areas like medicine, policy, and scientific research, where correlation is not enough -models need to understand cause and effect. <br><br> This recent study examines how well LLMs perform causal reasoning, where they align with humans, and where they fail. <br><br> ‚òòÔ∏è Here‚Äôs where things stand: <br> &nbsp;  üëâ GPT-4o and Claude-3 performed best, demonstrating an effect called explaining away, where the presence of one cause reduces the likelihood of another. <br> &nbsp; üëâ Gemini-Pro and GPT-3.5 struggled, relying more on associative reasoning rather than true causal inference. <br> &nbsp;  üëâ All models violated independence assumptions, meaning they sometimes treated unrelated causes as correlated. <br><br> Researchers tested these models using collider graphs, a method to evaluate how AI understands causal relationships. <br><br> üìä What is working? <br> &nbsp;  üëâ Structured Causal Models (SCMs): Using predefined causal structures improves model predictions. <br> &nbsp;  üëâ Fine-tuning on causal datasets: Helps align LLMs closer to human-like causal reasoning. <br> &nbsp;  üëâ Neuro-symbolic approaches: Combining statistical AI with symbolic logic strengthens causal inference - lots of talk about this these days, something to notice ü´°  <br><br> But even with these improvements, LLMs still struggle with generalizing causal relationships beyond their training data. | LLM Casual Reasoning |
| [A-MEM: Agentic Memory for LLM Agents](https://arxiv.org/pdf/2502.12110v1) | LLM agents are getting better at reasoning, but their memory? A mess! üßê  <br><br> When it comes to long-term memory, they still rely on basic retrieval systems that do not evolve over time. <br><br> And this is a big issue because AI agents need adaptive memory to recall past interactions, learn from new information, and reason across sessions.This recent paper proposes A-MEM, a new agentic memory system designed to make LLMs more flexible and capable of evolving their knowledge. <br><br> ‚òòÔ∏è Here is what makes A-MEM different: <br> &nbsp;  üëâ It does not just store information like a database. Instead, it dynamically restructures knowledge as new data is added. <br> &nbsp;  üëâ It is inspired by Zettelkasten, a human-like note-taking system where memories are linked with contextual descriptions and structured keywords. <br> &nbsp;  üëâ It updates itself over time, refining past knowledge instead of treating memory as static. <br><br> üìä What does this change? <br> &nbsp;  üëâ More adaptive reasoning: A-MEM allows LLMs to connect ideas across sessions, rather than just searching for keywords. <br> &nbsp;  üëâ Improved long-term coherence: AI agents can maintain context across multiple interactions without forgetting past details. <br> &nbsp;  üëâ Better performance: Empirical tests on six foundation models show A-MEM outperforms existing memory systems, leading to more reliable and consistent outputs. | Agent Memory |
| [Inner Thinking Transformer: Leveraging Dynamic Depth Scaling to Foster Adaptive Internal Thinking](https://arxiv.org/pdf/2502.13842v1) | LLMs are getting bigger, but does that mean they‚Äôre getting better? Not always! <br><br> Scaling up parameters is expensive. Smaller models? They struggle with complex reasoning. <br><br> Here's where the problem starts. <br> &nbsp;  üëâ LLMs treat every token equally, whether it's a simple word or a critical reasoning step. <br> &nbsp;  üëâ Harder tokens cause sudden spikes in gradients, showing that the model isn‚Äôt thinking deeply enough where it should. <br> &nbsp;  üëâ And yet, adding more parameters just to fix this is wasteful. <br><br> So, what‚Äôs the alternative? <br><br> This recent study proposes a potential alternative called - Inner Thinking Transformer (ITT). Here's how it works: <br> &nbsp;  üëâ Instead of processing all tokens uniformly, ITT allocates more computation to the hard ones. <br> &nbsp;  üëâ It revisits and refines difficult tokens through residual thinking connections. <br> &nbsp;  üëâ It differentiates reasoning phases, making its "thought process" more structured. <br><br> And here‚Äôs what happens: <br> &nbsp;  üëâ A 162M parameter model with ITT performs at 96.5% of a 466M Transformer while using 43.2% less training data. | Transformers |
| [Optimizing Model Selection for Compound AI Systems](https://arxiv.org/pdf/2502.14815) | Should AI systems learn to ‚Äúhire‚Äù the right LLM for each task, just like we hire the right expert for a job? <br><br> Most AI systems today rely on one LLM for everything. <br><br> Self-refining agents, multi-agent debates, reasoning pipelines - all call the same model across every step. <br><br> And that‚Äôs a problem. <br> &nbsp;  -> Some LLMs are great at generation, others at critique, some at fact verification. <br> &nbsp;  -> But when you use a single model for all steps, you get suboptimal performance. <br> &nbsp;  -> Fixing this manually? Impossible - the search space is exponential.<br><br> This recent paper proposes LLMSelector, a framework that dynamically picks the best LLM for each step. <br><br> How? <br> &nbsp;  -> It analyzes how different models perform on specific tasks. <br> &nbsp;  -> It allocates models based on their strengths - like an AI hiring manager. <br> &nbsp;  -> And it achieves 5%-70% accuracy gains across benchmarks. <br><br> And the best part? <br><br> It doesn‚Äôt require scaling up models - just using them smarter. | LLM Selector |
| []() |  |  |


## Week 4/4
| Title | Summary | Topics |
| --- | --- | --- |
| [PIP-KAG: Mitigating Knowledge Conflicts in Knowledge-Augmented Generation via Parametric Pruning](https://arxiv.org/pdf/2502.15543) | LLMs are integrating external knowledge better than ever. <br><br> And yet, they still struggle when external facts contradict their internal memory. <br><br> So, I looked at why this happens -  <br> &nbsp; -> LLMs rely on both parametric knowledge (stored in weights) and retrieved knowledge (from external sources). <br> &nbsp; -> But when these two sources conflict, models often default to their internal memory, even when it is outdated or incorrect. <br> &nbsp; -> Fixing this? Not easy - prompting and fine-tuning help, but only partially. <br><br> So, what‚Äôs the alternative? <br><br> This recent paper proposes and interesting approach called - Parametric Pruning-based KnowledgeAugmented Generation (PIP-KAG). <br> &nbsp; -> It prunes outdated knowledge, removing conflicting parameters from the model. <br> &nbsp; -> It installs an external knowledge preference module, helping the model rely more on retrieved facts. <br> &nbsp; -> And it reduces model size by 13%, making it more efficient without hurting performance. <br><br> And the results? <br><br> Significantly fewer hallucinations and better alignment with external facts. | Knowledge-Augmented Generation |
| []() |  |  |
| []() |  |  |
