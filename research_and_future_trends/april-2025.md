## Week 1/4
| Title | Summary | Topics |
| --- | --- | --- |
| [Think Before Recommend: Unleashing the Latent Reasoning Power for Sequential Recommendation](https://arxiv.org/pdf/2503.22675) | Most recommendation systems today work in a one-pass manner. User interacts, the model predicts, and that’s it. <br><br> And this is a problem because user preferences aren’t static. They evolve, shift, and sometimes contradict past behavior.  <br><br> Yet, traditional models don’t take a moment to rethink their suggestions before serving them.  <br><br> The usual method? Simple sequential predictions. <br> &nbsp;  -> User clicks on a few items, the model picks up a pattern, and boom - a recommendation appears. <br> &nbsp;  -> But if the user’s interests are nuanced or sparse, the model struggles to get it right. <br><br> So, what’s different now? <br> &nbsp;  -> A reasoning-based approach where the model revisits its own predictions before making a decision. <br> &nbsp;  -> Two strategies - one that ensembles multiple reasoning paths, another that progressively refines recommendations step by step. <br><br> And the results? <br> &nbsp;  -> Better personalization, especially for long-tail and sparse data scenarios. <br> &nbsp;  -> Significant accuracy boosts, all with a minimal increase in processing time. <br><br> So, what are our options? <br> &nbsp;  1> Keep using traditional models that only skim the surface of user preferences. <br> &nbsp;  2> Shift towards models that refine their predictions by reasoning over past interactions. <br><br> And then, there’s one more possibility. <br><br> See, we need recommendation systems that aren’t just fast, but thoughtful. <br> &nbsp;  -> They need to process feedback dynamically, rather than sticking to rigid patterns. <br> &nbsp;  -> They need to be designed for long-term learning, not just quick suggestions. <br> &nbsp;  -> They need models that don’t just predict - but think. <br><br> The question is - will we make this shift? <br><br> We’ve already seen how naive recommendation systems fall short in real-world applications. The only way forward is to build models that take a second to think before they suggest.| Recommender System |
| [Effectively Controlling Reasoning Models through Thinking Intervention](https://arxiv.org/pdf/2503.24370v1) | The approach involves guiding the model's reasoning process by injecting specific thinking tokens during the intermediate steps. <br><br> By doing so, the models not only follow instructions better but also manage complex tasks like instruction hierarchy and safety alignment more effectively. <br><br> The idea is simple yet powerful: rather than simply providing input prompts, we can actively intervene during the model's reasoning phase.  <br><br> This method helps ensure that the model's output is more aligned with the intended task.  <br><br> For example, when handling instructions that require a specific format or safety protocols, a small intervention can lead to measurable improvements in performance and safety. <br><br> Key heighlights include:  <br> &nbsp;  -> Enhanced instruction following by incorporating dynamic, context-specific guidance. <br> &nbsp;  -> Improved management of instruction hierarchies, allowing models to prioritize key tasks. <br> &nbsp;  -> A boost in safety alignment, ensuring that AI responses adhere to necessary safety standards. | LLM Reasoning |



## Week 2/4
| Title | Summary | Topics |
| --- | --- | --- |
| []() |  |  |
| []() |  |  |



## Week 3/4
| Title | Summary | Topics |
| --- | --- | --- |
| []() |  |  |
| []() |  |  |



## Week 4/4
| Title | Summary | Topics |
| --- | --- | --- |
| []() |  |  |
| []() |  |  |
