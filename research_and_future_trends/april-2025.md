## Week 1/4
| Title | Summary | Topics |
| --- | --- | --- |
| [Think Before Recommend: Unleashing the Latent Reasoning Power for Sequential Recommendation](https://arxiv.org/pdf/2503.22675) | Most recommendation systems today work in a one-pass manner. User interacts, the model predicts, and that’s it. <br><br> And this is a problem because user preferences aren’t static. They evolve, shift, and sometimes contradict past behavior.  <br><br> Yet, traditional models don’t take a moment to rethink their suggestions before serving them.  <br><br> The usual method? Simple sequential predictions. <br> &nbsp;  -> User clicks on a few items, the model picks up a pattern, and boom - a recommendation appears. <br> &nbsp;  -> But if the user’s interests are nuanced or sparse, the model struggles to get it right. <br><br> So, what’s different now? <br> &nbsp;  -> A reasoning-based approach where the model revisits its own predictions before making a decision. <br> &nbsp;  -> Two strategies - one that ensembles multiple reasoning paths, another that progressively refines recommendations step by step. <br><br> And the results? <br> &nbsp;  -> Better personalization, especially for long-tail and sparse data scenarios. <br> &nbsp;  -> Significant accuracy boosts, all with a minimal increase in processing time. <br><br> So, what are our options? <br> &nbsp;  1> Keep using traditional models that only skim the surface of user preferences. <br> &nbsp;  2> Shift towards models that refine their predictions by reasoning over past interactions. <br><br> And then, there’s one more possibility. <br><br> See, we need recommendation systems that aren’t just fast, but thoughtful. <br> &nbsp;  -> They need to process feedback dynamically, rather than sticking to rigid patterns. <br> &nbsp;  -> They need to be designed for long-term learning, not just quick suggestions. <br> &nbsp;  -> They need models that don’t just predict - but think. <br><br> The question is - will we make this shift? <br><br> We’ve already seen how naive recommendation systems fall short in real-world applications. The only way forward is to build models that take a second to think before they suggest.| Recommender System |
| [Effectively Controlling Reasoning Models through Thinking Intervention](https://arxiv.org/pdf/2503.24370v1) | The approach involves guiding the model's reasoning process by injecting specific thinking tokens during the intermediate steps. <br><br> By doing so, the models not only follow instructions better but also manage complex tasks like instruction hierarchy and safety alignment more effectively. <br><br> The idea is simple yet powerful: rather than simply providing input prompts, we can actively intervene during the model's reasoning phase.  <br><br> This method helps ensure that the model's output is more aligned with the intended task.  <br><br> For example, when handling instructions that require a specific format or safety protocols, a small intervention can lead to measurable improvements in performance and safety. <br><br> Key heighlights include:  <br> &nbsp;  -> Enhanced instruction following by incorporating dynamic, context-specific guidance. <br> &nbsp;  -> Improved management of instruction hierarchies, allowing models to prioritize key tasks. <br> &nbsp;  -> A boost in safety alignment, ensuring that AI responses adhere to necessary safety standards. | LLM Reasoning |
| [Harnessing the Reasoning Economy - A Survey of Efficient Reasoning for Large Language Models](https://arxiv.org/pdf/2503.24377) | This new survey dives into the concept of "reasoning economy" for large language models (LLMs), exploring how to optimize their decision-making without wasting computational resources. <br><br> The paper highlights two modes: <br> &nbsp; 🔹 System 1 – Quick, intuitive responses (think: instant answers). <br> &nbsp; 🔹 System 2 – Slow, deliberate reasoning (think: solving complex math). <br><br> But here’s the catch: Over-relying on deep thinking can lead to "overthinking" (redundant steps) or "underthinking" (missing critical analysis).  <br><br> The study unpacks solutions like adaptive computation (tailoring effort to task complexity) and reward model optimizations to cut unnecessary steps while preserving accuracy. <br><br> Results? <br> &nbsp; ✅ Smarter resource use = faster, cheaper AI applications. <br> &nbsp; ✅ Better alignment with human-like reasoning efficiency. | LLM Reasoning |
| [Scaling Language-Free Visual Representation Learning](https://arxiv.org/pdf/2504.01017) | This recent study challenges the belief that language supervision (like CLIP) is essential for training powerful vision models. Researchers trained self-supervised learning (SSL) models on the same web-scale data as CLIP (MetaCLIP’s 2B images) and found something surprising: <br><br> 🔍 Key Insights: <br> &nbsp;  🔹 SSL scales better: When models grow to 7B parameters, SSL outperforms CLIP on diverse tasks like Visual Question Answering (VQA), including text-heavy challenges like OCR & charts. <br> &nbsp;  🔹 Data quality > labels: Curating images with text (e.g., signs, charts) boosted OCR performance by +13.6% - no language supervision needed! <br> &nbsp;  🔹 Classic tasks stay strong: SSL maintains competitive accuracy on classification and segmentation, proving versatility. <br><br> It suggests vision models can learn rich, language-aligned features purely from images, opening doors for applications where paired text is scarce or biased. 💡  <br><br> Scaling SSL further, optimizing data composition, and exploring how vision-centric models can complement (or even replace) language-supervised approaches. | Vision Learning |
| [Why do LLMs attend to the first token? �](https://arxiv.org/pdf/2504.02732) | this recent study audits attention sinks in transformer models to see why they lean on the first token. <br><br> 🧐 Here’s what they found: <br> &nbsp;  👉 The first token acts as a buffer that absorbs a significant portion of attention, preventing excessive mixing. <br> &nbsp;  👉 Larger models and those trained on longer contexts develop stronger attention sinks to keep information stable. <br> &nbsp;  👉 Experiments on models like Gemma 7B and the LLaMa 3.1 family reveal that this strategy effectively controls perturbations in later tokens. <br><br> This means that even if it appears that the first token is overburdened, it is a deliberate design choice to enhance model stability. <br><br> 🤔 BTW, how does this mechanism work? <br> &nbsp;  👉 By focusing attention on the first token, the model limits the spread of small changes across the network. <br> &nbsp;  👉 This controlled mixing keeps the representations robust, reducing the risks of over-smoothing and representational collapse. <br> &nbsp;  👉 As a result, even with very long sequences, the model's output remains stable and reliable. | LLMs |
| []() |  |  |


## Week 2/4
| Title | Summary | Topics |
| --- | --- | --- |
| [Do Larger Language Models Imply Better Reasoning? A Pretraining Scaling Law for Reasoning](https://arxiv.org/pdf/2504.03635) | Bigger AI Models ≠ Better Reasoning 🤔 <br><br> This new study reveals a counterintuitive truth: massive language models often underperform on reasoning tasks compared to mid-sized counterparts. Here’s why: <br><br> 🔑  Key Findings: <br> &nbsp; 1/ U-Shaped Performance Curve: Reasoning accuracy drops when models get too big. Overparameterization leads to memorization over genuine reasoning. <br> &nbsp; 2/ Optimal Model Size Exists: Performance peaks at a sweet spot tied to your data’s complexity. Think "Goldilocks zone" for AI. <br> &nbsp; 3/ Graph Search Entropy: A new metric predicts ideal model size. Complexity = 124 extra parameters per 1-bit entropy increase.. Interesting! <br> &nbsp; 4/ Memorization ≠ Reasoning: Big models parrot patterns but struggle to connect unseen dots. <br> &nbsp; 5/ Cost Efficiency: Bigger isn’t better. Smaller, optimized models save compute without sacrificing reasoning. <br> &nbsp; 6/ Data Complexity Rules: Dense knowledge graphs? Prioritize model depth. Sparse data? Scale cautiously. | LLM Reasoning |
| [SmolVLM: Redefining small and efficient multimodal models](https://arxiv.org/pdf/2504.05299) | Hugging Face recently released SmolVLM - a family of ultra-efficient AI models that punch way above their weight class. <br><br> ____ <br> 🔑 Key Highlights: <br> &nbsp; 1/ Tiny Footprint, Big Impact: The smallest model (256M params) uses <1GB GPU memory, yet outperforms models 300x larger on tasks like OCR and document understanding. <br> &nbsp; 2/ Video Mastery on Edge Devices: Handles video comprehension seamlessly, scoring 52.1% on Video-MME (a top benchmark) with just 4.9GB VRAM. <br> &nbsp; 3/ Smarter Tokenization: Aggressive pixel-shuffling and sub-image splitting reduce visual tokens by 4x, maintaining accuracy while slashing compute costs. <br> &nbsp; 4/ Cost-Effective Scaling: Larger isn’t better. SmolVLM-2.2B rivals 7B-parameter models with half the memory. <br> &nbsp; 5/ Balanced Design: Optimized vision-language parameter ratios prevent overfitting common in compact models. | VLMs |
| [Reasoning Models Know When They’re Right: Probing Hidden States for Self-Verification](https://arxiv.org/pdf/2504.05419) | We often think of large language models as black boxes - generating answers without really knowing if they’re correct. But what if they do know? And just… don’t act on it? <br><br>That’s what this new study explores... <br><br>Researchers probed the hidden states of reasoning models like DeepSeek-R1 and Qwen to ask: Can these models detect if an answer is correct mid-reasoning? <br><br>Turns out, yes. <br><br>📌 Key takeaways: <br> &nbsp;  → A simple probe can extract correctness signals from hidden states with >90% accuracy in some cases <br> &nbsp;  → These models encode confidence before an answer is even fully formed <br> &nbsp;  → Using this signal to stop reasoning early saved up to 24% tokens with no loss in accuracy | LLM Reasoning |
| [OLMOTRACE: Tracing Language Model Outputs Back to Trillions of Training Tokens](https://arxiv.org/pdf/2504.07096) | What if you could trace an AI’s output back to its exact source in its training data - even across trillions of tokens? Meet OLMoTrace, a new tool that does just that - in real time! <br><br> 🔍 Modern language models are trained on massive datasets, but understanding why they generate specific responses has always been a challenge.  <br><br> OLMoTrace solves this by pinpointing verbatim matches between model outputs and their training data.  <br><br> Using the open-source infini-gram engine, it highlights text spans and shows their original sources - all within ~4.5 seconds per query! <br><br> It could really help in:  <br> &nbsp;  1/ Fact-checking: Verify if a model’s statement aligns with reliable sources. <br> &nbsp;  2/ Creativity Check: Discover if "original" phrases are actually learned from training docs. <br> &nbsp;  3/ Math Tracing: See how models solve problems by matching steps to training examples. | Model Tracing |
| []() |  |  |
| []() |  |  |


## Week 3/4
| Title | Summary | Topics |
| --- | --- | --- |
| [Leveraging Reasoning Model Answers to Enhance Non-Reasoning Model Capability](https://arxiv.org/pdf/2504.09639) | This new study explores how leveraging outputs from advanced reasoning models can significantly boost the performance of simpler, non-reasoning models - without hefty computational costs. <br><br> 🔑  Key Insights from the Paper <br> &nbsp; 1/ Methodology: Using Supervised Fine-Tuning (SFT), they trained non-reasoning models on high-quality answers generated by reasoning-intensive LLMs like DeepSeek-R1. <br> &nbsp; 2/ Results: Models fine-tuned with reasoning-derived answers showed marked improvements on benchmarks like GSM8K (+8.5%) and HumanEval (+10.4%). However, chat-oriented tasks saw slight dips, highlighting a trade-off between task types. <br> &nbsp; 3/ Techniques Tested: From direct answer extraction to integrating summarized reasoning steps, the study compared multiple strategies. The best approach? Combining concise reasoning summaries with final answers - balancing clarity and depth. | Reasoning LLM |
| [NodeRAG: Structuring Graph-based RAG with Heterogeneous Nodes](https://arxiv.org/pdf/2504.11544) | This new paper introduces NodeRAG, a graph-based framework that transforms information into a heterogeneous graph with distinct node types: <br> &nbsp;  ✅ Semantic units (independent events) <br> &nbsp;  🏷️ Entities (names and keywords) <br> &nbsp;  🔄 Relationships (connections between entities) <br> &nbsp;  📊 Attributes (entity characteristics) <br> &nbsp;  🧩 High-level elements (community insights) <br><br> What makes NodeRAG special is how it: <br> &nbsp;  1/ Decomposes information into fine-grained nodes <br> &nbsp;  2/ Preserves structural relationships between concepts <br> &nbsp;  3/ Uses advanced graph algorithms for precision retrieval <br> &nbsp;  4/ Balances original text with LLM-generated insights <br><br> The results? <br> NodeRAG outperforms previous methods like GraphRAG and LightRAG across multiple benchmarks while using FEWER tokens - proving more efficient retrieval quality doesn't require more context. | RAG |
| []() |  |  |
| []() |  |  |



## Week 4/4
| Title | Summary | Topics |
| --- | --- | --- |
| [A Survey of AI Agent Protocols](https://arxiv.org/pdf/2504.16736) | Here’s the core insight: while LLM agents are rapidly advancing, their communication remains fragmented. Just like the early web struggled without common standards, today’s agent ecosystems lack unified protocols for agents to talk to tools, to data sources, or even to each other. <br><br> 🤔 Imagine what standardized agent protocols could unlock: <br> &nbsp;  -> Agents forming temporary coalitions to solve complex problems <br> &nbsp;  -> Cross-platform collaboration, like your finance bot collaborating with a logistics agent <br> &nbsp;  -> Scalable networks of interoperable, intelligent agents - forming a kind of Agent Internet <br><br> 👀 The paper categorizes current protocols into four types: <br> &nbsp;  1️⃣ Context-Oriented, General-Purpose (e.g. Anthropic’s MCP 🧭) <br> &nbsp;  2️⃣ Context-Oriented, Domain-Specific (e.g. agents.json) <br> &nbsp;  3️⃣ Inter-Agent, General-Purpose (e.g. Google’s A2A or the open-source ANP) <br> &nbsp;  4️⃣ Inter-Agent, Domain-Specific (e.g. PXP for human-agent interaction, LMOS for IoT+agents) <br><br> It also evaluates protocols across 7 critical dimensions like: <br> &nbsp;  ⚡ Efficiency  <br> &nbsp;  📈 Scalability  <br> &nbsp;  🔐 Security  <br> &nbsp;  📡 Reliability  <br> &nbsp;  🧩 Extensibility  <br> &nbsp;  🔧 Operability  <br> &nbsp;  🌍 Interoperability  <br><br> 🌱 We’re still in the early days, and standardization could be the soil that enables the next generation of collaborative, distributed intelligence. <br><br> If you're building or deploying agents today, keep protocol design top-of-mind. Interoperability might not just be a nice-to-have, it could be the multiplier that unlocks value across the entire ecosystem. | AI Agents |
| []() |  |  |
