## Week 1/4
| Title | Summary | Topics |
| --- | --- | --- |
| [Think Before Recommend: Unleashing the Latent Reasoning Power for Sequential Recommendation](https://arxiv.org/pdf/2503.22675) | Most recommendation systems today work in a one-pass manner. User interacts, the model predicts, and that’s it. <br><br> And this is a problem because user preferences aren’t static. They evolve, shift, and sometimes contradict past behavior.  <br><br> Yet, traditional models don’t take a moment to rethink their suggestions before serving them.  <br><br> The usual method? Simple sequential predictions. <br> &nbsp;  -> User clicks on a few items, the model picks up a pattern, and boom - a recommendation appears. <br> &nbsp;  -> But if the user’s interests are nuanced or sparse, the model struggles to get it right. <br><br> So, what’s different now? <br> &nbsp;  -> A reasoning-based approach where the model revisits its own predictions before making a decision. <br> &nbsp;  -> Two strategies - one that ensembles multiple reasoning paths, another that progressively refines recommendations step by step. <br><br> And the results? <br> &nbsp;  -> Better personalization, especially for long-tail and sparse data scenarios. <br> &nbsp;  -> Significant accuracy boosts, all with a minimal increase in processing time. <br><br> So, what are our options? <br> &nbsp;  1> Keep using traditional models that only skim the surface of user preferences. <br> &nbsp;  2> Shift towards models that refine their predictions by reasoning over past interactions. <br><br> And then, there’s one more possibility. <br><br> See, we need recommendation systems that aren’t just fast, but thoughtful. <br> &nbsp;  -> They need to process feedback dynamically, rather than sticking to rigid patterns. <br> &nbsp;  -> They need to be designed for long-term learning, not just quick suggestions. <br> &nbsp;  -> They need models that don’t just predict - but think. <br><br> The question is - will we make this shift? <br><br> We’ve already seen how naive recommendation systems fall short in real-world applications. The only way forward is to build models that take a second to think before they suggest.| Recommender System |
| [Effectively Controlling Reasoning Models through Thinking Intervention](https://arxiv.org/pdf/2503.24370v1) | The approach involves guiding the model's reasoning process by injecting specific thinking tokens during the intermediate steps. <br><br> By doing so, the models not only follow instructions better but also manage complex tasks like instruction hierarchy and safety alignment more effectively. <br><br> The idea is simple yet powerful: rather than simply providing input prompts, we can actively intervene during the model's reasoning phase.  <br><br> This method helps ensure that the model's output is more aligned with the intended task.  <br><br> For example, when handling instructions that require a specific format or safety protocols, a small intervention can lead to measurable improvements in performance and safety. <br><br> Key heighlights include:  <br> &nbsp;  -> Enhanced instruction following by incorporating dynamic, context-specific guidance. <br> &nbsp;  -> Improved management of instruction hierarchies, allowing models to prioritize key tasks. <br> &nbsp;  -> A boost in safety alignment, ensuring that AI responses adhere to necessary safety standards. | LLM Reasoning |
| [Harnessing the Reasoning Economy - A Survey of Efficient Reasoning for Large Language Models](https://arxiv.org/pdf/2503.24377) | This new survey dives into the concept of "reasoning economy" for large language models (LLMs), exploring how to optimize their decision-making without wasting computational resources. <br><br> The paper highlights two modes: <br> &nbsp; 🔹 System 1 – Quick, intuitive responses (think: instant answers). <br> &nbsp; 🔹 System 2 – Slow, deliberate reasoning (think: solving complex math). <br><br> But here’s the catch: Over-relying on deep thinking can lead to "overthinking" (redundant steps) or "underthinking" (missing critical analysis).  <br><br> The study unpacks solutions like adaptive computation (tailoring effort to task complexity) and reward model optimizations to cut unnecessary steps while preserving accuracy. <br><br> Results? <br> &nbsp; ✅ Smarter resource use = faster, cheaper AI applications. <br> &nbsp; ✅ Better alignment with human-like reasoning efficiency. | LLM Reasoning |
| [Scaling Language-Free Visual Representation Learning](https://arxiv.org/pdf/2504.01017) | This recent study challenges the belief that language supervision (like CLIP) is essential for training powerful vision models. Researchers trained self-supervised learning (SSL) models on the same web-scale data as CLIP (MetaCLIP’s 2B images) and found something surprising: <br><br> 🔍 Key Insights: <br> &nbsp;  🔹 SSL scales better: When models grow to 7B parameters, SSL outperforms CLIP on diverse tasks like Visual Question Answering (VQA), including text-heavy challenges like OCR & charts. <br> &nbsp;  🔹 Data quality > labels: Curating images with text (e.g., signs, charts) boosted OCR performance by +13.6% - no language supervision needed! <br> &nbsp;  🔹 Classic tasks stay strong: SSL maintains competitive accuracy on classification and segmentation, proving versatility. <br><br> It suggests vision models can learn rich, language-aligned features purely from images, opening doors for applications where paired text is scarce or biased. 💡  <br><br> Scaling SSL further, optimizing data composition, and exploring how vision-centric models can complement (or even replace) language-supervised approaches. | Vision Learning |
| [Why do LLMs attend to the first token? �](https://arxiv.org/pdf/2504.02732) | this recent study audits attention sinks in transformer models to see why they lean on the first token. <br><br> 🧐 Here’s what they found: <br> &nbsp;  👉 The first token acts as a buffer that absorbs a significant portion of attention, preventing excessive mixing. <br> &nbsp;  👉 Larger models and those trained on longer contexts develop stronger attention sinks to keep information stable. <br> &nbsp;  👉 Experiments on models like Gemma 7B and the LLaMa 3.1 family reveal that this strategy effectively controls perturbations in later tokens. <br><br> This means that even if it appears that the first token is overburdened, it is a deliberate design choice to enhance model stability. <br><br> 🤔 BTW, how does this mechanism work? <br> &nbsp;  👉 By focusing attention on the first token, the model limits the spread of small changes across the network. <br> &nbsp;  👉 This controlled mixing keeps the representations robust, reducing the risks of over-smoothing and representational collapse. <br> &nbsp;  👉 As a result, even with very long sequences, the model's output remains stable and reliable. | LLMs |
| []() |  |  |


## Week 2/4
| Title | Summary | Topics |
| --- | --- | --- |
| [Do Larger Language Models Imply Better Reasoning? A Pretraining Scaling Law for Reasoning](https://arxiv.org/pdf/2504.03635) | Bigger AI Models ≠ Better Reasoning 🤔 <br><br> This new study reveals a counterintuitive truth: massive language models often underperform on reasoning tasks compared to mid-sized counterparts. Here’s why: <br><br> 🔑  Key Findings: <br> &nbsp; 1/ U-Shaped Performance Curve: Reasoning accuracy drops when models get too big. Overparameterization leads to memorization over genuine reasoning. <br> &nbsp; 2/ Optimal Model Size Exists: Performance peaks at a sweet spot tied to your data’s complexity. Think "Goldilocks zone" for AI. <br> &nbsp; 3/ Graph Search Entropy: A new metric predicts ideal model size. Complexity = 124 extra parameters per 1-bit entropy increase.. Interesting! <br> &nbsp; 4/ Memorization ≠ Reasoning: Big models parrot patterns but struggle to connect unseen dots. <br> &nbsp; 5/ Cost Efficiency: Bigger isn’t better. Smaller, optimized models save compute without sacrificing reasoning. <br> &nbsp; 6/ Data Complexity Rules: Dense knowledge graphs? Prioritize model depth. Sparse data? Scale cautiously. | LLM Reasoning |
| [SmolVLM: Redefining small and efficient multimodal models](https://arxiv.org/pdf/2504.05299) | Hugging Face recently released SmolVLM - a family of ultra-efficient AI models that punch way above their weight class. <br><br> ____ <br> 🔑 Key Highlights: <br> &nbsp; 1/ Tiny Footprint, Big Impact: The smallest model (256M params) uses <1GB GPU memory, yet outperforms models 300x larger on tasks like OCR and document understanding. <br> &nbsp; 2/ Video Mastery on Edge Devices: Handles video comprehension seamlessly, scoring 52.1% on Video-MME (a top benchmark) with just 4.9GB VRAM. <br> &nbsp; 3/ Smarter Tokenization: Aggressive pixel-shuffling and sub-image splitting reduce visual tokens by 4x, maintaining accuracy while slashing compute costs. <br> &nbsp; 4/ Cost-Effective Scaling: Larger isn’t better. SmolVLM-2.2B rivals 7B-parameter models with half the memory. <br> &nbsp; 5/ Balanced Design: Optimized vision-language parameter ratios prevent overfitting common in compact models. | VLMs |
| [Reasoning Models Know When They’re Right: Probing Hidden States for Self-Verification](https://arxiv.org/pdf/2504.05419) | We often think of large language models as black boxes - generating answers without really knowing if they’re correct. But what if they do know? And just… don’t act on it? <br><br>That’s what this new study explores... <br><br>Researchers probed the hidden states of reasoning models like DeepSeek-R1 and Qwen to ask: Can these models detect if an answer is correct mid-reasoning? <br><br>Turns out, yes. <br><br>📌 Key takeaways: <br> &nbsp;  → A simple probe can extract correctness signals from hidden states with >90% accuracy in some cases <br> &nbsp;  → These models encode confidence before an answer is even fully formed <br> &nbsp;  → Using this signal to stop reasoning early saved up to 24% tokens with no loss in accuracy | LLM Reasoning |
| [OLMOTRACE: Tracing Language Model Outputs Back to Trillions of Training Tokens](https://arxiv.org/pdf/2504.07096) | What if you could trace an AI’s output back to its exact source in its training data - even across trillions of tokens? Meet OLMoTrace, a new tool that does just that - in real time! <br><br> 🔍 Modern language models are trained on massive datasets, but understanding why they generate specific responses has always been a challenge.  <br><br> OLMoTrace solves this by pinpointing verbatim matches between model outputs and their training data.  <br><br> Using the open-source infini-gram engine, it highlights text spans and shows their original sources - all within ~4.5 seconds per query! <br><br> It could really help in:  <br> &nbsp;  1/ Fact-checking: Verify if a model’s statement aligns with reliable sources. <br> &nbsp;  2/ Creativity Check: Discover if "original" phrases are actually learned from training docs. <br> &nbsp;  3/ Math Tracing: See how models solve problems by matching steps to training examples. | Model Tracing |
| []() |  |  |
| []() |  |  |


## Week 3/4
| Title | Summary | Topics |
| --- | --- | --- |
| [Leveraging Reasoning Model Answers to Enhance Non-Reasoning Model Capability](https://arxiv.org/pdf/2504.09639) | This new study explores how leveraging outputs from advanced reasoning models can significantly boost the performance of simpler, non-reasoning models - without hefty computational costs. <br><br> 🔑  Key Insights from the Paper <br> &nbsp; 1/ Methodology: Using Supervised Fine-Tuning (SFT), they trained non-reasoning models on high-quality answers generated by reasoning-intensive LLMs like DeepSeek-R1. <br> &nbsp; 2/ Results: Models fine-tuned with reasoning-derived answers showed marked improvements on benchmarks like GSM8K (+8.5%) and HumanEval (+10.4%). However, chat-oriented tasks saw slight dips, highlighting a trade-off between task types. <br> &nbsp; 3/ Techniques Tested: From direct answer extraction to integrating summarized reasoning steps, the study compared multiple strategies. The best approach? Combining concise reasoning summaries with final answers - balancing clarity and depth. | Reasoning LLM |
| []() |  |  |



## Week 4/4
| Title | Summary | Topics |
| --- | --- | --- |
| []() |  |  |
| []() |  |  |
