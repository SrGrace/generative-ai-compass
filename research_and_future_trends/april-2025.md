## Week 1/4
| Title | Summary | Topics |
| --- | --- | --- |
| [Think Before Recommend: Unleashing the Latent Reasoning Power for Sequential Recommendation](https://arxiv.org/pdf/2503.22675) | Most recommendation systems today work in a one-pass manner. User interacts, the model predicts, and thatâ€™s it. <br><br> And this is a problem because user preferences arenâ€™t static. They evolve, shift, and sometimes contradict past behavior.  <br><br> Yet, traditional models donâ€™t take a moment to rethink their suggestions before serving them.  <br><br> The usual method? Simple sequential predictions. <br> &nbsp;  -> User clicks on a few items, the model picks up a pattern, and boom - a recommendation appears. <br> &nbsp;  -> But if the userâ€™s interests are nuanced or sparse, the model struggles to get it right. <br><br> So, whatâ€™s different now? <br> &nbsp;  -> A reasoning-based approach where the model revisits its own predictions before making a decision. <br> &nbsp;  -> Two strategies - one that ensembles multiple reasoning paths, another that progressively refines recommendations step by step. <br><br> And the results? <br> &nbsp;  -> Better personalization, especially for long-tail and sparse data scenarios. <br> &nbsp;  -> Significant accuracy boosts, all with a minimal increase in processing time. <br><br> So, what are our options? <br> &nbsp;  1> Keep using traditional models that only skim the surface of user preferences. <br> &nbsp;  2> Shift towards models that refine their predictions by reasoning over past interactions. <br><br> And then, thereâ€™s one more possibility. <br><br> See, we need recommendation systems that arenâ€™t just fast, but thoughtful. <br> &nbsp;  -> They need to process feedback dynamically, rather than sticking to rigid patterns. <br> &nbsp;  -> They need to be designed for long-term learning, not just quick suggestions. <br> &nbsp;  -> They need models that donâ€™t just predict - but think. <br><br> The question is - will we make this shift? <br><br> Weâ€™ve already seen how naive recommendation systems fall short in real-world applications. The only way forward is to build models that take a second to think before they suggest.| Recommender System |
| [Effectively Controlling Reasoning Models through Thinking Intervention](https://arxiv.org/pdf/2503.24370v1) | The approach involves guiding the model's reasoning process by injecting specific thinking tokens during the intermediate steps. <br><br> By doing so, the models not only follow instructions better but also manage complex tasks like instruction hierarchy and safety alignment more effectively. <br><br> The idea is simple yet powerful: rather than simply providing input prompts, we can actively intervene during the model's reasoning phase.  <br><br> This method helps ensure that the model's output is more aligned with the intended task.  <br><br> For example, when handling instructions that require a specific format or safety protocols, a small intervention can lead to measurable improvements in performance and safety. <br><br> Key heighlights include:  <br> &nbsp;  -> Enhanced instruction following by incorporating dynamic, context-specific guidance. <br> &nbsp;  -> Improved management of instruction hierarchies, allowing models to prioritize key tasks. <br> &nbsp;  -> A boost in safety alignment, ensuring that AI responses adhere to necessary safety standards. | LLM Reasoning |
| [Harnessing the Reasoning Economy - A Survey of Efficient Reasoning for Large Language Models](https://arxiv.org/pdf/2503.24377) | This new survey dives into the concept of "reasoning economy" for large language models (LLMs), exploring how to optimize their decision-making without wasting computational resources. <br><br> The paper highlights two modes: <br> &nbsp; ğŸ”¹ System 1 â€“ Quick, intuitive responses (think: instant answers). <br> &nbsp; ğŸ”¹ System 2 â€“ Slow, deliberate reasoning (think: solving complex math). <br><br> But hereâ€™s the catch: Over-relying on deep thinking can lead to "overthinking" (redundant steps) or "underthinking" (missing critical analysis).  <br><br> The study unpacks solutions like adaptive computation (tailoring effort to task complexity) and reward model optimizations to cut unnecessary steps while preserving accuracy. <br><br> Results? <br> &nbsp; âœ… Smarter resource use = faster, cheaper AI applications. <br> &nbsp; âœ… Better alignment with human-like reasoning efficiency. | LLM Reasoning |
| [Scaling Language-Free Visual Representation Learning](https://arxiv.org/pdf/2504.01017) | This recent study challenges the belief that language supervision (like CLIP) is essential for training powerful vision models. Researchers trained self-supervised learning (SSL) models on the same web-scale data as CLIP (MetaCLIPâ€™s 2B images) and found something surprising: <br><br> ğŸ” Key Insights: <br> &nbsp;  ğŸ”¹ SSL scales better: When models grow to 7B parameters, SSL outperforms CLIP on diverse tasks like Visual Question Answering (VQA), including text-heavy challenges like OCR & charts. <br> &nbsp;  ğŸ”¹ Data quality > labels: Curating images with text (e.g., signs, charts) boosted OCR performance by +13.6% - no language supervision needed! <br> &nbsp;  ğŸ”¹ Classic tasks stay strong: SSL maintains competitive accuracy on classification and segmentation, proving versatility. <br><br> It suggests vision models can learn rich, language-aligned features purely from images, opening doors for applications where paired text is scarce or biased. ğŸ’¡  <br><br> Scaling SSL further, optimizing data composition, and exploring how vision-centric models can complement (or even replace) language-supervised approaches. | Vision Learning |
| [Why do LLMs attend to the first token? ï¿½](https://arxiv.org/pdf/2504.02732) | this recent study audits attention sinks in transformer models to see why they lean on the first token. <br><br> ğŸ§ Hereâ€™s what they found: <br> &nbsp;  ğŸ‘‰ The first token acts as a buffer that absorbs a significant portion of attention, preventing excessive mixing. <br> &nbsp;  ğŸ‘‰ Larger models and those trained on longer contexts develop stronger attention sinks to keep information stable. <br> &nbsp;  ğŸ‘‰ Experiments on models like Gemma 7B and the LLaMa 3.1 family reveal that this strategy effectively controls perturbations in later tokens. <br><br> This means that even if it appears that the first token is overburdened, it is a deliberate design choice to enhance model stability. <br><br> ğŸ¤” BTW, how does this mechanism work? <br> &nbsp;  ğŸ‘‰ By focusing attention on the first token, the model limits the spread of small changes across the network. <br> &nbsp;  ğŸ‘‰ This controlled mixing keeps the representations robust, reducing the risks of over-smoothing and representational collapse. <br> &nbsp;  ğŸ‘‰ As a result, even with very long sequences, the model's output remains stable and reliable. | LLMs |
| []() |  |  |


## Week 2/4
| Title | Summary | Topics |
| --- | --- | --- |
| [Do Larger Language Models Imply Better Reasoning? A Pretraining Scaling Law for Reasoning](https://arxiv.org/pdf/2504.03635) | Bigger AI Models â‰  Better Reasoning ğŸ¤” <br><br> This new study reveals a counterintuitive truth: massive language models often underperform on reasoning tasks compared to mid-sized counterparts. Hereâ€™s why: <br><br> ğŸ”‘  Key Findings: <br> &nbsp; 1/ U-Shaped Performance Curve: Reasoning accuracy drops when models get too big. Overparameterization leads to memorization over genuine reasoning. <br> &nbsp; 2/ Optimal Model Size Exists: Performance peaks at a sweet spot tied to your dataâ€™s complexity. Think "Goldilocks zone" for AI. <br> &nbsp; 3/ Graph Search Entropy: A new metric predicts ideal model size. Complexity = 124 extra parameters per 1-bit entropy increase.. Interesting! <br> &nbsp; 4/ Memorization â‰  Reasoning: Big models parrot patterns but struggle to connect unseen dots. <br> &nbsp; 5/ Cost Efficiency: Bigger isnâ€™t better. Smaller, optimized models save compute without sacrificing reasoning. <br> &nbsp; 6/ Data Complexity Rules: Dense knowledge graphs? Prioritize model depth. Sparse data? Scale cautiously. | LLM Reasoning |
| [SmolVLM: Redefining small and efficient multimodal models](https://arxiv.org/pdf/2504.05299) | Hugging Face recently released SmolVLM - a family of ultra-efficient AI models that punch way above their weight class. <br><br> ____ <br> ğŸ”‘ Key Highlights: <br> &nbsp; 1/ Tiny Footprint, Big Impact: The smallest model (256M params) uses <1GB GPU memory, yet outperforms models 300x larger on tasks like OCR and document understanding. <br> &nbsp; 2/ Video Mastery on Edge Devices: Handles video comprehension seamlessly, scoring 52.1% on Video-MME (a top benchmark) with just 4.9GB VRAM. <br> &nbsp; 3/ Smarter Tokenization: Aggressive pixel-shuffling and sub-image splitting reduce visual tokens by 4x, maintaining accuracy while slashing compute costs. <br> &nbsp; 4/ Cost-Effective Scaling: Larger isnâ€™t better. SmolVLM-2.2B rivals 7B-parameter models with half the memory. <br> &nbsp; 5/ Balanced Design: Optimized vision-language parameter ratios prevent overfitting common in compact models. | VLMs |
| [Reasoning Models Know When Theyâ€™re Right: Probing Hidden States for Self-Verification](https://arxiv.org/pdf/2504.05419) | We often think of large language models as black boxes - generating answers without really knowing if theyâ€™re correct. But what if they do know? And justâ€¦ donâ€™t act on it? <br><br>Thatâ€™s what this new study explores... <br><br>Researchers probed the hidden states of reasoning models like DeepSeek-R1 and Qwen to ask: Can these models detect if an answer is correct mid-reasoning? <br><br>Turns out, yes. <br><br>ğŸ“Œ Key takeaways: <br> &nbsp;  â†’ A simple probe can extract correctness signals from hidden states with >90% accuracy in some cases <br> &nbsp;  â†’ These models encode confidence before an answer is even fully formed <br> &nbsp;  â†’ Using this signal to stop reasoning early saved up to 24% tokens with no loss in accuracy | LLM Reasoning |
| [OLMOTRACE: Tracing Language Model Outputs Back to Trillions of Training Tokens](https://arxiv.org/pdf/2504.07096) | What if you could trace an AIâ€™s output back to its exact source in its training data - even across trillions of tokens? Meet OLMoTrace, a new tool that does just that - in real time! <br><br> ğŸ” Modern language models are trained on massive datasets, but understanding why they generate specific responses has always been a challenge.  <br><br> OLMoTrace solves this by pinpointing verbatim matches between model outputs and their training data.  <br><br> Using the open-source infini-gram engine, it highlights text spans and shows their original sources - all within ~4.5 seconds per query! <br><br> It could really help in:  <br> &nbsp;  1/ Fact-checking: Verify if a modelâ€™s statement aligns with reliable sources. <br> &nbsp;  2/ Creativity Check: Discover if "original" phrases are actually learned from training docs. <br> &nbsp;  3/ Math Tracing: See how models solve problems by matching steps to training examples. | Model Tracing |
| []() |  |  |
| []() |  |  |


## Week 3/4
| Title | Summary | Topics |
| --- | --- | --- |
| [Leveraging Reasoning Model Answers to Enhance Non-Reasoning Model Capability](https://arxiv.org/pdf/2504.09639) | This new study explores how leveraging outputs from advanced reasoning models can significantly boost the performance of simpler, non-reasoning models - without hefty computational costs. <br><br> ğŸ”‘  Key Insights from the Paper <br> &nbsp; 1/ Methodology: Using Supervised Fine-Tuning (SFT), they trained non-reasoning models on high-quality answers generated by reasoning-intensive LLMs like DeepSeek-R1. <br> &nbsp; 2/ Results: Models fine-tuned with reasoning-derived answers showed marked improvements on benchmarks like GSM8K (+8.5%) and HumanEval (+10.4%). However, chat-oriented tasks saw slight dips, highlighting a trade-off between task types. <br> &nbsp; 3/ Techniques Tested: From direct answer extraction to integrating summarized reasoning steps, the study compared multiple strategies. The best approach? Combining concise reasoning summaries with final answers - balancing clarity and depth. | Reasoning LLM |
| [NodeRAG: Structuring Graph-based RAG with Heterogeneous Nodes](https://arxiv.org/pdf/2504.11544) | This new paper introduces NodeRAG, a graph-based framework that transforms information into a heterogeneous graph with distinct node types: <br> &nbsp;  âœ… Semantic units (independent events) <br> &nbsp;  ğŸ·ï¸ Entities (names and keywords) <br> &nbsp;  ğŸ”„ Relationships (connections between entities) <br> &nbsp;  ğŸ“Š Attributes (entity characteristics) <br> &nbsp;  ğŸ§© High-level elements (community insights) <br><br> What makes NodeRAG special is how it: <br> &nbsp;  1/ Decomposes information into fine-grained nodes <br> &nbsp;  2/ Preserves structural relationships between concepts <br> &nbsp;  3/ Uses advanced graph algorithms for precision retrieval <br> &nbsp;  4/ Balances original text with LLM-generated insights <br><br> The results? <br> NodeRAG outperforms previous methods like GraphRAG and LightRAG across multiple benchmarks while using FEWER tokens - proving more efficient retrieval quality doesn't require more context. | RAG |
| []() |  |  |
| []() |  |  |



## Week 4/4
| Title | Summary | Topics |
| --- | --- | --- |
| [A Survey of AI Agent Protocols](https://arxiv.org/pdf/2504.16736) | Hereâ€™s the core insight: while LLM agents are rapidly advancing, their communication remains fragmented. Just like the early web struggled without common standards, todayâ€™s agent ecosystems lack unified protocols for agents to talk to tools, to data sources, or even to each other. <br><br> ğŸ¤” Imagine what standardized agent protocols could unlock: <br> &nbsp;  -> Agents forming temporary coalitions to solve complex problems <br> &nbsp;  -> Cross-platform collaboration, like your finance bot collaborating with a logistics agent <br> &nbsp;  -> Scalable networks of interoperable, intelligent agents - forming a kind of Agent Internet <br><br> ğŸ‘€ The paper categorizes current protocols into four types: <br> &nbsp;  1ï¸âƒ£ Context-Oriented, General-Purpose (e.g. Anthropicâ€™s MCP ğŸ§­) <br> &nbsp;  2ï¸âƒ£ Context-Oriented, Domain-Specific (e.g. agents.json) <br> &nbsp;  3ï¸âƒ£ Inter-Agent, General-Purpose (e.g. Googleâ€™s A2A or the open-source ANP) <br> &nbsp;  4ï¸âƒ£ Inter-Agent, Domain-Specific (e.g. PXP for human-agent interaction, LMOS for IoT+agents) <br><br> It also evaluates protocols across 7 critical dimensions like: <br> &nbsp;  âš¡ Efficiency  <br> &nbsp;  ğŸ“ˆ Scalability  <br> &nbsp;  ğŸ” Security  <br> &nbsp;  ğŸ“¡ Reliability  <br> &nbsp;  ğŸ§© Extensibility  <br> &nbsp;  ğŸ”§ Operability  <br> &nbsp;  ğŸŒ Interoperability  <br><br> ğŸŒ± Weâ€™re still in the early days, and standardization could be the soil that enables the next generation of collaborative, distributed intelligence. <br><br> If you're building or deploying agents today, keep protocol design top-of-mind. Interoperability might not just be a nice-to-have, it could be the multiplier that unlocks value across the entire ecosystem. | AI Agents |
| []() |  |  |
