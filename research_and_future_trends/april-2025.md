## Week 1/4
| Title | Summary | Topics |
| --- | --- | --- |
| [Think Before Recommend: Unleashing the Latent Reasoning Power for Sequential Recommendation](https://arxiv.org/pdf/2503.22675) | Most recommendation systems today work in a one-pass manner. User interacts, the model predicts, and that‚Äôs it. <br><br> And this is a problem because user preferences aren‚Äôt static. They evolve, shift, and sometimes contradict past behavior.  <br><br> Yet, traditional models don‚Äôt take a moment to rethink their suggestions before serving them.  <br><br> The usual method? Simple sequential predictions. <br> &nbsp;  -> User clicks on a few items, the model picks up a pattern, and boom - a recommendation appears. <br> &nbsp;  -> But if the user‚Äôs interests are nuanced or sparse, the model struggles to get it right. <br><br> So, what‚Äôs different now? <br> &nbsp;  -> A reasoning-based approach where the model revisits its own predictions before making a decision. <br> &nbsp;  -> Two strategies - one that ensembles multiple reasoning paths, another that progressively refines recommendations step by step. <br><br> And the results? <br> &nbsp;  -> Better personalization, especially for long-tail and sparse data scenarios. <br> &nbsp;  -> Significant accuracy boosts, all with a minimal increase in processing time. <br><br> So, what are our options? <br> &nbsp;  1> Keep using traditional models that only skim the surface of user preferences. <br> &nbsp;  2> Shift towards models that refine their predictions by reasoning over past interactions. <br><br> And then, there‚Äôs one more possibility. <br><br> See, we need recommendation systems that aren‚Äôt just fast, but thoughtful. <br> &nbsp;  -> They need to process feedback dynamically, rather than sticking to rigid patterns. <br> &nbsp;  -> They need to be designed for long-term learning, not just quick suggestions. <br> &nbsp;  -> They need models that don‚Äôt just predict - but think. <br><br> The question is - will we make this shift? <br><br> We‚Äôve already seen how naive recommendation systems fall short in real-world applications. The only way forward is to build models that take a second to think before they suggest.| Recommender System |
| [Effectively Controlling Reasoning Models through Thinking Intervention](https://arxiv.org/pdf/2503.24370v1) | The approach involves guiding the model's reasoning process by injecting specific thinking tokens during the intermediate steps. <br><br> By doing so, the models not only follow instructions better but also manage complex tasks like instruction hierarchy and safety alignment more effectively. <br><br> The idea is simple yet powerful: rather than simply providing input prompts, we can actively intervene during the model's reasoning phase.  <br><br> This method helps ensure that the model's output is more aligned with the intended task.  <br><br> For example, when handling instructions that require a specific format or safety protocols, a small intervention can lead to measurable improvements in performance and safety. <br><br> Key heighlights include:  <br> &nbsp;  -> Enhanced instruction following by incorporating dynamic, context-specific guidance. <br> &nbsp;  -> Improved management of instruction hierarchies, allowing models to prioritize key tasks. <br> &nbsp;  -> A boost in safety alignment, ensuring that AI responses adhere to necessary safety standards. | LLM Reasoning |
| [Harnessing the Reasoning Economy - A Survey of Efficient Reasoning for Large Language Models](https://arxiv.org/pdf/2503.24377) | This new survey dives into the concept of "reasoning economy" for large language models (LLMs), exploring how to optimize their decision-making without wasting computational resources. <br><br> The paper highlights two modes: <br> &nbsp; üîπ System 1 ‚Äì Quick, intuitive responses (think: instant answers). <br> &nbsp; üîπ System 2 ‚Äì Slow, deliberate reasoning (think: solving complex math). <br><br> But here‚Äôs the catch: Over-relying on deep thinking can lead to "overthinking" (redundant steps) or "underthinking" (missing critical analysis).  <br><br> The study unpacks solutions like adaptive computation (tailoring effort to task complexity) and reward model optimizations to cut unnecessary steps while preserving accuracy. <br><br> Results? <br> &nbsp; ‚úÖ Smarter resource use = faster, cheaper AI applications. <br> &nbsp; ‚úÖ Better alignment with human-like reasoning efficiency. | LLM Reasoning |
| [Scaling Language-Free Visual Representation Learning](https://arxiv.org/pdf/2504.01017) | This recent study challenges the belief that language supervision (like CLIP) is essential for training powerful vision models. Researchers trained self-supervised learning (SSL) models on the same web-scale data as CLIP (MetaCLIP‚Äôs 2B images) and found something surprising: <br><br> üîç Key Insights: <br> &nbsp;  üîπ SSL scales better: When models grow to 7B parameters, SSL outperforms CLIP on diverse tasks like Visual Question Answering (VQA), including text-heavy challenges like OCR & charts. <br> &nbsp;  üîπ Data quality > labels: Curating images with text (e.g., signs, charts) boosted OCR performance by +13.6% - no language supervision needed! <br> &nbsp;  üîπ Classic tasks stay strong: SSL maintains competitive accuracy on classification and segmentation, proving versatility. <br><br> It suggests vision models can learn rich, language-aligned features purely from images, opening doors for applications where paired text is scarce or biased. üí°  <br><br> Scaling SSL further, optimizing data composition, and exploring how vision-centric models can complement (or even replace) language-supervised approaches. | Vision Learning |
| []() |  |  |


## Week 2/4
| Title | Summary | Topics |
| --- | --- | --- |
| []() |  |  |
| []() |  |  |



## Week 3/4
| Title | Summary | Topics |
| --- | --- | --- |
| []() |  |  |
| []() |  |  |



## Week 4/4
| Title | Summary | Topics |
| --- | --- | --- |
| []() |  |  |
| []() |  |  |
