## Week 1/4
| Title | Summary | Topics |
| --- | --- | --- |
| [Think Before Recommend: Unleashing the Latent Reasoning Power for Sequential Recommendation](https://arxiv.org/pdf/2503.22675) | Most recommendation systems today work in a one-pass manner. User interacts, the model predicts, and thatâ€™s it. <br><br> And this is a problem because user preferences arenâ€™t static. They evolve, shift, and sometimes contradict past behavior.  <br><br> Yet, traditional models donâ€™t take a moment to rethink their suggestions before serving them.  <br><br> The usual method? Simple sequential predictions. <br> &nbsp;  -> User clicks on a few items, the model picks up a pattern, and boom - a recommendation appears. <br> &nbsp;  -> But if the userâ€™s interests are nuanced or sparse, the model struggles to get it right. <br><br> So, whatâ€™s different now? <br> &nbsp;  -> A reasoning-based approach where the model revisits its own predictions before making a decision. <br> &nbsp;  -> Two strategies - one that ensembles multiple reasoning paths, another that progressively refines recommendations step by step. <br><br> And the results? <br> &nbsp;  -> Better personalization, especially for long-tail and sparse data scenarios. <br> &nbsp;  -> Significant accuracy boosts, all with a minimal increase in processing time. <br><br> So, what are our options? <br> &nbsp;  1> Keep using traditional models that only skim the surface of user preferences. <br> &nbsp;  2> Shift towards models that refine their predictions by reasoning over past interactions. <br><br> And then, thereâ€™s one more possibility. <br><br> See, we need recommendation systems that arenâ€™t just fast, but thoughtful. <br> &nbsp;  -> They need to process feedback dynamically, rather than sticking to rigid patterns. <br> &nbsp;  -> They need to be designed for long-term learning, not just quick suggestions. <br> &nbsp;  -> They need models that donâ€™t just predict - but think. <br><br> The question is - will we make this shift? <br><br> Weâ€™ve already seen how naive recommendation systems fall short in real-world applications. The only way forward is to build models that take a second to think before they suggest.| Recommender System |
| [Effectively Controlling Reasoning Models through Thinking Intervention](https://arxiv.org/pdf/2503.24370v1) | The approach involves guiding the model's reasoning process by injecting specific thinking tokens during the intermediate steps. <br><br> By doing so, the models not only follow instructions better but also manage complex tasks like instruction hierarchy and safety alignment more effectively. <br><br> The idea is simple yet powerful: rather than simply providing input prompts, we can actively intervene during the model's reasoning phase.  <br><br> This method helps ensure that the model's output is more aligned with the intended task.  <br><br> For example, when handling instructions that require a specific format or safety protocols, a small intervention can lead to measurable improvements in performance and safety. <br><br> Key heighlights include:  <br> &nbsp;  -> Enhanced instruction following by incorporating dynamic, context-specific guidance. <br> &nbsp;  -> Improved management of instruction hierarchies, allowing models to prioritize key tasks. <br> &nbsp;  -> A boost in safety alignment, ensuring that AI responses adhere to necessary safety standards. | LLM Reasoning |
| [Harnessing the Reasoning Economy - A Survey of Efficient Reasoning for Large Language Models](https://arxiv.org/pdf/2503.24377) | This new survey dives into the concept of "reasoning economy" for large language models (LLMs), exploring how to optimize their decision-making without wasting computational resources. <br><br> The paper highlights two modes: <br> &nbsp; ğŸ”¹ System 1 â€“ Quick, intuitive responses (think: instant answers). <br> &nbsp; ğŸ”¹ System 2 â€“ Slow, deliberate reasoning (think: solving complex math). <br><br> But hereâ€™s the catch: Over-relying on deep thinking can lead to "overthinking" (redundant steps) or "underthinking" (missing critical analysis).  <br><br> The study unpacks solutions like adaptive computation (tailoring effort to task complexity) and reward model optimizations to cut unnecessary steps while preserving accuracy. <br><br> Results? <br> &nbsp; âœ… Smarter resource use = faster, cheaper AI applications. <br> &nbsp; âœ… Better alignment with human-like reasoning efficiency. | LLM Reasoning |
| [Scaling Language-Free Visual Representation Learning](https://arxiv.org/pdf/2504.01017) | This recent study challenges the belief that language supervision (like CLIP) is essential for training powerful vision models. Researchers trained self-supervised learning (SSL) models on the same web-scale data as CLIP (MetaCLIPâ€™s 2B images) and found something surprising: <br><br> ğŸ” Key Insights: <br> &nbsp;  ğŸ”¹ SSL scales better: When models grow to 7B parameters, SSL outperforms CLIP on diverse tasks like Visual Question Answering (VQA), including text-heavy challenges like OCR & charts. <br> &nbsp;  ğŸ”¹ Data quality > labels: Curating images with text (e.g., signs, charts) boosted OCR performance by +13.6% - no language supervision needed! <br> &nbsp;  ğŸ”¹ Classic tasks stay strong: SSL maintains competitive accuracy on classification and segmentation, proving versatility. <br><br> It suggests vision models can learn rich, language-aligned features purely from images, opening doors for applications where paired text is scarce or biased. ğŸ’¡  <br><br> Scaling SSL further, optimizing data composition, and exploring how vision-centric models can complement (or even replace) language-supervised approaches. | Vision Learning |
| [Why do LLMs attend to the first token? ï¿½](https://arxiv.org/pdf/2504.02732) | this recent study audits attention sinks in transformer models to see why they lean on the first token. <br><br> ğŸ§ Hereâ€™s what they found: <br> &nbsp;  ğŸ‘‰ The first token acts as a buffer that absorbs a significant portion of attention, preventing excessive mixing. <br> &nbsp;  ğŸ‘‰ Larger models and those trained on longer contexts develop stronger attention sinks to keep information stable. <br> &nbsp;  ğŸ‘‰ Experiments on models like Gemma 7B and the LLaMa 3.1 family reveal that this strategy effectively controls perturbations in later tokens. <br><br> This means that even if it appears that the first token is overburdened, it is a deliberate design choice to enhance model stability. <br><br> ğŸ¤” BTW, how does this mechanism work? <br> &nbsp;  ğŸ‘‰ By focusing attention on the first token, the model limits the spread of small changes across the network. <br> &nbsp;  ğŸ‘‰ This controlled mixing keeps the representations robust, reducing the risks of over-smoothing and representational collapse. <br> &nbsp;  ğŸ‘‰ As a result, even with very long sequences, the model's output remains stable and reliable. | LLMs |
| []() |  |  |


## Week 2/4
| Title | Summary | Topics |
| --- | --- | --- |
| []() |  |  |
| []() |  |  |



## Week 3/4
| Title | Summary | Topics |
| --- | --- | --- |
| []() |  |  |
| []() |  |  |



## Week 4/4
| Title | Summary | Topics |
| --- | --- | --- |
| []() |  |  |
| []() |  |  |
