## Week 1/4
| Title | Summary | Topics |
| --- | --- | --- |
| [Rethinking Memory in AI: Taxonomy, Operations, Topics, and Future Directions](https://arxiv.org/pdf/2505.00675) | There are three types of memory in AI systems:<br> &nbsp; 1> Parametric memory (baked into model weights) <br> &nbsp;  2> Contextual structured memory (like graphs or DBs) <br> &nbsp;  3> Contextual unstructured memory (raw chat logs, images, videos, etc.) <br><br> But most current systems lean heavily on just one or two. That’s like asking a human to operate with half a brain. <br><br> Even more interesting are the 6 memory operations that intelligent agents must perform: <br> &nbsp; 1> Consolidation – Convert fleeting context into long-term memory <br> &nbsp;  2> Indexing – Label and organize info for fast access<br> &nbsp;  3> Updating – Modify memory when new info arrives <br> &nbsp;  4> Forgetting – Remove irrelevant or stale knowledge <br> &nbsp;  5> Retrieval – Pull the right memory at the right time <br> &nbsp;  6> Compression – Shrink memory while keeping what matters <br><br> Now ask yourself: how many LLM agents today can actually do all six? <br><br> Answer: Almost none. <br><br> Most LLM agents today just extend context windows and call that “memory.” <br><br> But that’s like saying a bigger backpack is the same as a better brain. <br><br> If we want long-term, useful agents—whether for personal assistants, co-pilots, or autonomous decision-makers—we need memory systems that actively learn what to keep and what to forget. <br><br> So what’s next? <br> &nbsp;  ✅ Memory needs to span multiple modalities: text, image, video <br> &nbsp;  ✅ It needs to evolve over time—learning patterns, updating beliefs <br> &nbsp;  ✅ And maybe most importantly, we’ll need multi-agent shared memory systems, where agents learn from each other’s experiences | Memory |
| [AutoLibra: Agent Metric Induction from Open-Ended Feedback](https://arxiv.org/pdf/2505.02820) | There are two main types of evaluation in AI agent research: <br> &nbsp; 1/ Goal-oriented evaluation (did the agent do the thing?) <br> &nbsp; 2/ Behavior evaluation (how did the agent do the thing?)  <br><br> But current evaluations often miss the nuance. They're either too coarse or rely on a ton of manual work. 😩 That’s like judging a chef only on the final dish, not their technique. <br><br> Even more interesting are the key factors for good evaluation: <br> &nbsp; 1/ Grounded in behavior: Evaluation should be based on what the agent actually does. <br> &nbsp; 2/ Self-validating: We need ways to check if our evaluations are any good!  <br> &nbsp; 3/ Generalizable: Evaluation methods should work across different agents and tasks.  <br><br> Now ask yourself: How many agent evaluation methods tick all three boxes <br> Answer: Not enough. <br><br> Most agent evaluations today focus on simple success rates, or require experts to hand-craft metrics based on heuristics.  <br><br> But that's like grading a complex project with a simple pass/fail. 📝 We miss so much of the rich detail! <br><br> If we want truly capable agents whether for complex problem-solving, or creative tasks, we need evaluation that captures the quality of their actions... <br><br> and "AutoLibra" offers a way.  <br> &nbsp; ✅ We need methods that can automatically turn open-ended human feedback into concrete evaluation metrics - AutoLibra does this by "grounding" feedback to agent behavior and clustering similar behaviors.  <br> &nbsp; ✅ We need to move beyond just "did it work?" to "how well did it work?" - AutoLibra does this by capturing nuances like "agent's autonomy" or "interaction accuracy".  <br> &nbsp; ✅ And maybe most importantly, we'll need "meta-evaluation" to assess the quality of our evaluation itself - Again AutoLibra does this by using metrics like "coverage" and "redundancy". | Agent Evaluation |
| []() |  |  |
| []() |  |  |


## Week 2/4
| Title | Summary | Topics |
| --- | --- | --- |
| [How well do LLMs reason over tabular data, really?](https://arxiv.org/pdf/2505.07453) | Everyone’s wowed by LLMs generating text, but what about their skills with tabular data? 🤔 <br><br> 💡 Most evaluation metrics miss the mark <br> &nbsp;  -> Free-form metrics like SacreBleu and BERT-score fail to tell correct from incorrect answers on tabular queries.  <br> &nbsp;  -> An LLM-as-a-judge approach, however, achieves over 95 percent accuracy in judging correctness . <br><br> 📉 Performance drops with size and complexity <br> &nbsp;  -> As tables grow from 1K to 8K tokens, accuracy on tasks like calculating averages and differences falls below 20 percent for most 7B-parameter models. <br> &nbsp;  -> Only GPT-4o-mini holds near 30 percent on these harder queries . <br><br> 🔍 Real-world data stumps LLMs <br> &nbsp;  -> Missing values can halve sum-calculation accuracy <br> &nbsp;  -> Duplicate rows confuse result consistency <br> &nbsp;  -> Column or row shuffling sometimes breaks reasoning paths <br><br> If we treat LLMs as black boxes on tabular tasks, we risk faulty analytics. To build reliable AI assistants for data work we need to: <br> &nbsp;  -> Adopt robust evaluation pipelines like LLM-as-a-judge <br> &nbsp;  -> Train and fine-tune on noisy, real-world table variations <br> &nbsp; -> Design architectures that explicitly model relational structure | LLM Reasoning |
| [Qwen3 Technical Report](https://arxiv.org/pdf/2505.09388) | Here's what makes Qwen3 stand out 🧵👇 <br>🧠 Thinking vs. Non-Thinking Modes <br> Qwen3 introduces a dual-mode setup:  <br> &nbsp;  -> Thinking mode for complex reasoning (math, code, logic)  <br> &nbsp;  -> Non-thinking mode for fast, fluent responses <br><br> You can dynamically switch between them using a simple flag in the prompt (/think or /no think). This flexibility gives devs fine-grained control over speed vs. reasoning depth. <br><br> ⏳ Thinking Budget <br> Want to control how much the model thinks? Qwen3 lets you cap reasoning effort using a thinking token budget. <br> Result? Smarter latency-performance tradeoffs, especially in real-world deployments. <br><br> 📊 Smaller models, big results <br> Through strong-to-weak distillation, the 4B and 8B models often outperform older 7B–14B models (including Qwen2.5 and Llama-3) on STEM, multilingual, and coding benchmarks.  <br> &nbsp;  -> Efficient, edge-ready, and high-performing. <br><br> 🧪 36 trillion tokens. 119 languages. <br> Massive multilingual pretraining. Fine-grained data annotation. <br> Qwen3 expands from 29 to 119 supported languages and dialects, unlocking broader global use cases 🌍 <br><br> 💻 Best-in-class open-source model performance <br> Qwen3-235B (MoE) outperforms DeepSeek-R1 and even matches or beats GPT-4o on 18 of 23 reasoning tasks - using fewer parameters and lower inference costs. <br>Even interesting, Qwen3-32B beats models twice its size like Llama-4-Scout in 17 of 23 benchmarks 📈 | LLM |
| []() |  |  |
| []() |  |  |


## Week 3/4
| Title | Summary | Topics |
| --- | --- | --- |
| [When Thinking Fails: The Pitfalls of Reasoning for Instruction-Following in LLMs](https://arxiv.org/pdf/2505.11423) | 🧠 Reasoning hurts? Sometimes, yes. <br><br> We’ve all been told: “Make LLMs reason step by step. It helps.” <br><br> Chain-of-Thought (CoT) prompting is now standard practice. <br><br> But what if I told you that this kind of reasoning often reduces accuracy in instruction-following tasks? <br><br> That’s exactly what this paper - “When Thinking Fails” found. And the results are hard to ignore. <br><br> 🧩 What they tested: <br>15 models across 2 benchmarks: <br> &nbsp;  -> IFEval: Simple, rule-checkable constraints (e.g., “mention AI three times”) <br> &nbsp;  -> ComplexBench: Multi-layered constraints (e.g., logical composition, chaining, nesting) <br><br> 📉 What they discovered: <br> &nbsp;  -> CoT prompting degraded instruction-following in nearly all models <br> &nbsp;  -> Accuracy often dropped by 16 to 32 percentage points in models like LLaMA 3 <br> &nbsp;  -> Models started inserting “helpful” content that broke constraints <br> &nbsp;  -> Reasoning sometimes diverted focus away from instructions entirely <br><br> 👀 Why this happens:<br>Using attention analysis, they showed that CoT reduces focus on instruction-relevant tokens. <br>They even introduced a new metric: Constraint Attention. <br> Lower constraint attention? Higher failure rate. Simple as that. <br><br> 🛠️ What helps fix it? <br>They tested 4 mitigation strategies: <br> &nbsp;  1️⃣ Few-shot examples <br> &nbsp;  2️⃣ Self-reflection <br> &nbsp;  3️⃣ Self-selective reasoning (model decides if it should reason) <br> &nbsp;  4️⃣ Classifier-selective reasoning (external model decides) <br><br> ✅ Classifier-selective reasoning performed best - recovering most of the lost performance. <br><br> Reasoning is powerful - but only when used selectively. <br><br> Sometimes, thinking less leads to better outcomes - in real-life as well 🫡  | LLM Reasoning |
| [SMOTExT: SMOTE meets Large Language Models](https://arxiv.org/pdf/2505.13434) | One of the toughest challenges in NLP is creating quality data when you don’t have much to start with. <br><br> Especially in sensitive domains like healthcare. <br><br> Where privacy matters more than performance. <br><br> The new paper SMOTExT explores a smart solution -  <br>📌 Generate synthetic samples by blending two real ones. <br> But in the latent space, not in the words. <br><br> Here’s how it works<br> &nbsp;  1️⃣ Take two text samples <br> &nbsp;  2️⃣ Encode them into BERT-style embeddings <br> &nbsp;  3️⃣ Interpolate between them in vector space <br> &nbsp;  4️⃣ Use xRAG to decode that midpoint into coherent text <br><br> Simple idea. Strong results.💡  <br> &nbsp; -> Unlike backtranslation or random word swaps, this keeps the data on-distribution <br> &nbsp;  -> It’s model-agnostic. You generate once, use anywhere <br> &nbsp;  -> You don’t need labels or task-specific retraining <br> &nbsp;  -> You avoid direct exposure to sensitive data while preserving structure <br><br> 🔍 They tested it on 20 Newsgroups too: <br> &nbsp;  -> Real-only training: 83.6% F1 <br> &nbsp;  -> Real + SMOTExT: 84.0% F1 <br> &nbsp;  -> SMOTExT-only: 80.3% F1 <br><br> Training only on synthetic data got within 3 points of the real baseline. That’s promising. Especially when privacy is a constraint. | Synthetic Data Generation |
| [Learning to Reason via Mixture-of-Thought for Logical Reasoning](https://arxiv.org/pdf/2505.15817) | Want better reasoning from LLMs? Try mixing how they think. <br><br> Most LLMs reason in one mode. Usually natural language. <br><br> They break a problem into steps and try to explain their way to the answer. <br><br> But here’s the catch: That mode often fails. <br><br> In fact, two-thirds of errors in NL reasoning come from just two mistakes: <br> &nbsp;  1/ Missing important branches in logic <br> &nbsp;  2/ Reversing cause and effect <br><br> Here's a simple fix: let models reason in multiple ways - Mixture-of-Thought <br>Instead of just words, LLMs reason using -  <br> &nbsp;  1/ Natural Language <br> &nbsp;  2/ Python Code <br> &nbsp;  3/ Truth Tables (symbolic logic) <br><br> Each mode brings its own strengths -  <br> &nbsp;  3/ Truth tables catch missing logic. <br> &nbsp;  2/ Code adds structure. <br> &nbsp;  1/ NL gives flexibility. <br><br> Together, they cover more ground than any one mode alone. <br><br> How it works: <br> &nbsp;  -> Train LLMs across all 3 reasoning modes using a self-checking loop <br> &nbsp;  -> At inference, generate answers in all 3 formats <br> &nbsp;  -> Use majority vote to pick the final answer <br><br> The result? <br> &nbsp;  ✅ Up to +11.7 percentage points gain over natural language alone <br> &nbsp;  ✅ Smaller open-source models like Gemma-2B match the performance of closed models like GPT-4 + Logic-LM <br> &nbsp;  ✅ Bigger gains on deeper, harder logical reasoning tasks <br><br> Example: <br>“Thor is happy. Hulk gets angry. Hulk breaks a bridge. If a destroyer breaks a bridge, Peter is not a civilian…” <br>Now ask: “If Thor is happy, does Peter wear a uniform?” <br><br> Try solving it. Then picture three reasoning paths: <br> &nbsp;  -> A paragraph explanation <br> &nbsp;  -> A Python class for Hulk <br> &nbsp;  -> A table of logical truth values <br><br> When the model uses all three, it reasons better. | LLM Reasoning |
| []() |  |  |
| []() |  |  |


## Week 4/4
| Title | Summary | Topics |
| --- | --- | --- |
| [The Mirage of Multimodality: Where Truth is Tested and Honesty Unravels](https://arxiv.org/pdf/2505.20214) | More vision. Less truth. <br><br> Multimodal LLMs are supposed to help us see better. <br><br> But they may actually be hallucinating more. <br><br> In complex visual tasks, the best reasoning models often fabricate details. <br><br> Why? Because when they “think slow,” they over-commit to false premises. <br><br> Models designed for step-by-step reasoning like QVQ and Llama-V CoT, struggle more than chat models in deceptive visual contexts. <br><br> They follow a depth-first path, digging deeper into flawed logic. <br><br> Chat models, trained to explore many options, use breadth-first reasoning and are more cautious under uncertainty. <br><br> Performance drops fast as prompts get trickier. <br><br> Even top-tier reasoning models fall for illusions - like monsters behind doors, impossible shadows, and fake reflections. <br><br> Reasoning models show inverse scaling: larger sizes often perform worse under misleading visuals. <br><br> More tokens = more false confidence. <br><br> So, what works better? <br> &nbsp; ✅ Chat models trained on diverse dialogs <br> &nbsp; ✅ Breadth-first planning <br> &nbsp; ✅ Human-in-the-loop evaluation <br> &nbsp; ✅ Specialized judges instead of relying on LLMs to rate LLMs <br><br> Multimodality helps but only when the model knows what not to trust. | Multimodal |
| [Can Agents Fix Agent Issues?](https://arxiv.org/pdf/2505.20749) | 🛠️ Agents can write code. But can they fix themselves? <br><br> We now have AI agents that can browse, build apps, and plan workflows. <br><br> But when these agents break, who fixes the bugs? <br><br> This new paper explores that exact question - Can software engineering (SE) agents debug other agents? <br><br> Turns out, not really. Not yet. 🫠  <br><br> The authors built AGENTISSUE-BENCH - the first benchmark of real-world bugs from tools like CrewAI, MetaGPT, AutoGen, and more. <br><br> They found 201 actual GitHub issues from top open-source agent frameworks. <br><br> Then they manually recreated 50 of them in Docker with test failures, bugs, and fix patches. <br><br> This alone took 500 hours. <br><br> Then they tested top SE agents: Agentless, AutoCodeRover, SWE-agent 🧠  <br><br> With both GPT-4o and Claude 3.5 as backends. <br><br> How did they perform? <br> &nbsp;  ❌ Best success rate: 12.67% <br> &nbsp;  ❌ Most failed to even locate the buggy file <br> &nbsp;  ❌ Fixes often passed tests but were semantically wrong <br><br> In short, agent-specific bugs are hard. <br><br> 📊 Here’s what makes agent systems tricky: <br> &nbsp;  -> LLM provider incompatibility <br> &nbsp;  -> Tool config and runtime errors <br> &nbsp;  -> Memory corruption in long workflows <br> &nbsp;  -> Workflow loops and hangs <br> &nbsp;  -> Bad prompt formatting <br> &nbsp;  -> Model output parsing failures <br><br> These are not your typical Python bugs. <br><br> They span APIs, async flows, memory graphs, and fragile toolchains. <br><br> Essentially, We need SE agents that understand agent architectures. <br><br> That can diagnose tool/LLM/memory bugs. <br><br> That can read logs, prompts, test cases, and config files and connect the dots. | AI Agents |
| []() |  |  |
| []() |  |  |
