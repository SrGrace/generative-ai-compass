## Week 1/4
| Title | Summary | Topics |
| --- | --- | --- |
| [Rethinking Memory in AI: Taxonomy, Operations, Topics, and Future Directions](https://arxiv.org/pdf/2505.00675) | There are three types of memory in AI systems:<br> &nbsp; 1> Parametric memory (baked into model weights) <br> &nbsp;  2> Contextual structured memory (like graphs or DBs) <br> &nbsp;  3> Contextual unstructured memory (raw chat logs, images, videos, etc.) <br><br> But most current systems lean heavily on just one or two. That‚Äôs like asking a human to operate with half a brain. <br><br> Even more interesting are the 6 memory operations that intelligent agents must perform: <br> &nbsp; 1> Consolidation ‚Äì Convert fleeting context into long-term memory <br> &nbsp;  2> Indexing ‚Äì Label and organize info for fast access<br> &nbsp;  3> Updating ‚Äì Modify memory when new info arrives <br> &nbsp;  4> Forgetting ‚Äì Remove irrelevant or stale knowledge <br> &nbsp;  5> Retrieval ‚Äì Pull the right memory at the right time <br> &nbsp;  6> Compression ‚Äì Shrink memory while keeping what matters <br><br> Now ask yourself: how many LLM agents today can actually do all six? <br><br> Answer: Almost none. <br><br> Most LLM agents today just extend context windows and call that ‚Äúmemory.‚Äù <br><br> But that‚Äôs like saying a bigger backpack is the same as a better brain. <br><br> If we want long-term, useful agents‚Äîwhether for personal assistants, co-pilots, or autonomous decision-makers‚Äîwe need memory systems that actively learn what to keep and what to forget. <br><br> So what‚Äôs next? <br> &nbsp;  ‚úÖ Memory needs to span multiple modalities: text, image, video <br> &nbsp;  ‚úÖ It needs to evolve over time‚Äîlearning patterns, updating beliefs <br> &nbsp;  ‚úÖ And maybe most importantly, we‚Äôll need multi-agent shared memory systems, where agents learn from each other‚Äôs experiences | Memory |
| [AutoLibra: Agent Metric Induction from Open-Ended Feedback](https://arxiv.org/pdf/2505.02820) | There are two main types of evaluation in AI agent research: <br> &nbsp; 1/ Goal-oriented evaluation (did the agent do the thing?) <br> &nbsp; 2/ Behavior evaluation (how did the agent do the thing?)  <br><br> But current evaluations often miss the nuance. They're either too coarse or rely on a ton of manual work. üò© That‚Äôs like judging a chef only on the final dish, not their technique. <br><br> Even more interesting are the key factors for good evaluation: <br> &nbsp; 1/ Grounded in behavior: Evaluation should be based on what the agent actually does. <br> &nbsp; 2/ Self-validating: We need ways to check if our evaluations are any good!  <br> &nbsp; 3/ Generalizable: Evaluation methods should work across different agents and tasks.  <br><br> Now ask yourself: How many agent evaluation methods tick all three boxes <br> Answer: Not enough. <br><br> Most agent evaluations today focus on simple success rates, or require experts to hand-craft metrics based on heuristics.  <br><br> But that's like grading a complex project with a simple pass/fail. üìù We miss so much of the rich detail! <br><br> If we want truly capable agents whether for complex problem-solving, or creative tasks, we need evaluation that captures the quality of their actions... <br><br> and "AutoLibra" offers a way.  <br> &nbsp; ‚úÖ We need methods that can automatically turn open-ended human feedback into concrete evaluation metrics - AutoLibra does this by "grounding" feedback to agent behavior and clustering similar behaviors.  <br> &nbsp; ‚úÖ We need to move beyond just "did it work?" to "how well did it work?" - AutoLibra does this by capturing nuances like "agent's autonomy" or "interaction accuracy".  <br> &nbsp; ‚úÖ And maybe most importantly, we'll need "meta-evaluation" to assess the quality of our evaluation itself - Again AutoLibra does this by using metrics like "coverage" and "redundancy". | Agent Evaluation |
| []() |  |  |
| []() |  |  |


## Week 2/4
| Title | Summary | Topics |
| --- | --- | --- |
| [How well do LLMs reason over tabular data, really?](https://arxiv.org/pdf/2505.07453) | Everyone‚Äôs wowed by LLMs generating text, but what about their skills with tabular data? ü§î <br><br> üí° Most evaluation metrics miss the mark <br> &nbsp;  -> Free-form metrics like SacreBleu and BERT-score fail to tell correct from incorrect answers on tabular queries.  <br> &nbsp;  -> An LLM-as-a-judge approach, however, achieves over 95 percent accuracy in judging correctness . <br><br> üìâ Performance drops with size and complexity <br> &nbsp;  -> As tables grow from 1K to 8K tokens, accuracy on tasks like calculating averages and differences falls below 20 percent for most 7B-parameter models. <br> &nbsp;  -> Only GPT-4o-mini holds near 30 percent on these harder queries . <br><br> üîç Real-world data stumps LLMs <br> &nbsp;  -> Missing values can halve sum-calculation accuracy <br> &nbsp;  -> Duplicate rows confuse result consistency <br> &nbsp;  -> Column or row shuffling sometimes breaks reasoning paths <br><br> If we treat LLMs as black boxes on tabular tasks, we risk faulty analytics. To build reliable AI assistants for data work we need to: <br> &nbsp;  -> Adopt robust evaluation pipelines like LLM-as-a-judge <br> &nbsp;  -> Train and fine-tune on noisy, real-world table variations <br> &nbsp; -> Design architectures that explicitly model relational structure | LLM Reasoning |
| [Qwen3 Technical Report](https://arxiv.org/pdf/2505.09388) | Here's what makes Qwen3 stand out üßµüëá <br>üß† Thinking vs. Non-Thinking Modes <br> Qwen3 introduces a dual-mode setup:  <br> &nbsp;  -> Thinking mode for complex reasoning (math, code, logic)  <br> &nbsp;  -> Non-thinking mode for fast, fluent responses <br><br> You can dynamically switch between them using a simple flag in the prompt (/think or /no think). This flexibility gives devs fine-grained control over speed vs. reasoning depth. <br><br> ‚è≥ Thinking Budget <br> Want to control how much the model thinks? Qwen3 lets you cap reasoning effort using a thinking token budget. <br> Result? Smarter latency-performance tradeoffs, especially in real-world deployments. <br><br> üìä Smaller models, big results <br> Through strong-to-weak distillation, the 4B and 8B models often outperform older 7B‚Äì14B models (including Qwen2.5 and Llama-3) on STEM, multilingual, and coding benchmarks.  <br> &nbsp;  -> Efficient, edge-ready, and high-performing. <br><br> üß™ 36 trillion tokens. 119 languages. <br> Massive multilingual pretraining. Fine-grained data annotation. <br> Qwen3 expands from 29 to 119 supported languages and dialects, unlocking broader global use cases üåç <br><br> üíª Best-in-class open-source model performance <br> Qwen3-235B (MoE) outperforms DeepSeek-R1 and even matches or beats GPT-4o on 18 of 23 reasoning tasks - using fewer parameters and lower inference costs. <br>Even interesting, Qwen3-32B beats models twice its size like Llama-4-Scout in 17 of 23 benchmarks üìà | LLM |
| []() |  |  |
| []() |  |  |


## Week 3/4
| Title | Summary | Topics |
| --- | --- | --- |
| [When Thinking Fails: The Pitfalls of Reasoning for Instruction-Following in LLMs](https://arxiv.org/pdf/2505.11423) | üß† Reasoning hurts? Sometimes, yes. <br><br> We‚Äôve all been told: ‚ÄúMake LLMs reason step by step. It helps.‚Äù <br><br> Chain-of-Thought (CoT) prompting is now standard practice. <br><br> But what if I told you that this kind of reasoning often reduces accuracy in instruction-following tasks? <br><br> That‚Äôs exactly what this paper - ‚ÄúWhen Thinking Fails‚Äù found. And the results are hard to ignore. <br><br> üß© What they tested: <br>15 models across 2 benchmarks: <br> &nbsp;  -> IFEval: Simple, rule-checkable constraints (e.g., ‚Äúmention AI three times‚Äù) <br> &nbsp;  -> ComplexBench: Multi-layered constraints (e.g., logical composition, chaining, nesting) <br><br> üìâ What they discovered: <br> &nbsp;  -> CoT prompting degraded instruction-following in nearly all models <br> &nbsp;  -> Accuracy often dropped by 16 to 32 percentage points in models like LLaMA 3 <br> &nbsp;  -> Models started inserting ‚Äúhelpful‚Äù content that broke constraints <br> &nbsp;  -> Reasoning sometimes diverted focus away from instructions entirely <br><br> üëÄ Why this happens:<br>Using attention analysis, they showed that CoT reduces focus on instruction-relevant tokens. <br>They even introduced a new metric: Constraint Attention. <br> Lower constraint attention? Higher failure rate. Simple as that. <br><br> üõ†Ô∏è What helps fix it? <br>They tested 4 mitigation strategies: <br> &nbsp;  1Ô∏è‚É£ Few-shot examples <br> &nbsp;  2Ô∏è‚É£ Self-reflection <br> &nbsp;  3Ô∏è‚É£ Self-selective reasoning (model decides if it should reason) <br> &nbsp;  4Ô∏è‚É£ Classifier-selective reasoning (external model decides) <br><br> ‚úÖ Classifier-selective reasoning performed best - recovering most of the lost performance. <br><br> Reasoning is powerful - but only when used selectively. <br><br> Sometimes, thinking less leads to better outcomes - in real-life as well ü´°  | LLM Reasoning |
| [SMOTExT: SMOTE meets Large Language Models](https://arxiv.org/pdf/2505.13434) | One of the toughest challenges in NLP is creating quality data when you don‚Äôt have much to start with. <br><br> Especially in sensitive domains like healthcare. <br><br> Where privacy matters more than performance. <br><br> The new paper SMOTExT explores a smart solution -  <br>üìå Generate synthetic samples by blending two real ones. <br> But in the latent space, not in the words. <br><br> Here‚Äôs how it works<br> &nbsp;  1Ô∏è‚É£ Take two text samples <br> &nbsp;  2Ô∏è‚É£ Encode them into BERT-style embeddings <br> &nbsp;  3Ô∏è‚É£ Interpolate between them in vector space <br> &nbsp;  4Ô∏è‚É£ Use xRAG to decode that midpoint into coherent text <br><br> Simple idea. Strong results.üí°  <br> &nbsp; -> Unlike backtranslation or random word swaps, this keeps the data on-distribution <br> &nbsp;  -> It‚Äôs model-agnostic. You generate once, use anywhere <br> &nbsp;  -> You don‚Äôt need labels or task-specific retraining <br> &nbsp;  -> You avoid direct exposure to sensitive data while preserving structure <br><br> üîç They tested it on 20 Newsgroups too: <br> &nbsp;  -> Real-only training: 83.6% F1 <br> &nbsp;  -> Real + SMOTExT: 84.0% F1 <br> &nbsp;  -> SMOTExT-only: 80.3% F1 <br><br> Training only on synthetic data got within 3 points of the real baseline. That‚Äôs promising. Especially when privacy is a constraint. | Synthetic Data Generation |
| [Learning to Reason via Mixture-of-Thought for Logical Reasoning](https://arxiv.org/pdf/2505.15817) | Want better reasoning from LLMs? Try mixing how they think. <br><br> Most LLMs reason in one mode. Usually natural language. <br><br> They break a problem into steps and try to explain their way to the answer. <br><br> But here‚Äôs the catch: That mode often fails. <br><br> In fact, two-thirds of errors in NL reasoning come from just two mistakes: <br> &nbsp;  1/ Missing important branches in logic <br> &nbsp;  2/ Reversing cause and effect <br><br> Here's a simple fix: let models reason in multiple ways - Mixture-of-Thought <br>Instead of just words, LLMs reason using -  <br> &nbsp;  1/ Natural Language <br> &nbsp;  2/ Python Code <br> &nbsp;  3/ Truth Tables (symbolic logic) <br><br> Each mode brings its own strengths -  <br> &nbsp;  3/ Truth tables catch missing logic. <br> &nbsp;  2/ Code adds structure. <br> &nbsp;  1/ NL gives flexibility. <br><br> Together, they cover more ground than any one mode alone. <br><br> How it works: <br> &nbsp;  -> Train LLMs across all 3 reasoning modes using a self-checking loop <br> &nbsp;  -> At inference, generate answers in all 3 formats <br> &nbsp;  -> Use majority vote to pick the final answer <br><br> The result? <br> &nbsp;  ‚úÖ Up to +11.7 percentage points gain over natural language alone <br> &nbsp;  ‚úÖ Smaller open-source models like Gemma-2B match the performance of closed models like GPT-4 + Logic-LM <br> &nbsp;  ‚úÖ Bigger gains on deeper, harder logical reasoning tasks <br><br> Example: <br>‚ÄúThor is happy. Hulk gets angry. Hulk breaks a bridge. If a destroyer breaks a bridge, Peter is not a civilian‚Ä¶‚Äù <br>Now ask: ‚ÄúIf Thor is happy, does Peter wear a uniform?‚Äù <br><br> Try solving it. Then picture three reasoning paths: <br> &nbsp;  -> A paragraph explanation <br> &nbsp;  -> A Python class for Hulk <br> &nbsp;  -> A table of logical truth values <br><br> When the model uses all three, it reasons better. | LLM Reasoning |
| []() |  |  |
| []() |  |  |


## Week 4/4
| Title | Summary | Topics |
| --- | --- | --- |
| [The Mirage of Multimodality: Where Truth is Tested and Honesty Unravels](https://arxiv.org/pdf/2505.20214) | More vision. Less truth. <br><br> Multimodal LLMs are supposed to help us see better. <br><br> But they may actually be hallucinating more. <br><br> In complex visual tasks, the best reasoning models often fabricate details. <br><br> Why? Because when they ‚Äúthink slow,‚Äù they over-commit to false premises. <br><br> Models designed for step-by-step reasoning like QVQ and Llama-V CoT, struggle more than chat models in deceptive visual contexts. <br><br> They follow a depth-first path, digging deeper into flawed logic. <br><br> Chat models, trained to explore many options, use breadth-first reasoning and are more cautious under uncertainty. <br><br> Performance drops fast as prompts get trickier. <br><br> Even top-tier reasoning models fall for illusions - like monsters behind doors, impossible shadows, and fake reflections. <br><br> Reasoning models show inverse scaling: larger sizes often perform worse under misleading visuals. <br><br> More tokens = more false confidence. <br><br> So, what works better? <br> &nbsp; ‚úÖ Chat models trained on diverse dialogs <br> &nbsp; ‚úÖ Breadth-first planning <br> &nbsp; ‚úÖ Human-in-the-loop evaluation <br> &nbsp; ‚úÖ Specialized judges instead of relying on LLMs to rate LLMs <br><br> Multimodality helps but only when the model knows what not to trust. | Multimodal |
| []() |  |  |
| []() |  |  |
