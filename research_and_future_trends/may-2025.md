## Week 1/4
| Title | Summary | Topics |
| --- | --- | --- |
| [Rethinking Memory in AI: Taxonomy, Operations, Topics, and Future Directions](https://arxiv.org/pdf/2505.00675) | There are three types of memory in AI systems:<br> &nbsp; 1> Parametric memory (baked into model weights) <br> &nbsp;  2> Contextual structured memory (like graphs or DBs) <br> &nbsp;  3> Contextual unstructured memory (raw chat logs, images, videos, etc.) <br><br> But most current systems lean heavily on just one or two. That‚Äôs like asking a human to operate with half a brain. <br><br> Even more interesting are the 6 memory operations that intelligent agents must perform: <br> &nbsp; 1> Consolidation ‚Äì Convert fleeting context into long-term memory <br> &nbsp;  2> Indexing ‚Äì Label and organize info for fast access<br> &nbsp;  3> Updating ‚Äì Modify memory when new info arrives <br> &nbsp;  4> Forgetting ‚Äì Remove irrelevant or stale knowledge <br> &nbsp;  5> Retrieval ‚Äì Pull the right memory at the right time <br> &nbsp;  6> Compression ‚Äì Shrink memory while keeping what matters <br><br> Now ask yourself: how many LLM agents today can actually do all six? <br><br> Answer: Almost none. <br><br> Most LLM agents today just extend context windows and call that ‚Äúmemory.‚Äù <br><br> But that‚Äôs like saying a bigger backpack is the same as a better brain. <br><br> If we want long-term, useful agents‚Äîwhether for personal assistants, co-pilots, or autonomous decision-makers‚Äîwe need memory systems that actively learn what to keep and what to forget. <br><br> So what‚Äôs next? <br> &nbsp;  ‚úÖ Memory needs to span multiple modalities: text, image, video <br> &nbsp;  ‚úÖ It needs to evolve over time‚Äîlearning patterns, updating beliefs <br> &nbsp;  ‚úÖ And maybe most importantly, we‚Äôll need multi-agent shared memory systems, where agents learn from each other‚Äôs experiences | Memory |
| [AutoLibra: Agent Metric Induction from Open-Ended Feedback](https://arxiv.org/pdf/2505.02820) | There are two main types of evaluation in AI agent research: <br> &nbsp; 1/ Goal-oriented evaluation (did the agent do the thing?) <br> &nbsp; 2/ Behavior evaluation (how did the agent do the thing?)  <br><br> But current evaluations often miss the nuance. They're either too coarse or rely on a ton of manual work. üò© That‚Äôs like judging a chef only on the final dish, not their technique. <br><br> Even more interesting are the key factors for good evaluation: <br> &nbsp; 1/ Grounded in behavior: Evaluation should be based on what the agent actually does. <br> &nbsp; 2/ Self-validating: We need ways to check if our evaluations are any good!  <br> &nbsp; 3/ Generalizable: Evaluation methods should work across different agents and tasks.  <br><br> Now ask yourself: How many agent evaluation methods tick all three boxes <br> Answer: Not enough. <br><br> Most agent evaluations today focus on simple success rates, or require experts to hand-craft metrics based on heuristics.  <br><br> But that's like grading a complex project with a simple pass/fail. üìù We miss so much of the rich detail! <br><br> If we want truly capable agents whether for complex problem-solving, or creative tasks, we need evaluation that captures the quality of their actions... <br><br> and "AutoLibra" offers a way.  <br> &nbsp; ‚úÖ We need methods that can automatically turn open-ended human feedback into concrete evaluation metrics - AutoLibra does this by "grounding" feedback to agent behavior and clustering similar behaviors.  <br> &nbsp; ‚úÖ We need to move beyond just "did it work?" to "how well did it work?" - AutoLibra does this by capturing nuances like "agent's autonomy" or "interaction accuracy".  <br> &nbsp; ‚úÖ And maybe most importantly, we'll need "meta-evaluation" to assess the quality of our evaluation itself - Again AutoLibra does this by using metrics like "coverage" and "redundancy". | Agent Evaluation |
| []() |  |  |
| []() |  |  |


## Week 2/4
| Title | Summary | Topics |
| --- | --- | --- |
| [How well do LLMs reason over tabular data, really?](https://arxiv.org/pdf/2505.07453) | Everyone‚Äôs wowed by LLMs generating text, but what about their skills with tabular data? ü§î <br><br> üí° Most evaluation metrics miss the mark <br> &nbsp;  -> Free-form metrics like SacreBleu and BERT-score fail to tell correct from incorrect answers on tabular queries.  <br> &nbsp;  -> An LLM-as-a-judge approach, however, achieves over 95 percent accuracy in judging correctness . <br><br> üìâ Performance drops with size and complexity <br> &nbsp;  -> As tables grow from 1K to 8K tokens, accuracy on tasks like calculating averages and differences falls below 20 percent for most 7B-parameter models. <br> &nbsp;  -> Only GPT-4o-mini holds near 30 percent on these harder queries . <br><br> üîç Real-world data stumps LLMs <br> &nbsp;  -> Missing values can halve sum-calculation accuracy <br> &nbsp;  -> Duplicate rows confuse result consistency <br> &nbsp;  -> Column or row shuffling sometimes breaks reasoning paths <br><br> If we treat LLMs as black boxes on tabular tasks, we risk faulty analytics. To build reliable AI assistants for data work we need to: <br> &nbsp;  -> Adopt robust evaluation pipelines like LLM-as-a-judge <br> &nbsp;  -> Train and fine-tune on noisy, real-world table variations <br> &nbsp; -> Design architectures that explicitly model relational structure | LLM Reasoning |
| []() |  |  |


## Week 3/4
| Title | Summary | Topics |
| --- | --- | --- |
| []() |  |  |
| []() |  |  |


## Week 4/4
| Title | Summary | Topics |
| --- | --- | --- |
| []() |  |  |
| []() |  |  |
