## Week 1/4
| Title | Summary | Topics |
| --- | --- | --- |
| [Rethinking Memory in AI: Taxonomy, Operations, Topics, and Future Directions](https://arxiv.org/pdf/2505.00675) | There are three types of memory in AI systems:<br> &nbsp; 1> Parametric memory (baked into model weights) <br> &nbsp;  2> Contextual structured memory (like graphs or DBs) <br> &nbsp;  3> Contextual unstructured memory (raw chat logs, images, videos, etc.) <br><br> But most current systems lean heavily on just one or two. That’s like asking a human to operate with half a brain. <br><br> Even more interesting are the 6 memory operations that intelligent agents must perform: <br> &nbsp; 1> Consolidation – Convert fleeting context into long-term memory <br> &nbsp;  2> Indexing – Label and organize info for fast access<br> &nbsp;  3> Updating – Modify memory when new info arrives <br> &nbsp;  4> Forgetting – Remove irrelevant or stale knowledge <br> &nbsp;  5> Retrieval – Pull the right memory at the right time <br> &nbsp;  6> Compression – Shrink memory while keeping what matters <br><br> Now ask yourself: how many LLM agents today can actually do all six? <br><br> Answer: Almost none. <br><br> Most LLM agents today just extend context windows and call that “memory.” <br><br> But that’s like saying a bigger backpack is the same as a better brain. <br><br> If we want long-term, useful agents—whether for personal assistants, co-pilots, or autonomous decision-makers—we need memory systems that actively learn what to keep and what to forget. <br><br> So what’s next? <br> &nbsp;  ✅ Memory needs to span multiple modalities: text, image, video <br> &nbsp;  ✅ It needs to evolve over time—learning patterns, updating beliefs <br> &nbsp;  ✅ And maybe most importantly, we’ll need multi-agent shared memory systems, where agents learn from each other’s experiences | Memory |
| [AutoLibra: Agent Metric Induction from Open-Ended Feedback](https://arxiv.org/pdf/2505.02820) | There are two main types of evaluation in AI agent research: <br> &nbsp; 1/ Goal-oriented evaluation (did the agent do the thing?) <br> &nbsp; 2/ Behavior evaluation (how did the agent do the thing?)  <br><br> But current evaluations often miss the nuance. They're either too coarse or rely on a ton of manual work. 😩 That’s like judging a chef only on the final dish, not their technique. <br><br> Even more interesting are the key factors for good evaluation: <br> &nbsp; 1/ Grounded in behavior: Evaluation should be based on what the agent actually does. <br> &nbsp; 2/ Self-validating: We need ways to check if our evaluations are any good!  <br> &nbsp; 3/ Generalizable: Evaluation methods should work across different agents and tasks.  <br><br> Now ask yourself: How many agent evaluation methods tick all three boxes <br> Answer: Not enough. <br><br> Most agent evaluations today focus on simple success rates, or require experts to hand-craft metrics based on heuristics.  <br><br> But that's like grading a complex project with a simple pass/fail. 📝 We miss so much of the rich detail! <br><br> If we want truly capable agents whether for complex problem-solving, or creative tasks, we need evaluation that captures the quality of their actions... <br><br> and "AutoLibra" offers a way.  <br> &nbsp; ✅ We need methods that can automatically turn open-ended human feedback into concrete evaluation metrics - AutoLibra does this by "grounding" feedback to agent behavior and clustering similar behaviors.  <br> &nbsp; ✅ We need to move beyond just "did it work?" to "how well did it work?" - AutoLibra does this by capturing nuances like "agent's autonomy" or "interaction accuracy".  <br> &nbsp; ✅ And maybe most importantly, we'll need "meta-evaluation" to assess the quality of our evaluation itself - Again AutoLibra does this by using metrics like "coverage" and "redundancy". | Agent Evaluation |
| []() |  |  |
| []() |  |  |


## Week 2/4
| Title | Summary | Topics |
| --- | --- | --- |
| [How well do LLMs reason over tabular data, really?](https://arxiv.org/pdf/2505.07453) | Everyone’s wowed by LLMs generating text, but what about their skills with tabular data? 🤔 <br><br> 💡 Most evaluation metrics miss the mark <br> &nbsp;  -> Free-form metrics like SacreBleu and BERT-score fail to tell correct from incorrect answers on tabular queries.  <br> &nbsp;  -> An LLM-as-a-judge approach, however, achieves over 95 percent accuracy in judging correctness . <br><br> 📉 Performance drops with size and complexity <br> &nbsp;  -> As tables grow from 1K to 8K tokens, accuracy on tasks like calculating averages and differences falls below 20 percent for most 7B-parameter models. <br> &nbsp;  -> Only GPT-4o-mini holds near 30 percent on these harder queries . <br><br> 🔍 Real-world data stumps LLMs <br> &nbsp;  -> Missing values can halve sum-calculation accuracy <br> &nbsp;  -> Duplicate rows confuse result consistency <br> &nbsp;  -> Column or row shuffling sometimes breaks reasoning paths <br><br> If we treat LLMs as black boxes on tabular tasks, we risk faulty analytics. To build reliable AI assistants for data work we need to: <br> &nbsp;  -> Adopt robust evaluation pipelines like LLM-as-a-judge <br> &nbsp;  -> Train and fine-tune on noisy, real-world table variations <br> &nbsp; -> Design architectures that explicitly model relational structure | LLM Reasoning |
| [Qwen3 Technical Report](https://arxiv.org/pdf/2505.09388) | Here's what makes Qwen3 stand out 🧵👇 <br>🧠 Thinking vs. Non-Thinking Modes <br> Qwen3 introduces a dual-mode setup:  <br> &nbsp;  -> Thinking mode for complex reasoning (math, code, logic)  <br> &nbsp;  -> Non-thinking mode for fast, fluent responses <br><br> You can dynamically switch between them using a simple flag in the prompt (/think or /no think). This flexibility gives devs fine-grained control over speed vs. reasoning depth. <br><br> ⏳ Thinking Budget <br> Want to control how much the model thinks? Qwen3 lets you cap reasoning effort using a thinking token budget. <br> Result? Smarter latency-performance tradeoffs, especially in real-world deployments. <br><br> 📊 Smaller models, big results <br> Through strong-to-weak distillation, the 4B and 8B models often outperform older 7B–14B models (including Qwen2.5 and Llama-3) on STEM, multilingual, and coding benchmarks.  <br> &nbsp;  -> Efficient, edge-ready, and high-performing. <br><br> 🧪 36 trillion tokens. 119 languages. <br> Massive multilingual pretraining. Fine-grained data annotation. <br> Qwen3 expands from 29 to 119 supported languages and dialects, unlocking broader global use cases 🌍 <br><br> 💻 Best-in-class open-source model performance <br> Qwen3-235B (MoE) outperforms DeepSeek-R1 and even matches or beats GPT-4o on 18 of 23 reasoning tasks - using fewer parameters and lower inference costs. <br>Even interesting, Qwen3-32B beats models twice its size like Llama-4-Scout in 17 of 23 benchmarks 📈 | LLM |
| []() |  |  |
| []() |  |  |


## Week 3/4
| Title | Summary | Topics |
| --- | --- | --- |
| [When Thinking Fails: The Pitfalls of Reasoning for Instruction-Following in LLMs](https://arxiv.org/pdf/2505.11423) | 🧠 Reasoning hurts? Sometimes, yes. <br><br> We’ve all been told: “Make LLMs reason step by step. It helps.” <br><br> Chain-of-Thought (CoT) prompting is now standard practice. <br><br> But what if I told you that this kind of reasoning often reduces accuracy in instruction-following tasks? <br><br> That’s exactly what this paper - “When Thinking Fails” found. And the results are hard to ignore. <br><br> 🧩 What they tested: <br>15 models across 2 benchmarks: <br> &nbsp;  -> IFEval: Simple, rule-checkable constraints (e.g., “mention AI three times”) <br> &nbsp;  -> ComplexBench: Multi-layered constraints (e.g., logical composition, chaining, nesting) <br><br> 📉 What they discovered: <br> &nbsp;  -> CoT prompting degraded instruction-following in nearly all models <br> &nbsp;  -> Accuracy often dropped by 16 to 32 percentage points in models like LLaMA 3 <br> &nbsp;  -> Models started inserting “helpful” content that broke constraints <br> &nbsp;  -> Reasoning sometimes diverted focus away from instructions entirely <br><br> 👀 Why this happens:<br>Using attention analysis, they showed that CoT reduces focus on instruction-relevant tokens. <br>They even introduced a new metric: Constraint Attention. <br> Lower constraint attention? Higher failure rate. Simple as that. <br><br> 🛠️ What helps fix it? <br>They tested 4 mitigation strategies: <br> &nbsp;  1️⃣ Few-shot examples <br> &nbsp;  2️⃣ Self-reflection <br> &nbsp;  3️⃣ Self-selective reasoning (model decides if it should reason) <br> &nbsp;  4️⃣ Classifier-selective reasoning (external model decides) <br><br> ✅ Classifier-selective reasoning performed best - recovering most of the lost performance. <br><br> Reasoning is powerful - but only when used selectively. <br><br> Sometimes, thinking less leads to better outcomes - in real-life as well 🫡  | LLM Reasoning |
| [SMOTExT: SMOTE meets Large Language Models](https://arxiv.org/pdf/2505.13434) | One of the toughest challenges in NLP is creating quality data when you don’t have much to start with. <br><br> Especially in sensitive domains like healthcare. <br><br> Where privacy matters more than performance. <br><br> The new paper SMOTExT explores a smart solution -  <br>📌 Generate synthetic samples by blending two real ones. <br> But in the latent space, not in the words. <br><br> Here’s how it works<br> &nbsp;  1️⃣ Take two text samples <br> &nbsp;  2️⃣ Encode them into BERT-style embeddings <br> &nbsp;  3️⃣ Interpolate between them in vector space <br> &nbsp;  4️⃣ Use xRAG to decode that midpoint into coherent text <br><br> Simple idea. Strong results.💡  <br> &nbsp; -> Unlike backtranslation or random word swaps, this keeps the data on-distribution <br> &nbsp;  -> It’s model-agnostic. You generate once, use anywhere <br> &nbsp;  -> You don’t need labels or task-specific retraining <br> &nbsp;  -> You avoid direct exposure to sensitive data while preserving structure <br><br> 🔍 They tested it on 20 Newsgroups too: <br> &nbsp;  -> Real-only training: 83.6% F1 <br> &nbsp;  -> Real + SMOTExT: 84.0% F1 <br> &nbsp;  -> SMOTExT-only: 80.3% F1 <br><br> Training only on synthetic data got within 3 points of the real baseline. That’s promising. Especially when privacy is a constraint. | Synthetic Data Generation |
| []() |  |  |
| []() |  |  |


## Week 4/4
| Title | Summary | Topics |
| --- | --- | --- |
| []() |  |  |
| []() |  |  |
