## Week 1/4
| Title | Summary | Topics |
| --- | --- | --- |
| [Rethinking Memory in AI: Taxonomy, Operations, Topics, and Future Directions](https://arxiv.org/pdf/2505.00675) | There are three types of memory in AI systems:<br> &nbsp; 1> Parametric memory (baked into model weights) <br> &nbsp;  2> Contextual structured memory (like graphs or DBs) <br> &nbsp;  3> Contextual unstructured memory (raw chat logs, images, videos, etc.) <br><br> But most current systems lean heavily on just one or two. Thatâ€™s like asking a human to operate with half a brain. <br><br> Even more interesting are the 6 memory operations that intelligent agents must perform: <br> &nbsp; 1> Consolidation â€“ Convert fleeting context into long-term memory <br> &nbsp;  2> Indexing â€“ Label and organize info for fast access<br> &nbsp;  3> Updating â€“ Modify memory when new info arrives <br> &nbsp;  4> Forgetting â€“ Remove irrelevant or stale knowledge <br> &nbsp;  5> Retrieval â€“ Pull the right memory at the right time <br> &nbsp;  6> Compression â€“ Shrink memory while keeping what matters <br><br> Now ask yourself: how many LLM agents today can actually do all six? <br><br> Answer: Almost none. <br><br> Most LLM agents today just extend context windows and call that â€œmemory.â€ <br><br> But thatâ€™s like saying a bigger backpack is the same as a better brain. <br><br> If we want long-term, useful agentsâ€”whether for personal assistants, co-pilots, or autonomous decision-makersâ€”we need memory systems that actively learn what to keep and what to forget. <br><br> So whatâ€™s next? <br> &nbsp;  âœ… Memory needs to span multiple modalities: text, image, video <br> &nbsp;  âœ… It needs to evolve over timeâ€”learning patterns, updating beliefs <br> &nbsp;  âœ… And maybe most importantly, weâ€™ll need multi-agent shared memory systems, where agents learn from each otherâ€™s experiences | Memory |
| [AutoLibra: Agent Metric Induction from Open-Ended Feedback](https://arxiv.org/pdf/2505.02820) | There are two main types of evaluation in AI agent research: <br> &nbsp; 1/ Goal-oriented evaluation (did the agent do the thing?) <br> &nbsp; 2/ Behavior evaluation (how did the agent do the thing?)  <br><br> But current evaluations often miss the nuance. They're either too coarse or rely on a ton of manual work. ğŸ˜© Thatâ€™s like judging a chef only on the final dish, not their technique. <br><br> Even more interesting are the key factors for good evaluation: <br> &nbsp; 1/ Grounded in behavior: Evaluation should be based on what the agent actually does. <br> &nbsp; 2/ Self-validating: We need ways to check if our evaluations are any good!  <br> &nbsp; 3/ Generalizable: Evaluation methods should work across different agents and tasks.  <br><br> Now ask yourself: How many agent evaluation methods tick all three boxes <br> Answer: Not enough. <br><br> Most agent evaluations today focus on simple success rates, or require experts to hand-craft metrics based on heuristics.  <br><br> But that's like grading a complex project with a simple pass/fail. ğŸ“ We miss so much of the rich detail! <br><br> If we want truly capable agents whether for complex problem-solving, or creative tasks, we need evaluation that captures the quality of their actions... <br><br> and "AutoLibra" offers a way.  <br> &nbsp; âœ… We need methods that can automatically turn open-ended human feedback into concrete evaluation metrics - AutoLibra does this by "grounding" feedback to agent behavior and clustering similar behaviors.  <br> &nbsp; âœ… We need to move beyond just "did it work?" to "how well did it work?" - AutoLibra does this by capturing nuances like "agent's autonomy" or "interaction accuracy".  <br> &nbsp; âœ… And maybe most importantly, we'll need "meta-evaluation" to assess the quality of our evaluation itself - Again AutoLibra does this by using metrics like "coverage" and "redundancy". | Agent Evaluation |
| []() |  |  |
| []() |  |  |


## Week 2/4
| Title | Summary | Topics |
| --- | --- | --- |
| [How well do LLMs reason over tabular data, really?](https://arxiv.org/pdf/2505.07453) | Everyoneâ€™s wowed by LLMs generating text, but what about their skills with tabular data? ğŸ¤” <br><br> ğŸ’¡ Most evaluation metrics miss the mark <br> &nbsp;  -> Free-form metrics like SacreBleu and BERT-score fail to tell correct from incorrect answers on tabular queries.  <br> &nbsp;  -> An LLM-as-a-judge approach, however, achieves over 95 percent accuracy in judging correctness . <br><br> ğŸ“‰ Performance drops with size and complexity <br> &nbsp;  -> As tables grow from 1K to 8K tokens, accuracy on tasks like calculating averages and differences falls below 20 percent for most 7B-parameter models. <br> &nbsp;  -> Only GPT-4o-mini holds near 30 percent on these harder queries . <br><br> ğŸ” Real-world data stumps LLMs <br> &nbsp;  -> Missing values can halve sum-calculation accuracy <br> &nbsp;  -> Duplicate rows confuse result consistency <br> &nbsp;  -> Column or row shuffling sometimes breaks reasoning paths <br><br> If we treat LLMs as black boxes on tabular tasks, we risk faulty analytics. To build reliable AI assistants for data work we need to: <br> &nbsp;  -> Adopt robust evaluation pipelines like LLM-as-a-judge <br> &nbsp;  -> Train and fine-tune on noisy, real-world table variations <br> &nbsp; -> Design architectures that explicitly model relational structure | LLM Reasoning |
| [Qwen3 Technical Report](https://arxiv.org/pdf/2505.09388) | Here's what makes Qwen3 stand out ğŸ§µğŸ‘‡ <br>ğŸ§  Thinking vs. Non-Thinking Modes <br> Qwen3 introduces a dual-mode setup:  <br> &nbsp;  -> Thinking mode for complex reasoning (math, code, logic)  <br> &nbsp;  -> Non-thinking mode for fast, fluent responses <br><br> You can dynamically switch between them using a simple flag in the prompt (/think or /no think). This flexibility gives devs fine-grained control over speed vs. reasoning depth. <br><br> â³ Thinking Budget <br> Want to control how much the model thinks? Qwen3 lets you cap reasoning effort using a thinking token budget. <br> Result? Smarter latency-performance tradeoffs, especially in real-world deployments. <br><br> ğŸ“Š Smaller models, big results <br> Through strong-to-weak distillation, the 4B and 8B models often outperform older 7Bâ€“14B models (including Qwen2.5 and Llama-3) on STEM, multilingual, and coding benchmarks.  <br> &nbsp;  -> Efficient, edge-ready, and high-performing. <br><br> ğŸ§ª 36 trillion tokens. 119 languages. <br> Massive multilingual pretraining. Fine-grained data annotation. <br> Qwen3 expands from 29 to 119 supported languages and dialects, unlocking broader global use cases ğŸŒ <br><br> ğŸ’» Best-in-class open-source model performance <br> Qwen3-235B (MoE) outperforms DeepSeek-R1 and even matches or beats GPT-4o on 18 of 23 reasoning tasks - using fewer parameters and lower inference costs. <br>Even interesting, Qwen3-32B beats models twice its size like Llama-4-Scout in 17 of 23 benchmarks ğŸ“ˆ | LLM |
| []() |  |  |
| []() |  |  |


## Week 3/4
| Title | Summary | Topics |
| --- | --- | --- |
| [When Thinking Fails: The Pitfalls of Reasoning for Instruction-Following in LLMs](https://arxiv.org/pdf/2505.11423) | ğŸ§  Reasoning hurts? Sometimes, yes. <br><br> Weâ€™ve all been told: â€œMake LLMs reason step by step. It helps.â€ <br><br> Chain-of-Thought (CoT) prompting is now standard practice. <br><br> But what if I told you that this kind of reasoning often reduces accuracy in instruction-following tasks? <br><br> Thatâ€™s exactly what this paper - â€œWhen Thinking Failsâ€ found. And the results are hard to ignore. <br><br> ğŸ§© What they tested: <br>15 models across 2 benchmarks: <br> &nbsp;  -> IFEval: Simple, rule-checkable constraints (e.g., â€œmention AI three timesâ€) <br> &nbsp;  -> ComplexBench: Multi-layered constraints (e.g., logical composition, chaining, nesting) <br><br> ğŸ“‰ What they discovered: <br> &nbsp;  -> CoT prompting degraded instruction-following in nearly all models <br> &nbsp;  -> Accuracy often dropped by 16 to 32 percentage points in models like LLaMA 3 <br> &nbsp;  -> Models started inserting â€œhelpfulâ€ content that broke constraints <br> &nbsp;  -> Reasoning sometimes diverted focus away from instructions entirely <br><br> ğŸ‘€ Why this happens:<br>Using attention analysis, they showed that CoT reduces focus on instruction-relevant tokens. <br>They even introduced a new metric: Constraint Attention. <br> Lower constraint attention? Higher failure rate. Simple as that. <br><br> ğŸ› ï¸ What helps fix it? <br>They tested 4 mitigation strategies: <br> &nbsp;  1ï¸âƒ£ Few-shot examples <br> &nbsp;  2ï¸âƒ£ Self-reflection <br> &nbsp;  3ï¸âƒ£ Self-selective reasoning (model decides if it should reason) <br> &nbsp;  4ï¸âƒ£ Classifier-selective reasoning (external model decides) <br><br> âœ… Classifier-selective reasoning performed best - recovering most of the lost performance. <br><br> Reasoning is powerful - but only when used selectively. <br><br> Sometimes, thinking less leads to better outcomes - in real-life as well ğŸ«¡  | LLM Reasoning |
| [SMOTExT: SMOTE meets Large Language Models](https://arxiv.org/pdf/2505.13434) | One of the toughest challenges in NLP is creating quality data when you donâ€™t have much to start with. <br><br> Especially in sensitive domains like healthcare. <br><br> Where privacy matters more than performance. <br><br> The new paper SMOTExT explores a smart solution -  <br>ğŸ“Œ Generate synthetic samples by blending two real ones. <br> But in the latent space, not in the words. <br><br> Hereâ€™s how it works<br> &nbsp;  1ï¸âƒ£ Take two text samples <br> &nbsp;  2ï¸âƒ£ Encode them into BERT-style embeddings <br> &nbsp;  3ï¸âƒ£ Interpolate between them in vector space <br> &nbsp;  4ï¸âƒ£ Use xRAG to decode that midpoint into coherent text <br><br> Simple idea. Strong results.ğŸ’¡  <br> &nbsp; -> Unlike backtranslation or random word swaps, this keeps the data on-distribution <br> &nbsp;  -> Itâ€™s model-agnostic. You generate once, use anywhere <br> &nbsp;  -> You donâ€™t need labels or task-specific retraining <br> &nbsp;  -> You avoid direct exposure to sensitive data while preserving structure <br><br> ğŸ” They tested it on 20 Newsgroups too: <br> &nbsp;  -> Real-only training: 83.6% F1 <br> &nbsp;  -> Real + SMOTExT: 84.0% F1 <br> &nbsp;  -> SMOTExT-only: 80.3% F1 <br><br> Training only on synthetic data got within 3 points of the real baseline. Thatâ€™s promising. Especially when privacy is a constraint. | Synthetic Data Generation |
| [Learning to Reason via Mixture-of-Thought for Logical Reasoning](https://arxiv.org/pdf/2505.15817) | Want better reasoning from LLMs? Try mixing how they think. <br><br> Most LLMs reason in one mode. Usually natural language. <br><br> They break a problem into steps and try to explain their way to the answer. <br><br> But hereâ€™s the catch: That mode often fails. <br><br> In fact, two-thirds of errors in NL reasoning come from just two mistakes: <br> &nbsp;  1/ Missing important branches in logic <br> &nbsp;  2/ Reversing cause and effect <br><br> Here's a simple fix: let models reason in multiple ways - Mixture-of-Thought <br>Instead of just words, LLMs reason using -  <br> &nbsp;  1/ Natural Language <br> &nbsp;  2/ Python Code <br> &nbsp;  3/ Truth Tables (symbolic logic) <br><br> Each mode brings its own strengths -  <br> &nbsp;  3/ Truth tables catch missing logic. <br> &nbsp;  2/ Code adds structure. <br> &nbsp;  1/ NL gives flexibility. <br><br> Together, they cover more ground than any one mode alone. <br><br> How it works: <br> &nbsp;  -> Train LLMs across all 3 reasoning modes using a self-checking loop <br> &nbsp;  -> At inference, generate answers in all 3 formats <br> &nbsp;  -> Use majority vote to pick the final answer <br><br> The result? <br> &nbsp;  âœ… Up to +11.7 percentage points gain over natural language alone <br> &nbsp;  âœ… Smaller open-source models like Gemma-2B match the performance of closed models like GPT-4 + Logic-LM <br> &nbsp;  âœ… Bigger gains on deeper, harder logical reasoning tasks <br><br> Example: <br>â€œThor is happy. Hulk gets angry. Hulk breaks a bridge. If a destroyer breaks a bridge, Peter is not a civilianâ€¦â€ <br>Now ask: â€œIf Thor is happy, does Peter wear a uniform?â€ <br><br> Try solving it. Then picture three reasoning paths: <br> &nbsp;  -> A paragraph explanation <br> &nbsp;  -> A Python class for Hulk <br> &nbsp;  -> A table of logical truth values <br><br> When the model uses all three, it reasons better. | LLM Reasoning |
| []() |  |  |
| []() |  |  |


## Week 4/4
| Title | Summary | Topics |
| --- | --- | --- |
| [The Mirage of Multimodality: Where Truth is Tested and Honesty Unravels](https://arxiv.org/pdf/2505.20214) | More vision. Less truth. <br><br> Multimodal LLMs are supposed to help us see better. <br><br> But they may actually be hallucinating more. <br><br> In complex visual tasks, the best reasoning models often fabricate details. <br><br> Why? Because when they â€œthink slow,â€ they over-commit to false premises. <br><br> Models designed for step-by-step reasoning like QVQ and Llama-V CoT, struggle more than chat models in deceptive visual contexts. <br><br> They follow a depth-first path, digging deeper into flawed logic. <br><br> Chat models, trained to explore many options, use breadth-first reasoning and are more cautious under uncertainty. <br><br> Performance drops fast as prompts get trickier. <br><br> Even top-tier reasoning models fall for illusions - like monsters behind doors, impossible shadows, and fake reflections. <br><br> Reasoning models show inverse scaling: larger sizes often perform worse under misleading visuals. <br><br> More tokens = more false confidence. <br><br> So, what works better? <br> &nbsp; âœ… Chat models trained on diverse dialogs <br> &nbsp; âœ… Breadth-first planning <br> &nbsp; âœ… Human-in-the-loop evaluation <br> &nbsp; âœ… Specialized judges instead of relying on LLMs to rate LLMs <br><br> Multimodality helps but only when the model knows what not to trust. | Multimodal |
| [Can Agents Fix Agent Issues?](https://arxiv.org/pdf/2505.20749) | ğŸ› ï¸ Agents can write code. But can they fix themselves? <br><br> We now have AI agents that can browse, build apps, and plan workflows. <br><br> But when these agents break, who fixes the bugs? <br><br> This new paper explores that exact question - Can software engineering (SE) agents debug other agents? <br><br> Turns out, not really. Not yet. ğŸ«   <br><br> The authors built AGENTISSUE-BENCH - the first benchmark of real-world bugs from tools like CrewAI, MetaGPT, AutoGen, and more. <br><br> They found 201 actual GitHub issues from top open-source agent frameworks. <br><br> Then they manually recreated 50 of them in Docker with test failures, bugs, and fix patches. <br><br> This alone took 500 hours. <br><br> Then they tested top SE agents: Agentless, AutoCodeRover, SWE-agent ğŸ§   <br><br> With both GPT-4o and Claude 3.5 as backends. <br><br> How did they perform? <br> &nbsp;  âŒ Best success rate: 12.67% <br> &nbsp;  âŒ Most failed to even locate the buggy file <br> &nbsp;  âŒ Fixes often passed tests but were semantically wrong <br><br> In short, agent-specific bugs are hard. <br><br> ğŸ“Š Hereâ€™s what makes agent systems tricky: <br> &nbsp;  -> LLM provider incompatibility <br> &nbsp;  -> Tool config and runtime errors <br> &nbsp;  -> Memory corruption in long workflows <br> &nbsp;  -> Workflow loops and hangs <br> &nbsp;  -> Bad prompt formatting <br> &nbsp;  -> Model output parsing failures <br><br> These are not your typical Python bugs. <br><br> They span APIs, async flows, memory graphs, and fragile toolchains. <br><br> Essentially, We need SE agents that understand agent architectures. <br><br> That can diagnose tool/LLM/memory bugs. <br><br> That can read logs, prompts, test cases, and config files and connect the dots. | AI Agents |
| []() |  |  |
| []() |  |  |
