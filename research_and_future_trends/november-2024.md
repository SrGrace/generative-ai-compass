# Best Gen AI Papers of the month - weekly updates (November 2024)

## Week 1/4
| Title | Summary | Topics |
| --- | --- | --- |
| [Distinguishing Ignorance from Error in LLM Hallucinations](https://arxiv.org/pdf/2410.22071) | ğŸ” The Challenge: Ignorance vs. Error - This recent study introduces the concept of two hallucination types: <br> &nbsp; ğŸš« Lack of Knowledge (HK-): When the model simply doesn't know the answer <br> &nbsp; âš ï¸ Despite Knowledge (HK+): When the model gives wrong answers despite knowing the correct information! <br> <br> This distinction is critical, as each type requires different interventions. For HK-, external information is necessary, while HK+ errors can potentially be corrected by adjusting the model's internal logic. ğŸ§  <br> <br>Researchers also introduced WACK (Wrong Answers despite having Correct Knowledge), a novel approach to create model-specific datasets that help distinguish between these types of hallucinations. ğŸ¯ <br> <br>ğŸ”‘ Key findings:  <br> &nbsp; âœ¨ Different AI models have unique "knowledge fingerprints" and hallucination patterns  <br> &nbsp; ğŸ“ Model-specific datasets outperform generic ones in detecting hallucinations  <br> &nbsp; ğŸ”® It's possible to predict potential hallucinations BEFORE they happen! | Hallucinations |
| [Nearest Neighbor Normalization Improves Multimodal Retrieval](https://arxiv.org/pdf/2410.24114) | This new paper proposes an ingenious solution called Nearest Neighbor Normalization (NNN) - The key idea is to estimate and correct for bias in each retrieval candidate, using only the k nearest neighbours from a reference dataset.  <br> <br> ğŸ”‘ This simple yet effective technique provides a few key benefits: <br> &nbsp; âœ… Consistent improvements in retrieval accuracy across a range of state-of-the-art models and datasets ğŸ“ˆ <br> &nbsp; âœ… Significant reductions in gender bias for image retrieval ğŸŒ <br> &nbsp; âœ… Efficient implementation using vector search, making it practical for real-world use ğŸš€ <br> <br> The authors demonstrate NNN can match the performance of finetuning the models, but with no additional training required. Really clever stuff! ğŸ‘ | Multimodal Retrieval improvement |
| [Adapting While Learning: Grounding LLMs for Scientific Problems with Intelligent Tool Usage Adaptation](https://arxiv.org/pdf/2411.00412) | This new research tackles one of AI's biggest challenges: knowing when to solve problems directly vs. when to use specialized tools - a skill that comes naturally to human experts! ğŸ§  <br> <br> ğŸ”‘ They propose a two-stage learning approach that: <br> &nbsp; ğŸ“š First teaches AI to internalize scientific knowledge (like we do in school) - World Knowledge Distillation (WKD) <br> &nbsp;  ğŸ¯ Then trains it to make smart decisions about using advanced tools (like experienced scientists do) - Tool Usage Adaptation (TUA) <br> <br> ğŸ“ˆ The results are incredible: <br> &nbsp;  ğŸ”¹ 28% jump in accuracy across complex scientific tasks <br> &nbsp;  ğŸ”¹ 14% better at deciding when tools are actually needed <br> &nbsp;  ğŸ”¹ Outperformed GPT-4 and Claude 3.5 on specialized scientific problems <br> &nbsp;  ğŸ”¹ All this from a smaller, more efficient model! ğŸ’ª <br> <br> ğŸŒ Real-world impact? Imagine AI research assistants that can: <br> &nbsp;  ğŸ§ª Know when to run complex simulations <br> &nbsp;  ğŸ“Š Decide when basic calculations are enough <br> &nbsp;  âš¡ Save computational resources <br> &nbsp;  ğŸ¯ Deliver more reliable results | Adapting while Learning |
| [Multi-expert Prompting Improves Reliability, Safety and Usefulness of Large Language Models](https://arxiv.org/pdf/2411.00492) | This new study introduces Multi-Expert Prompting - an approach that simulates multiple expert perspectives to improve the quality of LLM responses. <br> <br> ğŸ†• What's new? <br> &nbsp; ğŸ”¹ Instead of relying on a single "expert," this technique generates multiple expert viewpoints on a topic, simulating a panel of specialists.  <br> &nbsp; ğŸ”¹ By integrating their insights and selecting the most comprehensive response, this method avoids biases and broadens the scope of the answers. <br> &nbsp; ğŸ”¹ The process is inspired by decision-making models, ensuring that each "expert" has a unique, valuable perspective. <br> <br> ğŸ”‘ Some key highlights: <br> &nbsp; âœ… Outperforms SOTA methods by up to 8.69% on truthfulness benchmarks  <br> &nbsp; âœ… Completely eliminates toxic content and reduces hurtfulness  <br> &nbsp; âœ… Generates 75% more informative and 76.5% more useful responses compared to leading baselines  <br> &nbsp; âœ… Maintains transparency and explainability through a 7-step aggregation process | Prompting |
| [Attacking Vision-Language Computer Agents via Pop-ups](https://arxiv.org/pdf/2411.08028) | According to this recent paper from researchers at Georgia Tech, Stanford, and the University of Hong Kong, even SOTA models like GPT-4 can be easily fooled by carefully designed adversarial pop-ups. <br> <br> ğŸ”‘ Key findings: <br> &nbsp; ğŸ’¥ Over 80% of the time, the AI agents clicked on the malicious pop-ups instead of completing their intended tasks. This led to significant decreases in their overall task success rates. <br> &nbsp; ğŸ•¸ï¸ The attackers used a variety of techniques to grab the agents' attention, including summarizing the user's query, using fake virus alerts, and speculating the user's intent. <br> &nbsp; ğŸ›¡ï¸ Basic defence strategies like asking the agents to ignore pop-ups were largely ineffective. The agents struggled to distinguish legitimate UI elements from the adversarial ones. <br> <br> This highlights the critical need for AI systems to develop true understanding and reasoning, not just superficial pattern matching. Without it, they remain vulnerable to even simple visual tricks. | VLMs |
| [A Comprehensive Survey of Small Language Models in the Era of Large Language Models: Techniques, Enhancements, Applications, Collaboration with LLMs, and Trustworthiness](https://arxiv.org/pdf/2411.03350) | LLMs have undoubtedly revolutionized text generation, reasoning, and even specialized domains like healthcare and law.  <br> <br> However, their massive parameter sizes and computational demands bring certain challenges: <br> &nbsp; ğŸ’° High operational costs <br> &nbsp; ğŸ”’ Privacy concerns with cloud APIs <br> &nbsp; â° Limited real-time capabilities <br> &nbsp; ğŸ¥ Domain-specific limitations <br> <br> Enter Small Language Models (SLMs) - the game-changers that are:  <br> &nbsp; âœ… Cost-effective  <br> &nbsp; âœ… Privacy-friendly  <br> &nbsp; âœ… Perfect for edge devices  <br> &nbsp; âœ… Easily customizable for specific domains <br> <br> Think of SLMs as the "specialists" of the AI world - lean, efficient, and purpose-built for specific tasks. They're proving that you don't need billions of parameters to make a big impact! ğŸ’¡ | SLMs Survey |
| []() |  |  |
| []() |  |  |


## Week 2/4
| Title | Summary | Topics |
| --- | --- | --- |
| [Fact or Fiction? Can LLMs be Reliable Annotators for Political Truths?](https://arxiv.org/pdf/2411.05775) | This recent study explores the use of LLMs as annotators and even as "judges" to verify political information accuracy.  <br><br> The research suggests a promising, scalable, and cost-effective way to label political news for factual accuracy using open-source LLMs.  <br><br> ğŸ”‘ Key findings: <br> &nbsp; âœ… LLMs can effectively annotate political content for factual accuracy <br> &nbsp; ğŸ“Š Their annotations closely match human expert validation <br> &nbsp; ğŸ“ˆ Some models achieved over 80% accuracy and recall <br> &nbsp; ğŸ’¡ The approach is scalable and cost-effective <br><br> This LLM-driven framework could offer a valuable tool for media organizations, allowing faster, scalable analysis of political content.  | LLM Annotators |
| [LLMs as Method Actors: A Model for Prompt Engineering and Architecture](https://arxiv.org/pdf/2411.05778) | This recent study introduces the â€œMethod Actorsâ€ approach for improving how LLMs process complex reasoning tasks.  <br><br> By treating prompts as scripts and AI responses as performances, this approach brings an interesting perspective to prompt engineering!  <br> <br> ğŸŒ Hereâ€™s a glimpse of the findings: <br><br> ğŸ§© Task Focus: The study tested the method on "Connections" puzzles, a word association game by The New York Times, known for challenging AI with its need for nuanced reasoning. <br> <br> ğŸš€ Results: <br> &nbsp;ğŸ”¹ Traditional Approaches: Standard methods, like â€œChain of Thought,â€ allowed models like GPT-4o to solve up to 41% of puzzles. <br> &nbsp;ğŸ”¹ Method Actors Model: By setting up AI to "perform" as a puzzle solver, it achieved 86% success, even outperforming many human participants. <br> &nbsp;ğŸ”¹ OpenAIâ€™s o1-preview Model: With this technique, o1-preview achieved a nearly perfect 99% accuracy when tasked to â€œactâ€ through the puzzle-solving process. ğŸ«¡  <br><br> âœ¨ Beyond puzzles, the method actor model might inspire new ways to enhance AIâ€™s capabilities in fields requiring contextual understanding - imagine AI â€œplaying the roleâ€ of a helpful teacher, empathetic listener, or legal assistant. | Prompting |
| [Learning with Less: Knowledge Distillation from Large Language Models via Unlabeled Data](https://arxiv.org/pdf/2411.08028) | LLMs like GPT-4 and LLaMA are reshaping NLP, but their massive size and high computational needs can make them challenging to deploy practically. ğŸŒ  <br> <br> So, how can we use the knowledge of these giants efficiently?  <br> <br> The answer lies in a smarter approach to knowledge transfer called LLKD (Learning with Less Knowledge Distillation). <br> <br> Hereâ€™s how LLKD works and why it matters:  <br> &nbsp; ğŸŒ Challenge: Training smaller, deployable models with LLM-generated pseudo-labels sounds like a solution. But thereâ€™s a catch: not all pseudo-labels are created equal, and noisy labels can hurt the modelâ€™s performance. <br> <br> ğŸ€ Solution - LLKDâ€™s Intelligent Selection: <br> &nbsp;ğŸ”¹ High-Confidence Labels: LLKD picks samples where the LLM (teacher) is confident, ensuring reliable labels. <br> &nbsp;ğŸ”¹ High Information Need: It also identifies where the smaller model (student) shows high uncertainty, pinpointing areas needing more learning.  <br> <br> ğŸ“ˆ Results: <br> &nbsp;ğŸ”¹ Efficient Data Use: LLKD dramatically reduces labelled data needs by focusing on informative, reliable samples. <br> &nbsp;ğŸ”¹ Enhanced Performance: Experiments across diverse datasets like PubMed-RCT-20k and Yahoo! Answers show LLKD outperforms traditional methods, achieving better accuracy and data efficiency. | Knowledge Distillation |
| []() |  |  |
| []() |  |  |
| []() |  |  |


## Week 3/4
| Title | Summary | Topics |
| --- | --- | --- |
| []() |  |  |
| []() |  |  |


## Week 4/4
| Title | Summary | Topics |
| --- | --- | --- |
| []() |  |  |
| []() |  |  |
