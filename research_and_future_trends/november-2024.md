# Best Gen AI Papers of the month - weekly updates (November 2024)

## Week 1/4
| Title | Summary | Topics |
| --- | --- | --- |
| [Distinguishing Ignorance from Error in LLM Hallucinations](https://arxiv.org/pdf/2410.22071) | ğŸ” The Challenge: Ignorance vs. Error - This recent study introduces the concept of two hallucination types: <br> &nbsp; ğŸš« Lack of Knowledge (HK-): When the model simply doesn't know the answer <br> &nbsp; âš ï¸ Despite Knowledge (HK+): When the model gives wrong answers despite knowing the correct information! <br> <br>This distinction is critical, as each type requires different interventions. For HK-, external information is necessary, while HK+ errors can potentially be corrected by adjusting the model's internal logic. ğŸ§  <br> <br>Researchers also introduced WACK (Wrong Answers despite having Correct Knowledge), a novel approach to create model-specific datasets that help distinguish between these types of hallucinations. ğŸ¯ <br> <br>ğŸ”‘ Key findings:  <br> &nbsp; âœ¨ Different AI models have unique "knowledge fingerprints" and hallucination patterns  <br> &nbsp; ğŸ“ Model-specific datasets outperform generic ones in detecting hallucinations  <br> &nbsp; ğŸ”® It's possible to predict potential hallucinations BEFORE they happen! | Hallucinations |
| []() |  |  |
| []() |  |  |
| []() |  |  |


## Week 2/4
| Title | Summary | Topics |
| --- | --- | --- |
| []() |  |  |
| []() |  |  |


## Week 3/4
| Title | Summary | Topics |
| --- | --- | --- |
| []() |  |  |
| []() |  |  |


## Week 4/4
| Title | Summary | Topics |
| --- | --- | --- |
| []() |  |  |
| []() |  |  |
