# Best Gen AI Papers of the month - weekly updates (November 2024)

## Week 1/4
| Title | Summary | Topics |
| --- | --- | --- |
| [Distinguishing Ignorance from Error in LLM Hallucinations](https://arxiv.org/pdf/2410.22071) | ğŸ” The Challenge: Ignorance vs. Error - This recent study introduces the concept of two hallucination types: <br> &nbsp; ğŸš« Lack of Knowledge (HK-): When the model simply doesn't know the answer <br> &nbsp; âš ï¸ Despite Knowledge (HK+): When the model gives wrong answers despite knowing the correct information! <br> <br> This distinction is critical, as each type requires different interventions. For HK-, external information is necessary, while HK+ errors can potentially be corrected by adjusting the model's internal logic. ğŸ§  <br> <br>Researchers also introduced WACK (Wrong Answers despite having Correct Knowledge), a novel approach to create model-specific datasets that help distinguish between these types of hallucinations. ğŸ¯ <br> <br>ğŸ”‘ Key findings:  <br> &nbsp; âœ¨ Different AI models have unique "knowledge fingerprints" and hallucination patterns  <br> &nbsp; ğŸ“ Model-specific datasets outperform generic ones in detecting hallucinations  <br> &nbsp; ğŸ”® It's possible to predict potential hallucinations BEFORE they happen! | Hallucinations |
| [Nearest Neighbor Normalization Improves Multimodal Retrieval](https://arxiv.org/pdf/2410.24114) | This new paper proposes an ingenious solution called Nearest Neighbor Normalization (NNN) - The key idea is to estimate and correct for bias in each retrieval candidate, using only the k nearest neighbours from a reference dataset.  <br> <br> ğŸ”‘ This simple yet effective technique provides a few key benefits: <br> &nbsp; âœ… Consistent improvements in retrieval accuracy across a range of state-of-the-art models and datasets ğŸ“ˆ <br> &nbsp; âœ… Significant reductions in gender bias for image retrieval ğŸŒ <br> &nbsp; âœ… Efficient implementation using vector search, making it practical for real-world use ğŸš€ <br> <br> The authors demonstrate NNN can match the performance of finetuning the models, but with no additional training required. Really clever stuff! ğŸ‘ | Multimodal Retrieval improvement |
| [Adapting While Learning: Grounding LLMs for Scientific Problems with Intelligent Tool Usage Adaptation](https://arxiv.org/pdf/2411.00412) | This new research tackles one of AI's biggest challenges: knowing when to solve problems directly vs. when to use specialized tools - a skill that comes naturally to human experts! ğŸ§  <br> <br> ğŸ”‘ They propose a two-stage learning approach that: <br> &nbsp; ğŸ“š First teaches AI to internalize scientific knowledge (like we do in school) - World Knowledge Distillation (WKD) <br> &nbsp;  ğŸ¯ Then trains it to make smart decisions about using advanced tools (like experienced scientists do) - Tool Usage Adaptation (TUA) <br> <br> ğŸ“ˆ The results are incredible: <br> &nbsp;  ğŸ”¹ 28% jump in accuracy across complex scientific tasks <br> &nbsp;  ğŸ”¹ 14% better at deciding when tools are actually needed <br> &nbsp;  ğŸ”¹ Outperformed GPT-4 and Claude 3.5 on specialized scientific problems <br> &nbsp;  ğŸ”¹ All this from a smaller, more efficient model! ğŸ’ª <br> <br> ğŸŒ Real-world impact? Imagine AI research assistants that can: <br> &nbsp;  ğŸ§ª Know when to run complex simulations <br> &nbsp;  ğŸ“Š Decide when basic calculations are enough <br> &nbsp;  âš¡ Save computational resources <br> &nbsp;  ğŸ¯ Deliver more reliable results | Adapting while Learning |
| [Multi-expert Prompting Improves Reliability, Safety and Usefulness of Large Language Models](https://arxiv.org/pdf/2411.00492) | This new study introduces Multi-Expert Prompting - an approach that simulates multiple expert perspectives to improve the quality of LLM responses. <br> <br> ğŸ†• What's new? <br> &nbsp; ğŸ”¹ Instead of relying on a single "expert," this technique generates multiple expert viewpoints on a topic, simulating a panel of specialists.  <br> &nbsp; ğŸ”¹ By integrating their insights and selecting the most comprehensive response, this method avoids biases and broadens the scope of the answers. <br> &nbsp; ğŸ”¹ The process is inspired by decision-making models, ensuring that each "expert" has a unique, valuable perspective. <br> <br> ğŸ”‘ Some key highlights: <br> &nbsp; âœ… Outperforms SOTA methods by up to 8.69% on truthfulness benchmarks  <br> &nbsp; âœ… Completely eliminates toxic content and reduces hurtfulness  <br> &nbsp; âœ… Generates 75% more informative and 76.5% more useful responses compared to leading baselines  <br> &nbsp; âœ… Maintains transparency and explainability through a 7-step aggregation process | Prompting |
| [Attacking Vision-Language Computer Agents via Pop-ups](https://arxiv.org/pdf/2411.08028) | According to this recent paper from researchers at Georgia Tech, Stanford, and the University of Hong Kong, even SOTA models like GPT-4 can be easily fooled by carefully designed adversarial pop-ups. <br> <br> ğŸ”‘ Key findings: <br> &nbsp; ğŸ’¥ Over 80% of the time, the AI agents clicked on the malicious pop-ups instead of completing their intended tasks. This led to significant decreases in their overall task success rates. <br> &nbsp; ğŸ•¸ï¸ The attackers used a variety of techniques to grab the agents' attention, including summarizing the user's query, using fake virus alerts, and speculating the user's intent. <br> &nbsp; ğŸ›¡ï¸ Basic defence strategies like asking the agents to ignore pop-ups were largely ineffective. The agents struggled to distinguish legitimate UI elements from the adversarial ones. <br> <br> This highlights the critical need for AI systems to develop true understanding and reasoning, not just superficial pattern matching. Without it, they remain vulnerable to even simple visual tricks. | VLMs |
| [A Comprehensive Survey of Small Language Models in the Era of Large Language Models: Techniques, Enhancements, Applications, Collaboration with LLMs, and Trustworthiness](https://arxiv.org/pdf/2411.03350) | LLMs have undoubtedly revolutionized text generation, reasoning, and even specialized domains like healthcare and law.  <br> <br> However, their massive parameter sizes and computational demands bring certain challenges: <br> &nbsp; ğŸ’° High operational costs <br> &nbsp; ğŸ”’ Privacy concerns with cloud APIs <br> &nbsp; â° Limited real-time capabilities <br> &nbsp; ğŸ¥ Domain-specific limitations <br> <br> Enter Small Language Models (SLMs) - the game-changers that are:  <br> &nbsp; âœ… Cost-effective  <br> &nbsp; âœ… Privacy-friendly  <br> &nbsp; âœ… Perfect for edge devices  <br> &nbsp; âœ… Easily customizable for specific domains <br> <br> Think of SLMs as the "specialists" of the AI world - lean, efficient, and purpose-built for specific tasks. They're proving that you don't need billions of parameters to make a big impact! ğŸ’¡ | SLMs Survey |
| []() |  |  |
| []() |  |  |


## Week 2/4
| Title | Summary | Topics |
| --- | --- | --- |
| [Fact or Fiction? Can LLMs be Reliable Annotators for Political Truths?](https://arxiv.org/pdf/2411.05775) | This recent study explores the use of LLMs as annotators and even as "judges" to verify political information accuracy.  <br><br> The research suggests a promising, scalable, and cost-effective way to label political news for factual accuracy using open-source LLMs.  <br><br> ğŸ”‘ Key findings: <br> &nbsp; âœ… LLMs can effectively annotate political content for factual accuracy <br> &nbsp; ğŸ“Š Their annotations closely match human expert validation <br> &nbsp; ğŸ“ˆ Some models achieved over 80% accuracy and recall <br> &nbsp; ğŸ’¡ The approach is scalable and cost-effective <br><br> This LLM-driven framework could offer a valuable tool for media organizations, allowing faster, scalable analysis of political content.  | LLM Annotators |
| [LLMs as Method Actors: A Model for Prompt Engineering and Architecture](https://arxiv.org/pdf/2411.05778) | This recent study introduces the â€œMethod Actorsâ€ approach for improving how LLMs process complex reasoning tasks.  <br><br> By treating prompts as scripts and AI responses as performances, this approach brings an interesting perspective to prompt engineering!  <br> <br> ğŸŒ Hereâ€™s a glimpse of the findings: <br><br> ğŸ§© Task Focus: The study tested the method on "Connections" puzzles, a word association game by The New York Times, known for challenging AI with its need for nuanced reasoning. <br> <br> ğŸš€ Results: <br> &nbsp;ğŸ”¹ Traditional Approaches: Standard methods, like â€œChain of Thought,â€ allowed models like GPT-4o to solve up to 41% of puzzles. <br> &nbsp;ğŸ”¹ Method Actors Model: By setting up AI to "perform" as a puzzle solver, it achieved 86% success, even outperforming many human participants. <br> &nbsp;ğŸ”¹ OpenAIâ€™s o1-preview Model: With this technique, o1-preview achieved a nearly perfect 99% accuracy when tasked to â€œactâ€ through the puzzle-solving process. ğŸ«¡  <br><br> âœ¨ Beyond puzzles, the method actor model might inspire new ways to enhance AIâ€™s capabilities in fields requiring contextual understanding - imagine AI â€œplaying the roleâ€ of a helpful teacher, empathetic listener, or legal assistant. | Prompting |
| [Learning with Less: Knowledge Distillation from Large Language Models via Unlabeled Data](https://arxiv.org/pdf/2411.08028) | LLMs like GPT-4 and LLaMA are reshaping NLP, but their massive size and high computational needs can make them challenging to deploy practically. ğŸŒ  <br> <br> So, how can we use the knowledge of these giants efficiently?  <br><br> The answer lies in a smarter approach to knowledge transfer called LLKD (Learning with Less Knowledge Distillation). <br> <br> Hereâ€™s how LLKD works and why it matters:  <br> &nbsp; ğŸŒ Challenge: Training smaller, deployable models with LLM-generated pseudo-labels sounds like a solution. But thereâ€™s a catch: not all pseudo-labels are created equal, and noisy labels can hurt the modelâ€™s performance. <br> <br> ğŸ€ Solution - LLKDâ€™s Intelligent Selection: <br> &nbsp;ğŸ”¹ High-Confidence Labels: LLKD picks samples where the LLM (teacher) is confident, ensuring reliable labels. <br> &nbsp;ğŸ”¹ High Information Need: It also identifies where the smaller model (student) shows high uncertainty, pinpointing areas needing more learning.  <br> <br> ğŸ“ˆ Results: <br> &nbsp;ğŸ”¹ Efficient Data Use: LLKD dramatically reduces labelled data needs by focusing on informative, reliable samples. <br> &nbsp;ğŸ”¹ Enhanced Performance: Experiments across diverse datasets like PubMed-RCT-20k and Yahoo! Answers show LLKD outperforms traditional methods, achieving better accuracy and data efficiency. | Knowledge Distillation |
| [Toward Optimal Search and Retrieval for RAG](https://arxiv.org/pdf/2411.07396) | ğŸ¤” Imagine youâ€™re searching for critical information in a vast digital library. You need accuracy, but you also need speed. Every extra second counts, yet so does every relevant document you pull. <br><br> This balance is the challenge facing RAG systems today.<br><br> This recent study delves into improving just that, specifically focusing on optimizing the retriever component in search tasks for effective QA. <br><br> ğŸ”‘ Key insights: <br> &nbsp; ğŸ” More isn't always better: Performance plateaus at 10-20 retrieved documents for most tasks. Adding more can actually hurt performance! <br> &nbsp;  âš¡ï¸ Speed vs Accuracy trade-off: Here's something exciting - lowering search accuracy in RAG systems has minimal impact on performance while potentially boosting retrieval speed and memory efficiency.  <br> &nbsp;  ğŸ’¡ Gold standard matters: Including just ONE relevant "gold" document significantly improves accuracy. Each additional relevant document steadily increases performance. <br> &nbsp;  ğŸ¯ Practical takeaway: Focus on retrieving highly relevant documents rather than just more documents. Quality > Quantity. | RAG |
| [A Taxonomy of AgentOps for Enabling Observability of Foundation Model based Agents](https://arxiv.org/pdf/2411.05285v1) | The rise of Foundation Models (FMs) and LLMs has revolutionized AI applications across industries, enabling a new wave of autonomous agents that perform complex tasks with minimal human intervention. <br><br> But with great power comes great responsibility - ensuring reliability, transparency, and accountability in these systems is a Herculean challenge. ğŸ§ âš™ï¸<br><br> This is where AgentOps steps in. This new paper offers a robust framework akin to DevOps/MLOps but tailored for the entire lifecycle of agentic systems - from development to deployment and beyond. ğŸš€<br><br> ğŸ” Why It Matters? <br><br> AgentOps emphasizes observability and traceability, the cornerstones of reliable autonomous systems. These features are pivotal for: <br> &nbsp; 1ï¸âƒ£ Diagnosing unexpected behaviours. <br> &nbsp; 2ï¸âƒ£ Ensuring compliance with regulations like the EU AI Act. <br> &nbsp; 3ï¸âƒ£ Building user trust through transparent and accountable processes.<br><br> ğŸ”‘ Key components include: <br> &nbsp; ğŸ›  Agent Creation: Tools to design agents with fine-tuned models, tailored prompts, and versatile toolkits. <br> &nbsp; ğŸ‘ Observability: Detailed tracing of every action, from workflows to LLM interactions. <br> &nbsp; ğŸ“Š Evaluation & Feedback: Continuous testing, user feedback integration, and performance benchmarking. <br> &nbsp; ğŸ”’ Guardrails: Constraints to maintain ethical and safe decision-making, avoiding harmful or unintended actions. | Agents Taxonomy |
| [Mixture-of-Transformers: A Sparse and Scalable Architecture for Multi-Modal Foundation Models](https://arxiv.org/pdf/2411.04996) | This recent paper introduces Mixture-of-Transformers (MoT), a sparse transformer architecture that addresses these issues head-on! <br><br> What Makes MoT Special? <br> &nbsp; ğŸ’¡ Modality-Aware Sparsity: By decoupling non-embedding parameters (like feedforward networks and attention matrices) by modality, MoT efficiently processes each input type while retaining cross-modal synergy. <br> &nbsp; ğŸ’¡ Efficiency Unlocked: <br> &nbsp; &nbsp; ğŸ”¹ Matches the performance of dense baselines using 55.8% of the FLOPs for text+image generation tasks. <br> &nbsp; &nbsp;  ğŸ”¹ Extends seamlessly to speech, achieving comparable results with just 37.2% of the FLOPs. <br> &nbsp; &nbsp;  ğŸ”¹ In the Transfusion setup (text and diffusion-based image objectives), a smaller MoT model outperforms larger dense models in quality metrics like CLIP and FID scores. <br> &nbsp; ğŸ’¡ Faster Training: On high-performance GPUs, MoT reduces training wall-clock time by up to 52.8% for image tasks and 24.4% for text tasks. âš¡ | Transformers |
| []() |  |  |
| []() |  |  |



## Week 3/4
| Title | Summary | Topics |
| --- | --- | --- |
| [Search, Verify and Feedback: Towards Next Generation Post-training Paradigm of Foundation Models via Verifier Engineering](https://arxiv.org/pdf/2411.11504) | This recent paper introduces "Verifier Engineering", a novel post-training paradigm that shifts the focus from traditional data annotation to automated feedback systems. <br><br> ğŸ”‘ This approach integrates three key stages: <br> &nbsp; 1ï¸âƒ£ Search: Generate diverse candidate outputs. <br> &nbsp; 2ï¸âƒ£ Verify: Use multiple automated verifiers (e.g., rule-based systems, LLMs, or tools like calculators) to evaluate these outputs. <br> &nbsp; 3ï¸âƒ£ Feedback: Refine the model based on these evaluations using methods like reinforcement learning or inference adjustments. <br><br> ğŸ” Why is this important? <br> &nbsp; ğŸ”¹ It goes beyond the limitations of data-heavy approaches like RLHF by combining insights from multiple verification sources. <br> &nbsp; ğŸ”¹ Enables iterative learning, where feedback loops systematically improve performance. <br> &nbsp; ğŸ”¹ Targets a pathway to Artificial General Intelligence by focusing on adaptability, reasoning, and generalization. <br><br> ğŸ”¥ Imagine AI that learns not just from human inputs but also from autonomous critiques and refinements! This methodology could redefine the scalability and robustness of AI models. | LLMs Supervision |
| [An Empirical Study on LLM-based Agents for Automated Bug Fixing](https://arxiv.org/pdf/2411.10213) | This recent study sheds light on their potential and challenges, benchmarking top systems like MarsCode Agent and AutoCodeRover on the SWE-bench Lite dataset - a gold standard for real-world bug-fixing scenarios. ğŸ› ï¸ğŸ“Š <br><br> ğŸ”‘ Key Takeaways: <br> &nbsp; 1ï¸âƒ£ Agentic Power: Systems integrating dynamic reproduction and multi-step interactions outperform others, showcasing how autonomy in debugging accelerates problem-solving. <br> &nbsp; 2ï¸âƒ£ Fault Localization Matters: Accurate line-level bug detection dramatically boosts patch success rates - precision is the name of the game! ğŸ¯ <br> &nbsp; 3ï¸âƒ£ Reproduction - A Double-Edged Sword: Bug reproduction adds critical context when issue descriptions are sparse but can mislead when the description is already clear. Striking the right balance is key. ğŸ§© <br> &nbsp; 4ï¸âƒ£ Challenges Ahead: Despite the strides, LLM-based agents need enhanced reasoning and mechanisms to verify patch completeness and manage complex dependencies effectively. | Agents for Bug Fixing |
| [AIGS: GENERATING SCIENCE FROM AI-POWERED AUTOMATED FALSIFICATION](https://arxiv.org/pdf/2411.11910v1) | ğŸ’¡ğŸš€ The scientific research landscape is being transformed by AI, and this new paper AIGS: Generating Science from AI-Powered Automated Falsification introduces a novel approach to AI-driven discoveries.  <br><br> ğŸ”‘ Key Insights:  <br><br> ğŸ§  AI as a Full-Process Scientist <br><br> The authors present BABY-AIGS, a multi-agent system designed to mimic the complete research process. <br><br> From proposing hypotheses to conducting experiments and even verifying scientific laws, this system takes "AI as a scientist" to the next level. Each agent has a specific role: <br> &nbsp; ğŸ”¹ ProposalAgent generates ideas and methodologies. <br> &nbsp; ğŸ”¹ ExpAgent is responsible for experiment execution. <br> &nbsp; ğŸ”¹ ReviewAgent refines those ideas through detailed analysis. <br> &nbsp; ğŸ”¹ FalsificationAgent validates hypotheses with explicit falsification strategies, ensuring scientific rigor. <br><br> ğŸŒŸ Why Falsification Matters <br> &nbsp; Inspired by Karl Popper's philosophy (ğŸ¤” In case you're wondering like me - It is centred around the idea that scientific theories should be able to be proven false, or falsified), the authors argue that falsification - testing hypotheses to uncover flaws - is the cornerstone of scientific discovery. Unlike earlier systems, BABY-AIGS explicitly integrates falsification, which enhances both the creativity and reliability of its scientific insights. <br><br> ğŸ” Early Results and Impact <br><br> BABY-AIGS has been tested on three key tasks: data engineering, language modeling, and self-instruct alignment.  <br><br> While still early days, the system shows promise in autonomously producing meaningful discoveries, albeit not yet matching seasoned researchers.  <br><br> Here are some insights: <br> &nbsp; ğŸ”¹ Innovation: The system generates creative ideas and iteratively improves them. <br> &nbsp; ğŸ”¹ Execution: Thanks to a domain-specific language (DSL), the experiments are executable and structured. <br> &nbsp; ğŸ”¹ Validation: A falsification proc | Multi-Agent System |
| [Marco-o1: Towards Open Reasoning Models for Open-Ended Solutions](https://arxiv.org/pdf/2411.14405) | LLMs are evolving, moving beyond structured tasks like coding or physics to embrace the ambiguity of real-world challenges ğŸŒ.  <br><br> One such leap forward is this new study - Marco-o1 from Alibaba's MarcoPolo team, which is an advanced reasoning model for complex problem-solving and open-ended tasks. <br><br> ğŸ¤” What caught my attention is their innovative approach: Instead of just focusing on math and coding problems (which have clear right/wrong answers), they're tackling scenarios where the "correct" answer isn't so black-and-white. <br><br> ğŸ”‘ Key Insights: <br> &nbsp; ğŸ”¹ Chain-of-Thought fine-tuning combined with Monte Carlo Tree Search creates more robust reasoning paths <br> &nbsp; ğŸ”¹ Their unique "mini-step" approach lets the AI break down complex problems into smaller, manageable chunks <br> &nbsp; ğŸ”¹ A fascinating self-reflection mechanism where the AI actually stops to think "Wait! Did I make any mistakes?" ğŸª <br><br> ğŸ¯ The results?  <br> &nbsp; ğŸ“Š +6.17% and +5.60% improvements in reasoning tasks on English and Chinese datasets, respectively. | Reasoning LLM |
| []() |  |  |
| []() |  |  |


## Week 4/4
| Title | Summary | Topics |
| --- | --- | --- |
| [Multi-LLM-Agent Systems: Techniques and Business Perspectives](https://arxiv.org/pdf/2411.14033) | As we delve into the next evolution of AI, the concept of Multi-LLM-Agent Systems (MLAS) stands out as a novel innovation! ğŸš€  <br><br> This approach reshapes the way AI models collaborate, bridging technical advancements and business possibilities into a seamless ecosystem of intelligent agents. ğŸ¤–  <br><br> ğŸ§  What are MLAS? <br> &nbsp; ğŸ”¹ MLAS move beyond single-LLM setups, creating interconnected networks of AI agents.  <br> &nbsp; ğŸ”¹ Each agent specializes in tasks, collaborates using advanced protocols, and interacts autonomously with tools and environments.  <br> &nbsp; ğŸ”¹ The result? A system capable of higher flexibility, enhanced task performance, and robust privacy mechanisms. <br><br> ğŸ”‘ Key Benefits <br> &nbsp; ğŸŒğŸ¤ Enhanced Collaboration: Agents communicate dynamically, sharing knowledge and adapting in real time.  <br> &nbsp; ğŸ’¡ğŸ’° Business Innovation: From data privacy preservation to new monetization avenues, MLAS unlock untapped commercial potential.  <br> &nbsp; ğŸ”’ Scalability and Security: These systems are built with decentralized architectures, ensuring scalability while addressing vulnerabilities through advanced defences.  <br><br> ğŸ’¼ Business Applications <br> &nbsp; ğŸ›¡ï¸ Privacy-Preserved Data Utilization: Agents handle sensitive information while adhering to privacy standards, ensuring trust.  <br> &nbsp; ğŸ“ˆ Traffic Monetization: Intelligent agents optimize user engagement and drive revenue through targeted advertising strategies.  <br> &nbsp; ğŸ§¾ Intelligence Monetization: Insights generated by specialized agents create new revenue streams, making data actionable and valuable.  <br><br> Imagine a decentralized network of travel booking agents. Each agent specializes - flight bookings, hotel recommendations, or itinerary planning - while preserving user privacy and operating efficiently. This is the essence of MLAS in action! âœˆï¸ğŸ¨ | Multi-Agent Systems |
| [XGRAMMAR: FLEXIBLE AND EFFICIENT STRUCTURED GENERATION ENGINE FOR LARGE LANGUAGE MODELS](https://arxiv.org/pdf/2411.15100) | This recent paper proposes "XGrammar", an innovative engine that makes structured generation with LLMs not only flexible but also lightning-fast. ğŸ’¡  <br><br> ğŸ”‘ Key Insights: <br> &nbsp; ğŸ”¹ Efficient Parsing: It divides vocabulary into context-independent tokens (processed quickly using precomputed caches) and context-dependent tokens (handled at runtime). This reduces overhead dramatically. <br> &nbsp; ğŸ”¹ Speed Boost: By co-designing the grammar engine with LLM inference processes, XGrammar achieves up to 100x speedup over traditional solutions for context-free grammars. <br> &nbsp; ğŸ”¹ Near-zero Overhead: For end-to-end structured generation, this approach integrates smoothly into existing LLM workflows, bringing ~80x improvement in generation speed with minimal latency. <br> &nbsp; ğŸ”¹ XGrammar supports cross-platform deployment via WebAssembly, making it compatible with browser-based AI agents. <br><br> Imagine crafting a SQL query, generating a nested JSON object, or controlling a robotic arm with high precision - all in milliseconds! ğŸš€  | Structured Generation |
| [Adapter-based Approaches to Knowledge-enhanced Language Models - A Survey](https://arxiv.org/pdf/2411.16403) | This new research surveys "Adapter-based approaches to Knowledge-Enhanced Language Models (KELMs)" - a paradigm that enriches LLMs with structured knowledge from sources like Knowledge Graphs (KGs).  <br><br> This not only improves factual accuracy but also makes them more robust in knowledge-intensive tasks. ğŸ¤–ğŸ“š <br><br> Adapters are lightweight neural modules that integrate seamlessly into LLM architectures.  <br><br> Instead of fine-tuning massive models, adapters enable task-specific training with fewer resources and without catastrophic forgetting.  <br><br> ğŸ”‘ Key adapter types explored in this survey: <br> &nbsp; 1ï¸âƒ£ Houlsby Adapter - Pioneer in modular enhancement. <br> &nbsp; 2ï¸âƒ£ Pfeiffer Adapter - Enables multitask learning with efficient fusion techniques. <br> &nbsp; 3ï¸âƒ£ K-Adapter - Excels in domain-specific knowledge injection, especially in biomedicine. <br><br> ğŸ“Š Trends and Insights <br> &nbsp; ğŸ”¹ Open-domain tasks dominate research, but biomedical KELMs show immense promise, leveraging rich KGs like UMLS and DBpedia. ğŸ¥ <br> &nbsp; ğŸ”¹ Adapter-based KELMs have shown remarkable improvements in tasks like question answering (+8%) and sentiment analysis. <br> &nbsp; ğŸ”¹ While task-specific adapters are well-established, generative applications and low-resource domains remain ripe for innovation. | LLM Survey |
| [A Survey on LLM-as-a-Judge](https://arxiv.org/pdf/2411.15594) | The idea of LLM-as-a-Judge is becoming essential to how we evaluate complex tasks. <br><br> From grading Olympiad-level problems to conducting research peer reviews, LLMs offer scalable, cost-effective, and consistent evaluation solutions - something human experts often struggle with due to fatigue, subjectivity, biases, and time constraints. <br><br> Even Agentic solutions are becoming LLM-as-a-Judge heavy. <br><br> ğŸ“œ This recent survey dives into the potential of these systems, exploring: <br> &nbsp; âœ… Strategies to enhance reliability, such as reducing biases and improving consistency.  <br> &nbsp; ğŸ“Š Frameworks for rigorous evaluation to ensure alignment with human judgment.  <br> &nbsp; ğŸŒ Practical applications spanning education, law, finance, and beyond.  <br><br> The paper also introduces a novel benchmark to test LLM-as-a-Judge systems and outlines strategies for improvement, such as fine-tuning models for task-specific reliability. | LLM-as-a-judge |
| []() |  |  |
| []() |  |  |
| []() |  |  |
