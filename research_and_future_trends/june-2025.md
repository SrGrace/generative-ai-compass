## Week 1/4
| Title | Summary | Topics |
| --- | --- | --- |
| [Topology of Reasoning: Understanding Large Reasoning Models through Reasoning Graph Properties](https://arxiv.org/pdf/2506.05744) | Want better reasoning from LLMs? Study their paths, not just their outputs. <br><br>Most people tune prompts. <br> Some tune data. <br>This paper asks: what if we study how a model thinks, not just what it says? <br><br>Thatâ€™s where reasoning graphs come in. <br><br>ğŸ§© Reasoning isn't a straight line. <br> It's a graph with nodes for each reasoning step, and edges showing how the model moves through them. <br><br>And when researchers plotted these graphs, they noticed something: <br><br>âœ… Better models think in cycles. <br>  They revisit their own steps. Pause. Reconsider. <br>  Just like we do when we catch a mistake mid-sentence. <br><br>âœ… They explore more. <br>  Graph diameters were much larger for high-performing models. <br>  They search wider. Try more paths. Think longer before answering. <br><br>âœ… Their graphs show small-world patterns. <br>  Dense local clusters. Long-range connections. <br>  That means: efficient, yet thorough reasoning. <br><br>âœ…âœ… Even cooler: <br>  These structures emerge naturally in models trained on good SFT data. <br>  No special prompt. No custom loss function. <br>  Just better data. Tracked through the shape of thought. <br><br>So here's a thought: <br>  What if we judged reasoning quality not by BLEU scores, but by graph shape? <br><br>What if dataset design included: <br>  â€œDoes this make the model rethink?â€ <br>  â€œDoes this make it explore more paths?â€ <br><br>This paper gives a new perspective -  <br>  Not just: â€œDid the model get it right?â€ <br>  But: â€œHow did it get there?â€ <br><br>And that shift might matter more than we think. | LLM Reasoning |
| []() |  |  |
| []() |  |  |


## Week 2/4
| Title | Summary | Topics |
| --- | --- | --- |
| [Text-to-LoRA: Instant Transformer Adaption](https://arxiv.org/pdf/2506.06105) | Fine-tuning LLMs is slow. <br>LoRA adapters made it lighter. <br>But we still had to train a new LoRA for every task. <br><br>This paper proposes something smarter: <br><br>Text-to-LoRA (T2L) â€“ a hypernetwork that learns how to build LoRA adapters from scratch - interestingly, just from a text description of the task. <br><br>Hereâ€™s what's interesting ğŸ‘‡ <br><br>âœ… You donâ€™t need to fine-tune per task <br> &nbsp; -> T2L learns to compress hundreds of LoRAs into one shared hypernetwork. <br> &nbsp;  -> Then, when given a new task description, it generates a new adapter in one forward pass. <br><br>âœ… It generalizes <br> &nbsp;  -> Give it a task it hasnâ€™t seen. <br> &nbsp;  -> Just describe the task well. <br> &nbsp;  -> It can still generate a LoRA adapter that performs on par with task-specific ones. <br><br>âœ… Lossy compression helps <br> &nbsp;  -> The generated adapters are simpler than the originals. <br> &nbsp;  -> And that regularization improves performance on some tasks (especially noisy ones). <br><br>âœ… Description quality matters <br> &nbsp;  -> If you say â€œsolve this pleaseâ€, T2L gives you junk. <br> &nbsp; -> If you write â€œthis task requires algorithmic thinking and reasoning,â€ it steers the model to better paths. <br> &nbsp; -> The system is steerable - like human engineers who listen better to clearer requirements. <br><br>âœ… Supervised training > LoRA reconstruction <br> &nbsp; -> T2L trained via full SFT generalizes better than one trained to mimic LoRAs. <br> &nbsp;  -> Why? Because real tasks don't always produce similar adapters, even if the logic overlaps. <br><br>ğŸ“Š The result? <br> &nbsp;  -> T2L matches or exceeds task-specific LoRA performance on several tasks - without ever seeing them. <br> &nbsp;  -> And it works across Mistral, LLaMA, and Gemma. <br><br>The learning here is simple: <br> &nbsp;  -> We donâ€™t always need bigger models. <br> &nbsp;  -> We need smarter adaptation layers. <br> &nbsp;  -> And ways to build them fast, from language alone. | LLM Fine-Tuning | 
| []() |  |  |
| []() |  |  |


## Week 3/4
| Title | Summary | Topics |
| --- | --- | --- |
| []() |  |  |
| []() |  |  |


## Week 4/4
| Title | Summary | Topics |
| --- | --- | --- |
| []() |  |  |
| []() |  |  |
