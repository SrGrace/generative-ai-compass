## Week 1/4
| Title | Summary | Topics |
| --- | --- | --- |
| [Topology of Reasoning: Understanding Large Reasoning Models through Reasoning Graph Properties](https://arxiv.org/pdf/2506.05744) | Want better reasoning from LLMs? Study their paths, not just their outputs. <br><br>Most people tune prompts. <br> Some tune data. <br>This paper asks: what if we study how a model thinks, not just what it says? <br><br>That’s where reasoning graphs come in. <br><br>🧩 Reasoning isn't a straight line. <br> It's a graph with nodes for each reasoning step, and edges showing how the model moves through them. <br><br>And when researchers plotted these graphs, they noticed something: <br><br>✅ Better models think in cycles. <br>  They revisit their own steps. Pause. Reconsider. <br>  Just like we do when we catch a mistake mid-sentence. <br><br>✅ They explore more. <br>  Graph diameters were much larger for high-performing models. <br>  They search wider. Try more paths. Think longer before answering. <br><br>✅ Their graphs show small-world patterns. <br>  Dense local clusters. Long-range connections. <br>  That means: efficient, yet thorough reasoning. <br><br>✅✅ Even cooler: <br>  These structures emerge naturally in models trained on good SFT data. <br>  No special prompt. No custom loss function. <br>  Just better data. Tracked through the shape of thought. <br><br>So here's a thought: <br>  What if we judged reasoning quality not by BLEU scores, but by graph shape? <br><br>What if dataset design included: <br>  “Does this make the model rethink?” <br>  “Does this make it explore more paths?” <br><br>This paper gives a new perspective -  <br>  Not just: “Did the model get it right?” <br>  But: “How did it get there?” <br><br>And that shift might matter more than we think. | LLM Reasoning | 
| []() |  |  |
| []() |  |  |


## Week 2/4
| Title | Summary | Topics |
| --- | --- | --- |
| [Text-to-LoRA: Instant Transformer Adaption](https://arxiv.org/pdf/2506.06105) | Fine-tuning LLMs is slow. <br>LoRA adapters made it lighter. <br>But we still had to train a new LoRA for every task. <br><br>This paper proposes something smarter: <br><br>Text-to-LoRA (T2L) – a hypernetwork that learns how to build LoRA adapters from scratch - interestingly, just from a text description of the task. <br><br>Here’s what's interesting 👇 <br><br>✅ You don’t need to fine-tune per task <br> &nbsp; -> T2L learns to compress hundreds of LoRAs into one shared hypernetwork. <br> &nbsp;  -> Then, when given a new task description, it generates a new adapter in one forward pass. <br><br>✅ It generalizes <br> &nbsp;  -> Give it a task it hasn’t seen. <br> &nbsp;  -> Just describe the task well. <br> &nbsp;  -> It can still generate a LoRA adapter that performs on par with task-specific ones. <br><br>✅ Lossy compression helps <br> &nbsp;  -> The generated adapters are simpler than the originals. <br> &nbsp;  -> And that regularization improves performance on some tasks (especially noisy ones). <br><br>✅ Description quality matters <br> &nbsp;  -> If you say “solve this please”, T2L gives you junk. <br> &nbsp; -> If you write “this task requires algorithmic thinking and reasoning,” it steers the model to better paths. <br> &nbsp; -> The system is steerable - like human engineers who listen better to clearer requirements. <br><br>✅ Supervised training > LoRA reconstruction <br> &nbsp; -> T2L trained via full SFT generalizes better than one trained to mimic LoRAs. <br> &nbsp;  -> Why? Because real tasks don't always produce similar adapters, even if the logic overlaps. <br><br>📊 The result? <br> &nbsp;  -> T2L matches or exceeds task-specific LoRA performance on several tasks - without ever seeing them. <br> &nbsp;  -> And it works across Mistral, LLaMA, and Gemma. <br><br>The learning here is simple: <br> &nbsp;  -> We don’t always need bigger models. <br> &nbsp;  -> We need smarter adaptation layers. <br> &nbsp;  -> And ways to build them fast, from language alone. | LLM Fine-Tuning | 
| []() |  |  |
| []() |  |  |


## Week 3/4
| Title | Summary | Topics |
| --- | --- | --- |
| [From Bytes to Ideas: Language Modeling with Autoregressive U-Nets](https://www.arxiv.org/pdf/2506.14761) | Most LLMs today rely on predefined tokenizers - splitting text into subwords before the model even begins to learn.  <br><br> But what if we let the model decide how to break down language, and when? <br><br> That’s what this new autoregressive U-Net (AU-Net) from Meta explores. <br> &nbsp; ✅ Instead of fixed tokens, it starts with raw bytes. <br> &nbsp; ✅ Then it learns to group them - into characters, words, even phrases while it trains. <br> &nbsp; ✅ No handcrafted token vocabulary. No external tokenizer. No assumptions. <br><br> ⚙️ Here's how it works: <br>AU-Net uses an autoregressive U-Net structure: <br> &nbsp;  -> Pooling layers compress the byte stream to coarser units <br> &nbsp;  -> Upsampling layers bring back detail <br> &nbsp;  -> Attention connects the dots across levels <br><br> In simple terms: it learns language at multiple resolutions - all in one go. <br> &nbsp;  -> It performs on par with BPE-tokenized models - using the same compute <br> &nbsp;  -> It handles low-resource languages better by sharing structure at the byte level <br> &nbsp;  -> It works especially well on character-heavy tasks like code and rare-script languages <br><br> And most importantly: <br> &nbsp;  -> It brings tokenization inside the model, making it adaptive, universal, and language-agnostic. <br><br> 💡 A few ideas it unlocks: <br> &nbsp;  -> Tokenizing as part of training <br> &nbsp;  -> Supporting more diverse scripts <br> &nbsp;  -> Learning semantic structure without human rules <br><br> Interesting stuff! | LLM |
| []() |  |  |
| []() |  |  |


## Week 4/4
| Title | Summary | Topics |
| --- | --- | --- |
| [An Agentic System for Rare Disease Diagnosis with Traceable Reasoning](https://arxiv.org/pdf/2506.20430) | Rare diseases affect over 300 million people. But diagnosing them? <br><br> That’s often a painful journey through years of referrals, dead ends, and trial-and-error treatment. <br><br> So what happens when you equip an AI not just with data, but with memory, tools, logic - and a mission to reason like a real doctor? <br><br> You get DeepRare - An Agentic System for Rare Disease Diagnosis. <br><br> 🕵️‍♀️ How does it work? <br> Think of DeepRare as a 3-layer team: <br> &nbsp; 1> Central Host (LLM with memory) - The "brain" of the system, running the diagnostic loop. <br> &nbsp; 2> Agent Servers – Each agent handles a domain-specific task like: <br> &nbsp; &nbsp; -> Extracting phenotypes <br> &nbsp; &nbsp;  -> Standardizing diseases to OMIM/Orphanet terms <br> &nbsp; &nbsp;  -> Searching PubMed, OMIM, Wikipedia <br> &nbsp; &nbsp;  -> Matching similar patient cases <br> &nbsp; &nbsp;  -> Analyzing gene variants (VCF files) <br> &nbsp; 3> External Data Sources - Web-scale knowledge from medical literature, case reports, and bioinformatics tools. <br><br> 🔄 It works in two stages: <br> &nbsp; 1> Information Collection: It gathers clinical signs, variants, similar cases, and literature, updating its memory. <br> &nbsp; 2> Self-reflective Diagnosis: The system iterates through hypotheses, runs validations, and narrows down a final diagnosis with cited reasoning chains. <br><br> 💡 What makes DeepRare special is: <br> &nbsp; -> Multi-modal inputs (text, HPO, VCF - any mix) <br> &nbsp; -> Modular design: each part can scale or swap <br> &nbsp; -> Transparent outputs: every diagnosis comes with traceable reasoning and references <br> &nbsp; -> Works without needing fine-tuning on each disease <br><br> 📊 Results: In 6,401 real and literature cases: <br> &nbsp; -> Covered 2,919 rare diseases, 100% accuracy on 1,013 <br> &nbsp; -> 70.6% Recall@1 on gene+phenotype cases (vs 53.2% for Exomiser) <br> &nbsp; -> 95.4% agreement with doctors on reasoning <br> &nbsp; -> Outperformed 15 other systems including GPT-4, Claude, Gemini, and Exomiser <br><br> This is a great example of the power of specialized complex agentic systems. | Agentic Systems |
| []() |  |  |
