# Best RAG Papers

| Title | Summary | Tags | Timeline |
| --- | --- | --- | --- |
| [CRAG - Comprehensive RAG Benchmark](https://arxiv.org/pdf/2406.04744) | The Comprehensive RAG Benchmark (CRAG) addresses the limitations of existing RAG datasets by providing a diverse and dynamic set of 4,409 question-answer pairs and mock APIs for simulating web and Knowledge Graph searches. CRAG evaluates LLMs' QA capabilities across various domains and question categories, revealing that even state-of-the-art RAG solutions struggle with accuracy, especially on questions with higher dynamism, lower popularity, or higher complexity. The benchmark has already fostered significant engagement, paving the way for future research and improvements in RAG and general QA solutions. | RAG Evaluation | June 2024 |
| [Corrective Retrieval Augmented Generation](https://arxiv.org/pdf/2401.15884) | CRAG introduces a novel strategy to enhance the robustness and accuracy of large language models during retrieval-augmented generation processes. Addressing the potential pitfalls of relying on the relevance of retrieved documents, it employs a retrieval evaluator to gauge the quality and relevance of documents for a given query, enabling corrective retrieval strategies based on confidence scores. To overcome the limitations of static or outdated databases, CRAG integrates large-scale web searches, providing a richer pool of documents. Additionally, its unique knowledge refinement (decompose-filter-recompose) algorithm ensures the model focuses on pertinent information while discarding the irrelevant, thereby refining the quality of generation. Designed as a versatile, plug-and-play solution, CRAG significantly enhances RAG-based models' performance across a range of generation tasks, demonstrated through substantial improvements in four diverse datasets. | RAG Enhancement | Feb 2024 |
|  |  |  |  |
|  |  |  |  |
|  |  |  |  |
|  |  |  |  |
|  |  |  |  |
|  |  |  |  |
