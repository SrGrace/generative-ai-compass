# Best RAG Papers

| Title | Summary | Tags | Timeline |
| --- | --- | --- | --- |
| [CRAG - Comprehensive RAG Benchmark](https://arxiv.org/pdf/2406.04744) | The Comprehensive RAG Benchmark (CRAG) addresses the limitations of existing RAG datasets by providing a diverse and dynamic set of 4,409 question-answer pairs and mock APIs for simulating web and Knowledge Graph searches. CRAG evaluates LLMs' QA capabilities across various domains and question categories, revealing that even state-of-the-art RAG solutions struggle with accuracy, especially on questions with higher dynamism, lower popularity, or higher complexity. The benchmark has already fostered significant engagement, paving the way for future research and improvements in RAG and general QA solutions. | RAG Evaluation | June 2024 |
| [Corrective Retrieval Augmented Generation](https://arxiv.org/pdf/2401.15884) | CRAG introduces a novel strategy to enhance the robustness and accuracy of large language models during retrieval-augmented generation processes. Addressing the potential pitfalls of relying on the relevance of retrieved documents, it employs a retrieval evaluator to gauge the quality and relevance of documents for a given query, enabling corrective retrieval strategies based on confidence scores. To overcome the limitations of static or outdated databases, CRAG integrates large-scale web searches, providing a richer pool of documents. Additionally, its unique knowledge refinement (decompose-filter-recompose) algorithm ensures the model focuses on pertinent information while discarding the irrelevant, thereby refining the quality of generation. Designed as a versatile, plug-and-play solution, CRAG significantly enhances RAG-based models' performance across a range of generation tasks, demonstrated through substantial improvements in four diverse datasets. | RAG Enhancement | Feb 2024 |
| [Searching for Best Practices in Retrieval-Augmented Generation](https://arxiv.org/pdf/2407.01219) | The paper explores the effectiveness of RAG techniques in providing up-to-date information, reducing hallucinations, and improving response quality, especially in specialized fields. Despite their benefits, RAG methods often face challenges with complexity and slow response times. Through comprehensive experiments, the authors propose strategies to optimize RAG practices, balancing performance and efficiency. Additionally, the study highlights how multimodal retrieval techniques can enhance question-answering for visual inputs and expedite multimodal content generation using a "retrieval as generation" approach. | RAG Best Practices | July 2024 |
| [RAG and Beyond]() | 🔑 Key Takeaways: <br><br> &nbsp; [𝟭] 𝗣𝘂𝗿𝗽𝗼𝘀𝗲 𝗼𝗳 𝗘𝘅𝘁𝗲𝗿𝗻𝗮𝗹 𝗗𝗮𝘁𝗮 𝗜𝗻𝘁𝗲𝗴𝗿𝗮𝘁𝗶𝗼𝗻: Using external data enhances domain-specific expertise, temporal relevance, and reduces hallucination in LLMs, making outputs more controllable and interpretable.<br> &nbsp; [𝟮] 𝗙𝗼𝘂𝗿 𝗧𝘆𝗽𝗲𝘀 𝗼𝗳 𝗤𝘂𝗲𝗿𝘆 𝗖𝗼𝗺𝗽𝗹𝗲𝘅𝗶𝘁𝗶𝗲𝘀:<br> &nbsp; &nbsp; 🔸 𝗟𝗲𝘃𝗲𝗹 𝟭: 𝗘𝘅𝗽𝗹𝗶𝗰𝗶𝘁 𝗙𝗮𝗰𝘁 𝗤𝘂𝗲𝗿𝗶𝗲𝘀: Direct fact retrieval from documents (e.g., “Who is the PM of India?”).<br> &nbsp; &nbsp; 🔸 𝗟𝗲𝘃𝗲𝗹 𝟮: 𝗜𝗺𝗽𝗹𝗶𝗰𝗶𝘁 𝗙𝗮𝗰𝘁 𝗤𝘂𝗲𝗿𝗶𝗲𝘀: Requires basic reasoning or combining multiple pieces of data (e.g., “Who is the President of the country where New Delhi is located?”).<br> &nbsp; &nbsp; 🔸 𝗟𝗲𝘃𝗲𝗹 𝟯: 𝗜𝗻𝘁𝗲𝗿𝗽𝗿𝗲𝘁𝗮𝗯𝗹𝗲 𝗥𝗮𝘁𝗶𝗼𝗻𝗮𝗹𝗲 𝗤𝘂𝗲𝗿𝗶𝗲𝘀: Needs domain-specific logic or rules from manuals (e.g., applying FDA guidelines for drug approval).<br> &nbsp; &nbsp; 🔸 𝗟𝗲𝘃𝗲𝗹 𝟰: 𝗛𝗶𝗱𝗱𝗲𝗻 𝗥𝗮𝘁𝗶𝗼𝗻𝗮𝗹𝗲 𝗤𝘂𝗲𝗿𝗶𝗲𝘀: Involves deeper, implicit patterns not explicitly stated, such as inferring economic impacts.<br> &nbsp; [𝟯] 𝗧𝗲𝗰𝗵𝗻𝗶𝗾𝘂𝗲𝘀 𝗳𝗼𝗿 𝗘𝘅𝘁𝗲𝗿𝗻𝗮𝗹 𝗗𝗮𝘁𝗮 𝗜𝗻𝘁𝗲𝗴𝗿𝗮𝘁𝗶𝗼𝗻:<br> &nbsp; &nbsp; 🔸 𝗖𝗼𝗻𝘁𝗲𝘅𝘁-𝗯𝗮𝘀𝗲𝗱 𝗜𝗻𝘁𝗲𝗴𝗿𝗮𝘁𝗶𝗼𝗻: Injecting relevant external data as context for better grounding.<br> &nbsp; &nbsp; 🔸 𝗦𝗺𝗮𝗹𝗹 𝗠𝗼𝗱𝗲𝗹 𝗔𝘀𝘀𝗶𝘀𝘁𝗮𝗻𝗰𝗲: Use of smaller models trained on specific data to guide the LLM’s reasoning.<br> &nbsp; &nbsp; 🔸 𝗙𝗶𝗻𝗲-𝗧𝘂𝗻𝗶𝗻𝗴: Directly modifying LLM weights to adapt to specialized fields.<br> &nbsp; [𝟰] 𝗖𝗵𝗮𝗹𝗹𝗲𝗻𝗴𝗲𝘀 𝗶𝗻 𝗥𝗲𝗮𝗹-𝗪𝗼𝗿𝗹𝗱 𝗦𝗰𝗲𝗻𝗮𝗿𝗶𝗼𝘀:<br> &nbsp; &nbsp; 🔸 𝗘𝗳𝗳𝗶𝗰𝗶𝗲𝗻𝘁 𝗗𝗮𝘁𝗮 𝗥𝗲𝘁𝗿𝗶𝗲𝘃𝗮𝗹: Finding the right external data with precision.<br> &nbsp; &nbsp; 🔸 𝗜𝗻𝘁𝗲𝗿𝗽𝗿𝗲𝘁𝗮𝗯𝗶𝗹𝗶𝘁𝘆 & 𝗖𝗼𝗻𝘁𝗿𝗼𝗹𝗹𝗮𝗯𝗶𝗹𝗶𝘁𝘆: Making the model’s decision-making transparent, especially in fields like healthcare and finance.<br> &nbsp; &nbsp; 🔸 𝗧𝗮𝘀𝗸 𝗖𝗼𝗺𝗽𝗹𝗲𝘅𝗶𝘁𝘆 𝗠𝗮𝗻𝗮𝗴𝗲𝗺𝗲𝗻𝘁: Understanding when a task requires multiple capabilities to be disentangled for better results.<br> &nbsp; [𝟱] 𝗦𝗼𝗹𝘂𝘁𝗶𝗼𝗻𝘀:<br> &nbsp; &nbsp; 🔸 𝗥𝗔𝗚 𝗙𝗿𝗮𝗺𝗲𝘄𝗼𝗿𝗸𝘀: Combining retrieval and generation, with techniques like iterative RAG and graph/tree-based methods.<br> &nbsp; &nbsp; 🔸 𝗣𝗿𝗼𝗺𝗽𝘁 𝗘𝗻𝗴𝗶𝗻𝗲𝗲𝗿𝗶𝗻𝗴 - 𝗖𝗼𝗧, 𝗧𝗼𝗧, 𝗟𝗼𝗧 𝗲𝘁𝗰.: Enhancing LLM interpretability using structured reasoning paths.<br> &nbsp; &nbsp; 🔸 𝗙𝗶𝗻𝗲-𝗧𝘂𝗻𝗶𝗻𝗴 𝘄𝗶𝘁𝗵 𝗥𝗲𝗱𝘂𝗰𝗲𝗱 𝗖𝗼𝘀𝘁𝘀: Using adapter tuning, LoRA etc. to minimize the cost of domain-specific fine-tuning. | Survey | Sept 2024 |
|  |  |  |  |
|  |  |  |  |
|  |  |  |  |
|  |  |  |  |
