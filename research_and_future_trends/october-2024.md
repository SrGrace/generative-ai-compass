# Best Gen AI Papers of the month - weekly updates (October 2024)

## Week 1/4
| Title | Summary | Topics |
| --- | --- | --- |
| [Were RNNs All We Needed?](https://arxiv.org/pdf/2410.01201) | 🔍 Key Takeaways: <br> &nbsp; 🔹 Scalability Limitations: While Transformers perform well, their quadratic complexity makes them expensive for long sequences. <br> &nbsp; 🔹 Revisiting RNNs: The authors propose minimal versions of LSTMs and GRUs—minLSTM and minGRU—that remove hidden states from input, forget and update gates, making them: <br> &nbsp; &nbsp; 💨 175x-235x faster to train (for a sequence length of 512). <br> &nbsp; &nbsp; 🔢 Use significantly fewer parameters. <br> &nbsp; - This is possible because now these architectures no longer require backpropagate through time (BPTT). <br> &nbsp; 🔹 Performance Parity: Despite their simplicity, minLSTMs and minGRUs achieve performance comparable to cutting-edge models like Mamba, showing that RNNs still have a lot to offer! | RNN |
| [LLMs Know More Than They Show!](https://arxiv.org/pdf/2410.02707) | 🔑 Key Insights: <br> &nbsp; 🔹 Truthfulness is Token-Based: The research reveals that the information about the accuracy of LLM outputs is concentrated in specific tokens within the text. By focusing on these tokens, error detection methods can be significantly improved. <br> &nbsp; 🔹 Predicting Errors: Beyond detecting inaccuracies, the study found that LLMs’ internal representations can predict the types of errors they might produce—this is interesting, it could lead to tailored error mitigation strategies. <br> &nbsp; 🔹 The Model-Behaviour Mismatch: One of the most intriguing findings is the discrepancy between the model’s internal knowledge and its external output. As the authors put it: "LLMs may encode the correct answer, yet consistently generate an incorrect one." This mismatch points to the untapped potential within LLMs, offering an opportunity to develop strategies that leverage this internal knowledge and reduce hallucinations. <br><br>💡 Key Takeaways: <br> &nbsp; 🔸 By diving deeper into how LLMs process truthfulness internally, we can better understand their limitations and unlock new approaches to enhance reliability.  <br> &nbsp; 🔸 As AI continues to integrate into critical applications, these findings can play a pivotal role in shaping safer, more accurate systems. | LLMs |
| [SELECTIVE ATTENTION IMPROVES TRANSFORMER](https://arxiv.org/pdf/2410.02703) | 🪆 𝗛𝗼𝘄 𝗶𝘁 𝘄𝗼𝗿𝗸𝘀: <br> &nbsp; 1️⃣ Soft-Mask Creation: For each token, a soft-mask matrix is generated. This matrix indicates how much attention each token should give to others, with irrelevant tokens receiving less attention. <br> &nbsp; 2️⃣ Apply Constraints: The soft-mask matrix is constrained so that tokens can't mask themselves, and only future tokens are masked. <br> &nbsp; 3️⃣ Accumulate Masking: The mask is accumulated over tokens, reducing the attention paid to less relevant tokens over time. <br> &nbsp; 4️⃣ Adjust Attention Logits: The accumulated mask is subtracted from the attention logits before applying softmax, ensuring that less important tokens have reduced influence. <br> &nbsp; 5️⃣ Context Pruning: Irrelevant tokens that have been heavily masked are pruned from the attention buffer, reducing memory and computation during inference.<br><br>🔑 𝗞𝗲𝘆 𝗕𝗲𝗻𝗲𝗳𝗶𝘁𝘀: <br> &nbsp; 🔹 Improved Model Performance: Transformers equipped with selective attention outperform those with 2x the parameters and heads! <br> &nbsp; 🔹 Memory Efficiency: Reduces memory requirements by up to 47x in large context sizes, making it a game-changer for resource-constrained applications. <br> &nbsp; 🔹 Scalable Across Model Sizes: Works consistently across different model sizes and improves accuracy on challenging benchmarks like HellaSwag. | Transformers |
| [The Perfect Blend: Redefining RLHF with Mixture of Judges](https://arxiv.org/pdf/2409.20370) | 🔑 Key Insights: <br> &nbsp; 🔹 CGPO introduces a "Mixture of Judges" (MoJ) concept, blending multiple LLMs as specialized judges for criteria like factuality and safety. <br> &nbsp; 🔹 CGPO appears to address the "reward hacking" problem that plagues current RLHF methods, especially in coding benchmarks. <br> &nbsp; 🔹 The approach aims for better generalization and reduced need for extensive hyperparameter tuning. <br> &nbsp; 🔹 Early results are promising: <br> &nbsp; &nbsp;• 7.4% improvement in general chat (AlpacaEval-2) <br> &nbsp; &nbsp;• 12.5% gain in STEM reasoning (Arena-Hard) <br> &nbsp; &nbsp;• Consistent improvements in math and coding tasks<br><br>🏗️ CGPO Implementation Nuances: <br> &nbsp; 1️⃣ Model Selection & Fine-Tuning <br> &nbsp; &nbsp;• Choose a pre-trained LLM  <br> &nbsp; &nbsp;• Perform Supervised Fine-Tuning (SFT) on diverse task datasets (general chat, math, instruction following, etc.) <br> &nbsp; 2️⃣ Construct Mixture of Judges (MoJ) <br> &nbsp; &nbsp;• Combine rule-based modules with LLM-based judges <br> &nbsp; &nbsp;• Train specialized reward models (RMs) for criteria like helpfulness <br> &nbsp; &nbsp;• This multi-judge approach is key to preventing reward hacking <br> &nbsp; 3️⃣ Warm-Up Phase <br> &nbsp; &nbsp;• Run Direct Preference Optimization (DPO) for a few steps <br> &nbsp; &nbsp;• Use combined reward data to kickstart optimization <br> &nbsp; 4️⃣ CGPO Main Loop <br> &nbsp; &nbsp;a) Prompt Sampling → b) Response Generation → c) MoJ Evaluation → d) Policy Update using a constrained optimizer | Fine-tuning |
| [Instance-adaptive Zero-shot Chain-of-Thought Prompting](https://arxiv.org/pdf/2409.20441v2) | 🔑 Key findings: <br> &nbsp;🔹 Information flow is crucial: The best prompts extract relevant semantic information from the question and enable the model's rationale to aggregate info both directly from the question and via the prompt. <br> &nbsp;🔹 Dynamic adaptation: IAP selects the optimal prompt for each specific question, providing the AI with the most effective context for reasoning. <br> &nbsp;🔹 Impressive results: On challenging tasks in math, logic, and commonsense reasoning, IAP consistently outperformed other zero-shot Chain-of-Thought prompting methods – even those using carefully curated prompts. <br><br>This research reminds me of the vast difference a well-crafted prompt can make. It's not just about having a powerful AI model; it's about asking the right questions in the right way. | Prompting |
| [ADDITION IS ALL YOU NEED FOR ENERGY-EFFICIENT LANGUAGE MODELS](https://arxiv.org/pdf/2410.00907v2) | As AI increasingly permeates our lives, the need for sustainable, energy-efficient solutions in LLMs has never been more critical. This recent paper, "Addition is All You Need for Energy-Efficient Language Models," proposes a transformative algorithm called L-Mul (Linear-complexity Multiplication) that could redefine our expectations for model efficiency and performance. <br><br>🔑 Here's why this matters: <br> &nbsp; 🔹 Efficiency Boost: L-Mul approximates floating-point multiplication using simple integer addition, potentially reducing energy consumption by up to 95% for element-wise tensor operations. <br> &nbsp; 🔹 Precision Maintained: Despite its simplicity, L-Mul claims to achieve higher precision than 8-bit floating-point multiplication. <br> &nbsp; 🔹 Wide Applicability: Tested across various tasks including NLP, reasoning, and mathematics, L-Mul shows promise in maintaining performance while drastically cutting computational costs. <br> &nbsp; 🔹 Transformer Compatible: When applied to transformer models, L-Mul reportedly achieves equivalent precision to higher-bit operations in both fine-tuning and inference.<br><br>If these claims hold up under peer review, we could be looking at a game-changer for AI hardware and software design. <br><br>Imagine the possibilities: more accessible LLMs, reduced carbon footprint of AI operations, and accelerated AI research and development. | LMs Efficiency |
| []() |  |  |


## Week 2/4
| Title | Summary | Topics |
| --- | --- | --- |
| [DIFFERENTIAL TRANSFORMER](https://arxiv.org/pdf/2410.05258) | Differential Transformer (DIFF Transformer) — a new approach from Microsoft Research that rethinks the way attention should work. Instead of adding more attention, it subtracts it! <br><br>With a novel differential attention mechanism, it calculates attention scores by taking the difference between two softmax attention maps. This unique design cancels out irrelevant noise, leading to sharper, sparser focus, much like noise-cancelling headphones.<br><br>🔑 Key Benefits: <br> &nbsp; 🔍 Improved Long-Context Understanding: Handles large contexts with precision, making it ideal for complex text and document analysis. <br> &nbsp; 🎯 Better Key Information Retrieval: Retrieves the right data without getting distracted by irrelevant details. <br> &nbsp; 🚫 Reduced Hallucinations: By filtering out noise, it generates more accurate and reliable answers. <br> &nbsp; 📈 Enhanced In-Context Learning: More robust to input permutations and changes in context order. <br><br> In short, the DIFF Transformer could redefine what it means for models to focus — delivering more trustworthy and context-aware AI. | Transformers |
| [SCALABLE AND ACCURATE GRAPH REASONING WITH LLM-BASED MULTI-AGENTS](https://arxiv.org/pdf/2410.05130v1) | 📊 Graph structures are at the heart of many critical real-world applications from drug discovery to webpage analysis. <br><br> However, existing LLMs often struggle with the complexity, lack of reasoning paths and scale of such tasks, leading to suboptimal performance.  <br><br> This recent paper introduces an innovative framework called GraphAgent-Reasoner (GAR) that could reshape how we approach these challenges! <br><br> 🔑 Key innovations: <br> &nbsp; 🔹 Divide-and-conquer approach: Breaks complex graph problems into smaller, node-centric tasks <br> &nbsp; 🔹 Multi-Agent collaboration: Each node is assigned an AI agent, working together like a team of experts <br> &nbsp; 🔹 Highly scalable: Can efficiently handle graphs with 1000+ nodes by simply increasing the number of agents <br> &nbsp; 🔹 No Fine-Tuning Required: Unlike other models, this framework doesn’t require extensive retraining. <br><br> 📈 Results are impressive: <br> &nbsp; 🔹 Near-perfect accuracy on polynomial-time graph reasoning tasks in the GraphInstruct benchmark <br> &nbsp; 🔹 Significantly outperforms existing closed-source and fine-tuned models <br> &nbsp; 🔹 Maintains high performance even as graph size increases | Graph Reasoning |
| [Astute RAG](https://arxiv.org/pdf/2410.07176v1) | RAG has transformed how we integrate external knowledge into LLMs. But what happens when the information retrieved is misleading or irrelevant? <br><br> This new study from Google addresses this challenge with Astute RAG - a new approach that solves the key bottleneck of knowledge conflicts between an LLM's internal knowledge and external sources.  <br><br> 🪜 Astute RAG tackles this in three steps: <br> &nbsp; 🔹 Internal Knowledge First: It starts by tapping into the LLM’s internal knowledge to assess what's reliable. <br> &nbsp; 🔹 Source Comparison: It then compares this internal understanding with external information, identifying any conflicts or inconsistencies. <br> &nbsp; 🔹 Trustworthy Consolidation: Finally, Astute RAG consolidates the most reliable insights, ensuring accurate and trustworthy answers. <br><br> Experiments with advanced models like Gemini and Claude show that Astute RAG outperforms earlier RAG methods and even matches the performance of LLMs without RAG in worst-case scenarios. | RAG |
| []() |  |  |
| []() |  |  |


## Week 3/4
| Title | Summary | Topics |
| --- | --- | --- |
| []() |  |  |
| []() |  |  |


## Week 4/4
| Title | Summary | Topics |
| --- | --- | --- |
| []() |  |  |
| []() |  |  |
