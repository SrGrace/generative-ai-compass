# Best Gen AI Papers of the month - weekly updates (October 2024)

## Week 1/4
| Title | Summary | Topics |
| --- | --- | --- |
| [Were RNNs All We Needed?](https://arxiv.org/pdf/2410.01201) | 🔍 Key Takeaways: <br> &nbsp; 🔹 Scalability Limitations: While Transformers perform well, their quadratic complexity makes them expensive for long sequences. <br> &nbsp; 🔹 Revisiting RNNs: The authors propose minimal versions of LSTMs and GRUs—minLSTM and minGRU—that remove hidden states from input, forget and update gates, making them: <br> &nbsp; &nbsp; 💨 175x-235x faster to train (for a sequence length of 512). <br> &nbsp; &nbsp; 🔢 Use significantly fewer parameters. <br> &nbsp; - This is possible because now these architectures no longer require backpropagate through time (BPTT). <br> &nbsp; 🔹 Performance Parity: Despite their simplicity, minLSTMs and minGRUs achieve performance comparable to cutting-edge models like Mamba, showing that RNNs still have a lot to offer! | RNN |
| [LLMs Know More Than They Show!](https://arxiv.org/pdf/2410.02707) | 🔑 Key Insights: <br> &nbsp; 🔹 Truthfulness is Token-Based: The research reveals that the information about the accuracy of LLM outputs is concentrated in specific tokens within the text. By focusing on these tokens, error detection methods can be significantly improved. <br> &nbsp; 🔹 Predicting Errors: Beyond detecting inaccuracies, the study found that LLMs’ internal representations can predict the types of errors they might produce—this is interesting, it could lead to tailored error mitigation strategies. <br> &nbsp; 🔹 The Model-Behaviour Mismatch: One of the most intriguing findings is the discrepancy between the model’s internal knowledge and its external output. As the authors put it: "LLMs may encode the correct answer, yet consistently generate an incorrect one." This mismatch points to the untapped potential within LLMs, offering an opportunity to develop strategies that leverage this internal knowledge and reduce hallucinations. <br><br>💡 Key Takeaways: <br> &nbsp; 🔸 By diving deeper into how LLMs process truthfulness internally, we can better understand their limitations and unlock new approaches to enhance reliability.  <br> &nbsp; 🔸 As AI continues to integrate into critical applications, these findings can play a pivotal role in shaping safer, more accurate systems. | LLMs |
| [SELECTIVE ATTENTION IMPROVES TRANSFORMER](https://arxiv.org/pdf/2410.02703) | 🪆 𝗛𝗼𝘄 𝗶𝘁 𝘄𝗼𝗿𝗸𝘀: <br> &nbsp; 1️⃣ Soft-Mask Creation: For each token, a soft-mask matrix is generated. This matrix indicates how much attention each token should give to others, with irrelevant tokens receiving less attention. <br> &nbsp; 2️⃣ Apply Constraints: The soft-mask matrix is constrained so that tokens can't mask themselves, and only future tokens are masked. <br> &nbsp; 3️⃣ Accumulate Masking: The mask is accumulated over tokens, reducing the attention paid to less relevant tokens over time. <br> &nbsp; 4️⃣ Adjust Attention Logits: The accumulated mask is subtracted from the attention logits before applying softmax, ensuring that less important tokens have reduced influence. <br> &nbsp; 5️⃣ Context Pruning: Irrelevant tokens that have been heavily masked are pruned from the attention buffer, reducing memory and computation during inference.<br><br>🔑 𝗞𝗲𝘆 𝗕𝗲𝗻𝗲𝗳𝗶𝘁𝘀: <br> &nbsp; 🔹 Improved Model Performance: Transformers equipped with selective attention outperform those with 2x the parameters and heads! <br> &nbsp; 🔹 Memory Efficiency: Reduces memory requirements by up to 47x in large context sizes, making it a game-changer for resource-constrained applications. <br> &nbsp; 🔹 Scalable Across Model Sizes: Works consistently across different model sizes and improves accuracy on challenging benchmarks like HellaSwag. | Transformers |
| [The Perfect Blend: Redefining RLHF with Mixture of Judges](https://arxiv.org/pdf/2409.20370) | 🔑 Key Insights: <br> &nbsp; 🔹 CGPO introduces a "Mixture of Judges" (MoJ) concept, blending multiple LLMs as specialized judges for criteria like factuality and safety. <br> &nbsp; 🔹 CGPO appears to address the "reward hacking" problem that plagues current RLHF methods, especially in coding benchmarks. <br> &nbsp; 🔹 The approach aims for better generalization and reduced need for extensive hyperparameter tuning. <br> &nbsp; 🔹 Early results are promising: <br> &nbsp; &nbsp;• 7.4% improvement in general chat (AlpacaEval-2) <br> &nbsp; &nbsp;• 12.5% gain in STEM reasoning (Arena-Hard) <br> &nbsp; &nbsp;• Consistent improvements in math and coding tasks<br><br>🏗️ CGPO Implementation Nuances: <br> &nbsp; 1️⃣ Model Selection & Fine-Tuning <br> &nbsp; &nbsp;• Choose a pre-trained LLM  <br> &nbsp; &nbsp;• Perform Supervised Fine-Tuning (SFT) on diverse task datasets (general chat, math, instruction following, etc.) <br> &nbsp; 2️⃣ Construct Mixture of Judges (MoJ) <br> &nbsp; &nbsp;• Combine rule-based modules with LLM-based judges <br> &nbsp; &nbsp;• Train specialized reward models (RMs) for criteria like helpfulness <br> &nbsp; &nbsp;• This multi-judge approach is key to preventing reward hacking <br> &nbsp; 3️⃣ Warm-Up Phase <br> &nbsp; &nbsp;• Run Direct Preference Optimization (DPO) for a few steps <br> &nbsp; &nbsp;• Use combined reward data to kickstart optimization <br> &nbsp; 4️⃣ CGPO Main Loop <br> &nbsp; &nbsp;a) Prompt Sampling → b) Response Generation → c) MoJ Evaluation → d) Policy Update using a constrained optimizer | Fine-tuning |
| [Instance-adaptive Zero-shot Chain-of-Thought Prompting](https://arxiv.org/pdf/2409.20441v2) | 🔑 Key findings: <br> &nbsp;🔹 Information flow is crucial: The best prompts extract relevant semantic information from the question and enable the model's rationale to aggregate info both directly from the question and via the prompt. <br> &nbsp;🔹 Dynamic adaptation: IAP selects the optimal prompt for each specific question, providing the AI with the most effective context for reasoning. <br> &nbsp;🔹 Impressive results: On challenging tasks in math, logic, and commonsense reasoning, IAP consistently outperformed other zero-shot Chain-of-Thought prompting methods – even those using carefully curated prompts. <br><br>This research reminds me of the vast difference a well-crafted prompt can make. It's not just about having a powerful AI model; it's about asking the right questions in the right way. | Prompting |
| [ADDITION IS ALL YOU NEED FOR ENERGY-EFFICIENT LANGUAGE MODELS](https://arxiv.org/pdf/2410.00907v2) | As AI increasingly permeates our lives, the need for sustainable, energy-efficient solutions in LLMs has never been more critical. This recent paper, "Addition is All You Need for Energy-Efficient Language Models," proposes a transformative algorithm called L-Mul (Linear-complexity Multiplication) that could redefine our expectations for model efficiency and performance. <br><br>🔑 Here's why this matters: <br> &nbsp; 🔹 Efficiency Boost: L-Mul approximates floating-point multiplication using simple integer addition, potentially reducing energy consumption by up to 95% for element-wise tensor operations. <br> &nbsp; 🔹 Precision Maintained: Despite its simplicity, L-Mul claims to achieve higher precision than 8-bit floating-point multiplication. <br> &nbsp; 🔹 Wide Applicability: Tested across various tasks including NLP, reasoning, and mathematics, L-Mul shows promise in maintaining performance while drastically cutting computational costs. <br> &nbsp; 🔹 Transformer Compatible: When applied to transformer models, L-Mul reportedly achieves equivalent precision to higher-bit operations in both fine-tuning and inference.<br><br>If these claims hold up under peer review, we could be looking at a game-changer for AI hardware and software design. <br><br>Imagine the possibilities: more accessible LLMs, reduced carbon footprint of AI operations, and accelerated AI research and development. | LMs Efficiency |
| []() |  |  |


## Week 2/4
| Title | Summary | Topics |
| --- | --- | --- |
| [DIFFERENTIAL TRANSFORMER](https://arxiv.org/pdf/2410.05258) | Differential Transformer (DIFF Transformer) — a new approach from Microsoft Research that rethinks the way attention should work. Instead of adding more attention, it subtracts it! <br><br>With a novel differential attention mechanism, it calculates attention scores by taking the difference between two softmax attention maps. This unique design cancels out irrelevant noise, leading to sharper, sparser focus, much like noise-cancelling headphones.<br><br>🔑 Key Benefits: <br> &nbsp; 🔍 Improved Long-Context Understanding: Handles large contexts with precision, making it ideal for complex text and document analysis. <br> &nbsp; 🎯 Better Key Information Retrieval: Retrieves the right data without getting distracted by irrelevant details. <br> &nbsp; 🚫 Reduced Hallucinations: By filtering out noise, it generates more accurate and reliable answers. <br> &nbsp; 📈 Enhanced In-Context Learning: More robust to input permutations and changes in context order. <br><br> In short, the DIFF Transformer could redefine what it means for models to focus — delivering more trustworthy and context-aware AI. | Transformers |
| [SCALABLE AND ACCURATE GRAPH REASONING WITH LLM-BASED MULTI-AGENTS](https://arxiv.org/pdf/2410.05130v1) | 📊 Graph structures are at the heart of many critical real-world applications from drug discovery to webpage analysis. <br><br> However, existing LLMs often struggle with the complexity, lack of reasoning paths and scale of such tasks, leading to suboptimal performance.  <br><br> This recent paper introduces an innovative framework called GraphAgent-Reasoner (GAR) that could reshape how we approach these challenges! <br><br> 🔑 Key innovations: <br> &nbsp; 🔹 Divide-and-conquer approach: Breaks complex graph problems into smaller, node-centric tasks <br> &nbsp; 🔹 Multi-Agent collaboration: Each node is assigned an AI agent, working together like a team of experts <br> &nbsp; 🔹 Highly scalable: Can efficiently handle graphs with 1000+ nodes by simply increasing the number of agents <br> &nbsp; 🔹 No Fine-Tuning Required: Unlike other models, this framework doesn’t require extensive retraining. <br><br> 📈 Results are impressive: <br> &nbsp; 🔹 Near-perfect accuracy on polynomial-time graph reasoning tasks in the GraphInstruct benchmark <br> &nbsp; 🔹 Significantly outperforms existing closed-source and fine-tuned models <br> &nbsp; 🔹 Maintains high performance even as graph size increases | Graph Reasoning |
| [Astute RAG](https://arxiv.org/pdf/2410.07176v1) | RAG has transformed how we integrate external knowledge into LLMs. But what happens when the information retrieved is misleading or irrelevant? <br><br> This new study from Google addresses this challenge with Astute RAG - a new approach that solves the key bottleneck of knowledge conflicts between an LLM's internal knowledge and external sources.  <br><br> 🪜 Astute RAG tackles this in three steps: <br> &nbsp; 🔹 Internal Knowledge First: It starts by tapping into the LLM’s internal knowledge to assess what's reliable. <br> &nbsp; 🔹 Source Comparison: It then compares this internal understanding with external information, identifying any conflicts or inconsistencies. <br> &nbsp; 🔹 Trustworthy Consolidation: Finally, Astute RAG consolidates the most reliable insights, ensuring accurate and trustworthy answers. <br><br> Experiments with advanced models like Gemini and Claude show that Astute RAG outperforms earlier RAG methods and even matches the performance of LLMs without RAG in worst-case scenarios. | RAG |
| [MLE-BENCH: EVALUATING ML AGENTS ON ML ENGINEERING](https://arxiv.org/pdf/2410.07095) | As AI capabilities surge, evaluating their real-world ML engineering skills becomes essential. That’s where OpenAI’s MLE-Bench steps in - revolutionizing how we assess AI agents by putting them to the test on 75 diverse Kaggle competitions.  <br><br> This benchmark isn’t just about coding but breaking barriers of what AI can achieve in autonomous ML tasks like data preprocessing, model training, and debugging. <br><br> 🔑 Key Insights: <br> &nbsp; 🔹 Real-World Relevance: MLE-Bench mirrors the challenges of modern ML engineering, tackling areas like NLP, computer vision, and complex data workflows. <br> &nbsp; 🔹 Human vs. AI: AI agents are evaluated against human competitors, revealing both impressive breakthroughs and clear limitations. The top-performing setup even secured a bronze medal in 16.9% of competitions! <br> &nbsp; 🔹 Future Potential: If AI agents master these tasks autonomously, it could transform industries - from accelerating scientific research to creating next-gen AI solutions. But with such power comes the need for responsible scaling and careful oversight. | Agents Evaluation |
| [OPTIMA: OPTIMIZING EFFECTIVENESS AND EFFICIENCY FOR LLM-BASED MULTI-AGENT SYSTEM](https://arxiv.org/pdf/2410.08115) | 🔑 Key highlights: <br> &nbsp; 🔹 Significantly improves both communication efficiency and task performance of LLM agents <br> &nbsp; 🔹 Achieves up to 2.8x performance gains while using 90% fewer tokens on some tasks <br> &nbsp; 🔹 Introduces innovative techniques like iterative training and Monte Carlo Tree Search for data generation <br> &nbsp; 🔹 Shows promise for enhancing inference scaling laws and transferring skills to out-of-domain tasks  <br><br> 🪜 Core steps to implement OPTIMA <br> &nbsp; [1] Initialization: <br> &nbsp; &nbsp; - Generate diverse trajectories using format prompts <br> &nbsp; &nbsp; - Select best trajectories and fine-tune base model <br> &nbsp; [2] Choose training approach: <br> &nbsp; &nbsp;a) iSFT: Generate trajectories, select best, apply SFT <br> &nbsp; &nbsp;b) iDPO: Use Monte Carlo tree search (MCTS) for data generation, construct paired data, apply DPO <br> &nbsp; &nbsp;c) Hybrid: Alternate between iSFT and iDPO <br> &nbsp; [3] Implement reward function: <br> &nbsp; &nbsp;R = Rtask - λtoken * Rtoken + λloss * Rloss <br> &nbsp; [4] For iDPO, implement MCTS-inspired data generation <br> &nbsp; [5] Train model using chosen approach (SFT, DPO, or hybrid) <br> &nbsp; [6] Evaluate performance against baselines <br> &nbsp; [7] Repeat steps 2-6 for multiple iterations <br> &nbsp; [8] (Optional) Assess transfer learning and inference scaling | Multi-Agents Optimization |
| [AGENT S: AN OPEN AGENTIC FRAMEWORK THAT USES COMPUTERS LIKE A HUMAN](https://arxiv.org/pdf/2410.08164v1) | 🔍 What makes Agent S special? <br> &nbsp; 🔹 It autonomously interacts with GUIs just like humans do 🖱️⌨️ <br> &nbsp; 🔹 Tackles complex challenges in computer automation: <br> &nbsp; &nbsp; - Acquiring domain-specific knowledge 🧠 <br> &nbsp; &nbsp;  - Planning over long-task horizons 📅 <br> &nbsp; &nbsp; - Handling dynamic, ever-changing interfaces 🔄  <br> <br> 🛠️ Key Insights:  <br> &nbsp; 🔹 Experience-augmented hierarchical planning: Combines web search and memory retrieval for smarter decision-making 🌐🔍  <br> &nbsp; 🔹 Agent-Computer Interface: Enables precise reasoning and control of GUI agents 🎯🖥️  <br> &nbsp; 🔹 Continual learning: Improves as it tackles new tasks 📚🔄 <br><br> 📊 Impressive Results:  <br> &nbsp; 🔹 9.37% absolute improvement in success rate on the OSWorld benchmark  <br> &nbsp; 🔹 83.6% relative improvement over previous state-of-the-art 📈🏆  <br> &nbsp; 🔹 Generalizes well across different operating systems 💻🖥️📱 | Agentic Framework |
| []() |  |  |


## Week 3/4
| Title | Summary | Topics |
| --- | --- | --- |
| [THINKING LLMS: GENERAL INSTRUCTION FOLLOWING WITH THOUGHT GENERATION](https://arxiv.org/pdf/2410.10630) | 🔑 Key Insights: <br> &nbsp; • LLMs now generate internal thoughts before responding 🤔 <br> &nbsp; • Improved performance across diverse tasks, from marketing to creative writing ✍️ <br> &nbsp; • No additional human data required for training 🙅‍♂️ <br> &nbsp; • Outperforms traditional LLMs on benchmark tests 📈 <br><br> 🍀 Why This Matters: <br> &nbsp; 🔹 Enhanced Reasoning: By encouraging "thinking" before responding, LLMs can tackle complex tasks more effectively. Imagine AI assistants that truly ponder your questions! <br> &nbsp; 🔹 Versatile Applications: This isn't just for math or coding. TPO improves performance in marketing, health advice, and even creative tasks. <br> &nbsp; 🔹 Iterative Learning: An AI "judge" helps optimize responses, allowing the model to continuously improve its thinking process. It's like AI teaching AI to be smarter!  | Prompting |
| [GRAPH OF RECORDS: BOOSTING RETRIEVAL AUGMENTED GENERATION FOR LONG-CONTEXT SUMMARIZATION WITH GRAPHS](https://arxiv.org/pdf/2410.11001v1) | 🔑 Key highlights: <br> &nbsp; 🔹 GoR organizes AI-generated responses into a graph structure, uncovering complex connections between text chunks 🕸️ <br> &nbsp; 🔹 Uses graph neural networks and a novel training method to optimize summarization performance 🚀  <br> &nbsp; 🔹 Outperforms existing methods across multiple datasets, including scientific papers, news articles, and books 📚 <br> &nbsp; 🔹 GoR achieved up to 19% improvement in key metrics like Rouge-L, proving its effectiveness across various datasets📈  <br> &nbsp; 🔹 Lightweight and efficient compared to expanding AI model context windows 💻 <br><br> 🪜 Implementation roadmap: <br> &nbsp; 1️⃣ Simulate user queries from document chunks <br> &nbsp; 2️⃣ Use RAG to generate AI responses for each query <br> &nbsp; 3️⃣ Construct a graph connecting text chunks to AI responses <br> &nbsp; 4️⃣ Initialize a Graph Neural Network (GNN) - they used GAT 🧠 <br> &nbsp; 5️⃣ Train the GNN using a custom BERTScore-based objective <br> &nbsp; 6️⃣ Use the trained model to enhance document retrieval for summarization | RAG Enhancement |
| [LOOKING INWARD: LANGUAGE MODELS CAN LEARN ABOUT THEMSELVES BY INTROSPECTION](https://arxiv.org/pdf/2410.13787) | 🤔 Can LLMs learn about themselves? Introspection? <br><br> This recent paper reveals that LLMs can gain self-knowledge through introspection, accessing insights that aren't derived from their training data.<br><br> This means LLMs aren't just imitating patterns but have privileged access to information about their own behaviour! Interesting! 🤯<br><br> 🔑 Key Insights: <br> &nbsp; 🔹 Enhanced Self-Prediction: LLMs trained to predict their own behaviour were significantly more accurate than external models predicting the same. <br> &nbsp; 🔹 Improved Calibration: Introspective models are better at gauging probabilities and uncertainties about their outputs. <br> &nbsp; 🔹 Evolving AI Understanding: This research offers a glimpse into how future models could self-report on their beliefs, reasoning, and even subjective states like preferences or ethical stances. 🌍🤔 <br><br> This is interesting and exciting because self-awareness could lead to more reliable, adaptable AI. 🍀  | LLMs Interospection |
| [MODEL SWARMS: COLLABORATIVE SEARCH TO ADAPT LLM EXPERTS VIA SWARM INTELLIGENCE](https://arxiv.org/pdf/2410.11163) | In the evolving landscape of AI, how can we make LLMs more adaptable and efficient?  <br><br> This recent paper proposes MODEL SWARMS - a novel approach inspired by swarm intelligence, designed to enhance the collaboration between multiple LLM experts. 🧠🤖 <br><br> 🔑 Key Highlights: <br> &nbsp; 🌍🔗 Collaborative Search: Like a swarm of particles, LLM experts move through weight space, learning and optimizing based on personal and global best checkpoints. <br> &nbsp; 📈🔥 Versatile Applications: Whether it’s handling a single task, multi-tasking, adapting to human preferences, or optimizing reward models, MODEL SWARMS consistently outperforms existing methods by up to 21%! <br> &nbsp; 📉💡 Low-Data Friendly: It requires as few as 200 examples to adapt models, making it a powerful tool for low-data environments.  <br> &nbsp; 🎯✨ Discovering New Skills: Through collaboration, even weaker models can uncover hidden capabilities, leading to an impressive 70.8% win rate in human evaluations.  | LLMs Collaboration |
| []() |  |  |
| []() |  |  |


## Week 4/4
| Title | Summary | Topics |
| --- | --- | --- |
| []() |  |  |
| []() |  |  |
