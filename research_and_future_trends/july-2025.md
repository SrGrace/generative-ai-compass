## Week 1/4
| Title | Summary | Topics | 
| --- | --- | --- |
| [AI Research Agents for Machine Learning: Search, Exploration, and Generalization in MLE-bench](https://arxiv.org/pdf/2507.02554) | What if your AI could compete on Kaggle - no fine-tuning, no prompts, just code? 🤖🏅 <br><br>Meta’s new research proposes just that. <br><br>They built AIRA, an agentic system that explores, builds, and iterates on full ML pipelines without training a single model itself. <br><br> No gradients. <br>No fine-tuning. <br>Just smart search, structured reasoning, and days of patient trial and error. <br><br>Here’s how it works: <br><br>🧠 Search Policy: Selects what to try next (Greedy, MCTS, or Evolutionary strategies) <br> &nbsp; 🛠️ Operators: Predefined tools like draft, debug, improve, recall from memory <br> &nbsp;  📈 Fitness Function: Scores each solution based on validation performance <br> &nbsp;  🌐 Search Graph: Stores and tracks all attempts, making the process traceable and reusable <br><br>Key takeaways: <br> &nbsp; ✅ The operators matter more than the search method - A simple Greedy agent + good operators > complex search + weak actions. <br> &nbsp; ✅ Generalization is hard even for agents - Picking winners by validation often fails. Using test data (oracle) improved medal rates by 13%. <br> &nbsp; ✅ Time compounds - Performance climbed for up to 120 hours. The longer it ran, the better it got. <br> &nbsp; ✅ Structure beats scale - AIRA doesn’t rely on model size or prompting tricks. It wins by running a tight loop of reasoning and self-correction. <br><br>They tested AIRA on MLE-bench - a benchmark built from 75 real Kaggle problems. <br><br>🎯 Results: <br> &nbsp;  -> 47.7% success rate on Kaggle-style tasks <br> &nbsp;  -> Outperforms prior AutoML agents by 8+ points <br><br>Using AI to automate the training of ML models is one of the most exciting and promising areas of research today and this might be a big step towards that. | AI Agents |  
| [A Survey on Latent Reasoning](https://arxiv.org/pdf/2507.06203) | Most reasoning models today walk you through their thinking step-by-step - token by token. <br><br> But what if the best reasoning doesn’t need words at all? <br><br> That’s the idea behind Latent Chain-of-Thought (Latent CoT). <br><br> Instead of spelling out thoughts in natural language, latent reasoning keeps everything inside the model’s hidden layers. <br><br> No intermediate tokens. No verbose traces. Just deep, silent loops of structured computation. <br><br> 🔍 This survey breaks down the current landscape of latent reasoning, where models reason by: <br> &nbsp;  -> Looping through activations (vertical recurrence) <br> &nbsp;  -> Evolving internal state across steps (horizontal recurrence) <br> &nbsp;  -> Refining outputs through hidden feedback and memory <br><br> Key Insights: <br> &nbsp;  ✅ Models can refine thoughts internally using architectures like Universal Transformers, CoTFormer, and Recurrent-Depth <br> &nbsp;  ✅ You don’t need new architectures to unlock reasoning - training tricks like Coconut, CCOT, and Pause tokens can simulate latent loops <br> &nbsp;  ✅ Depth matters - Reasoning power often scales with the number of layers <br> &nbsp;  ✅ Layer specialization exists - Shallow layers extract, intermediates reason, deep layers decide <br> &nbsp;  ✅ Infinite-depth models like diffusion LLMs point toward a future where reasoning is not bound by step limits <br> &nbsp;  ✅ Latent reasoning has ~2700x more bandwidth than explicit CoT <br> &nbsp;  ✅ Each hidden state carries rich internal context, beyond what token streams can express. | Reasoning |
| []() |  |  |
| []() |  |  |


## Week 2/4
| Title | Summary | Topics |
| --- | --- | --- |
| []() |  |  |
| []() |  |  |


## Week 3/4
| Title | Summary | Topics |
| --- | --- | --- |
| []() |  |  |
| []() |  |  |


## Week 4/4
| Title | Summary | Topics |
| --- | --- | --- |
| []() |  |  |
| []() |  |  |

